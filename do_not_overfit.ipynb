{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "„335_ipynb“_kopija_blacked.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "900c9646e7154f41b19e56706a018018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e1d56b32362d494db9c6f691358c2bfd",
            "_dom_classes": [],
            "description": "Processing: ",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dba314e931446189e8cab20090eedf6"
          }
        },
        "e1d56b32362d494db9c6f691358c2bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dba314e931446189e8cab20090eedf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2654e38187c4bcf89b1e3b00f2ba271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_76500ead241642b6ab1dc77573b84f87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "TextModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Following data types have been inferred automatically, if they are correct press enter to continue or type 'quit' otherwise.",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ace1c0922e64acdbfe731bfa8880b56"
          }
        },
        "76500ead241642b6ab1dc77573b84f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ace1c0922e64acdbfe731bfa8880b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb87faf6930f47eea3f3593766da3b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef6781d36c2e47d792b99317cce0a29a",
            "_dom_classes": [],
            "description": "Processing: ",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 74,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 74,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9ce89eaa65c41b7953230d3357d986b"
          }
        },
        "ef6781d36c2e47d792b99317cce0a29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9ce89eaa65c41b7953230d3357d986b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd4c9d7a7ad3466480b152c4d79e8277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b5714e122792474c9feb195bdad117f7",
            "_dom_classes": [],
            "description": "Processing: ",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 7,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e7c487d9de9490dbadced501edd1439"
          }
        },
        "b5714e122792474c9feb195bdad117f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e7c487d9de9490dbadced501edd1439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virbickt/do-not-overfit/blob/main/do_not_overfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJFgCMvHy7na"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0yQSgdjl1yY",
        "outputId": "8a08d54b-3c31-4151-8a11-7d1eceadabcf"
      },
      "source": [
        "!pip install eli5 | grep -v 'Requirement already satisfied'\n",
        "!pip install -U pyrasgo[df] | grep -v 'Requirement already satisfied'\n",
        "#!pip install pycaret | grep -v 'Requirement already satisfied'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pyrasgo[df] in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCof4mWal39B"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvMbDhcKl6LF"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import pyrasgo\n",
        "import pycaret.classification as pycr\n",
        "import pycaret.utils as pycu\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.base import clone, BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxRKTIAxl78i"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zwnBuDBmCbj"
      },
      "source": [
        "def get_score(model: BaseEstimator, x: pd.DataFrame, y: pd.Series) -> float:\n",
        "    \"\"\"\n",
        "    Cross-validates the model over five subsets of the original dataset and outputs an average ROCAUC.\n",
        "    The score is not logged using MLFlow as the function is only to be used to produce an overview of the performance of\n",
        "    different models.\n",
        "\n",
        "    :param model: estimator to be fitted during each of 5 folds. This means that the model is fitted 5 times in total.\n",
        "    :param x: training vector, where n_samples is the number of samples and n_features is the number of features.\n",
        "    :param y: target vector relative to x.\n",
        "\n",
        "    :return: an average of the scores produced during each of 5 fits\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    scores = cross_val_score(\n",
        "        model, x, y, cv=5, scoring=\"roc_auc\"\n",
        "    )\n",
        "    return scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVShW3gS5jcc"
      },
      "source": [
        "def get_results(\n",
        "    model: BaseEstimator,\n",
        "    x: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    test_df: pd.DataFrame,\n",
        "    iteration: int,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Trains the model using the latest data and returns the prediction results on the test data saved as csv file.\n",
        "\n",
        "    :param model: model to be fitted\n",
        "    :param test_df: test data that has been untouched during the training process\n",
        "    :param iteration: the number of file to keep track of the files generated thorought\n",
        "\n",
        "    :return: predictions for each sample in the test set\n",
        "    :rtype: pd.DataFrame\n",
        "    \"\"\"\n",
        "    model = model\n",
        "    model.fit(x, y)\n",
        "\n",
        "    y_pred = model.predict_proba(test_df)[:, 1]\n",
        "\n",
        "    results = pd.DataFrame({\"target\": y_pred}).set_index(test_ids)\n",
        "    results.to_csv(f\"results_{iteration}.csv\")\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl5Yb9fKafwp"
      },
      "source": [
        "def get_scoreboard(\n",
        "    logreg: bool = False,\n",
        "    gnb: bool = False,\n",
        "    knn: bool = False,\n",
        "    svm: bool = False,\n",
        "    rf: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Presents the scores stored as global variables before running a function in a form of a dataframe.\n",
        "    :param bool logreg: whether to include Logistic Regression score into the scoreboard\n",
        "    :param bool gnb: whether to include Gaussian Naive Bayes score into the scoreboard\n",
        "    :param bool knn: whether to include K-Nearest Neighbors score into the scoreboard\n",
        "    :param bool svm: whether to include Support Vector Machine score into the scoreboard\n",
        "    :param bool rf: whether to include Random Forest Classifier score into the scoreboard\n",
        "    :return: ROCAUC scores obtained by cross-validation and the scores obtained on Kaggle public scoreboard.\n",
        "    :rtype: pd.DataFrame\n",
        "    \"\"\"\n",
        "    names = list()\n",
        "\n",
        "    if logreg:\n",
        "        names.append(\"Logistic Regression\")\n",
        "\n",
        "    if gnb:\n",
        "        names.append(\"Gaussian Naive Bayes\")\n",
        "\n",
        "    if knn:\n",
        "        names.append(\"K-Nearest Neighbors\")\n",
        "\n",
        "    if svm:\n",
        "        names.append(\"Support Vector Machine\")\n",
        "\n",
        "    if rf:\n",
        "        names.append(\"Random Forrest Classifier\")\n",
        "\n",
        "    scores = list()\n",
        "\n",
        "    if logreg:\n",
        "        scores.append(score_logreg)\n",
        "\n",
        "    if gnb:\n",
        "        scores.append(score_gnb)\n",
        "\n",
        "    if knn:\n",
        "        scores.append(score_knn)\n",
        "\n",
        "    if svm:\n",
        "        scores.append(score_svm)\n",
        "\n",
        "    if rf:\n",
        "        scores.append(score_rf)\n",
        "\n",
        "    kag_scores = list()\n",
        "\n",
        "    if logreg:\n",
        "        kag_scores.append(logreg_kaggle)\n",
        "\n",
        "    if gnb:\n",
        "        kag_scores.append(gnb_kaggle)\n",
        "\n",
        "    if knn:\n",
        "        kag_scores.append(knn_kaggle)\n",
        "\n",
        "    if svm:\n",
        "        kag_scores.append(svm_kaggle)\n",
        "\n",
        "    if rf:\n",
        "        kag_scores.append(rf_kaggle)\n",
        "\n",
        "    kag_probs = list()\n",
        "\n",
        "    if logreg:\n",
        "        kag_probs.append(logreg_kaggle_prob)\n",
        "\n",
        "    if gnb:\n",
        "        kag_probs.append(logreg_kaggle_prob)\n",
        "\n",
        "    if knn:\n",
        "        kag_probs.append(knn_kaggle_prob)\n",
        "\n",
        "    if svm:\n",
        "        kag_probs.append(svm_kaggle_prob)\n",
        "\n",
        "    if rf:\n",
        "        kag_probs.append(rf_kaggle_prob)\n",
        "\n",
        "    results = pd.DataFrame(\n",
        "        {\n",
        "            \"Model\": names,\n",
        "            \"ROCAUC\": scores,\n",
        "            \"Kaggle score\": kag_scores,\n",
        "            \"Kaggle score (proba)\": kag_probs,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWsiySkT799H"
      },
      "source": [
        "def get_diff_columns(train_df: pd.DataFrame, test_df: pd.DataFrame, threshold: float = 0.1) -> None:\n",
        "    \"\"\"\n",
        "    Performs the two-sample Kolmogorov-Smirnov test to find variables where distributions differ a lot from each other\n",
        "    and plots the distributions of those variables.\n",
        "\n",
        "    :param pd.DataFrame train_df: data to be used for training.\n",
        "    :param pd.DataFrame test_df: test data, which will eventually be used to obtain predictions.\n",
        "    :param float threshold: the value against which we judge the difference between two distributions to be statistically significant\n",
        "\n",
        "    :return: None\n",
        "\n",
        "    Borrowed from https://www.kaggle.com/yjunwoo14/don-t-overfit-assessment-pvt-0-834-pub-0-844 where the author herself/himself\n",
        "    has adapted the function taken from https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the columns where the distributions are very different\n",
        "    diff_data = []\n",
        "    for col in train_df.columns:\n",
        "        statistic, pvalue = ks_2samp(\n",
        "            train_df[col].values, \n",
        "            test_df[col].values\n",
        "        )\n",
        "        if pvalue > 0.05 and np.abs(statistic) < threshold:\n",
        "            diff_data.append({'feature': col, 'p': np.round(pvalue, 5), 'statistic': np.round(np.abs(statistic), 2)})\n",
        "\n",
        "    # Put the differences into a dataframe\n",
        "    diff_df = pd.DataFrame(diff_data).sort_values(by='statistic', ascending=False)\n",
        "    print(f\"number of features with diff distribution : {len(diff_df)}\")\n",
        "    \n",
        "    n_cols = 5\n",
        "    n_rows = 5\n",
        "    _, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n",
        "    axes = [x for l in axes for x in l]\n",
        "\n",
        "    # Create plots\n",
        "    for i, (_, row) in enumerate(diff_df.iterrows()):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "        extreme = np.max(np.abs(train_df[row.feature].tolist() + test_df[row.feature].tolist()))\n",
        "        train_df.loc[:, row.feature].hist(\n",
        "            ax=axes[i], alpha=0.5, label='Train', density=True,\n",
        "            bins=np.arange(-extreme, extreme, 0.25)\n",
        "        )\n",
        "        test_df.loc[:, row.feature].hist(\n",
        "            ax=axes[i], alpha=0.5, label='Test', density=True,\n",
        "            bins=np.arange(-extreme, extreme, 0.25)\n",
        "        )\n",
        "        axes[i].set_title(f\"Train mean = {np.round(train_df[row.feature].mean(), 3)}, Test mean = {np.round(test_df[row.feature].mean(), 3)}\")\n",
        "        axes[i].set_xlabel(f'Feature: {row.feature}')\n",
        "        axes[i].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "        \n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv-yCjScmVRa"
      },
      "source": [
        "def missing_values_pct(dataset: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Finds the percentage of missing values for each column.\n",
        "\n",
        "    :param pd.DataFrame dataset: dataset the columns of which will have the percentage of\n",
        "    missing values calculated.\n",
        "    :return: a dataset with features names and percentages of missing values in a descending order.\n",
        "    :rtype: pd.DataFrame\n",
        "    \"\"\"\n",
        "    percentage = (dataset.isnull().sum() / dataset.isnull().count()).sort_values(\n",
        "        ascending=False\n",
        "    )\n",
        "    total_null = dataset.isnull().sum().sort_values(ascending=False)\n",
        "    missing_data = [\n",
        "        total_null.drop(total_null[total_null == 0].index),\n",
        "        percentage.drop(percentage[percentage == 0].index),\n",
        "    ]\n",
        "    missing_data = pd.concat(missing_data, axis=1, keys=[\"Total\", \"Percentage\"])\n",
        "\n",
        "    return missing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npr93X8PDGz_"
      },
      "source": [
        "def backward_selection(df: pd.DataFrame, target: pd.Series, max_features=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function uses the pyrasgo.evaluate.feature_importance and pyrasgo.prune.features functions\n",
        "    to incrementally remove features from the training set until the RMSE no longer improves.\n",
        "    This function returns the dataframe with the features that give the best RMSE.\n",
        "\n",
        "    :param df: the dataframe with features that will be sequentially removed via backward selection.\n",
        "    :param target: target variable which is going to be used to calculate feature importances. \n",
        "    :param max_features: the maximum number of features that the dataframe will contain after pruning.\n",
        "    :return: the dataframe with a reduced number of features\n",
        "    :rtype: pd.DataFrame\n",
        "    \"\"\"\n",
        "    # get baseline AUC\n",
        "    select_df = df.copy()\n",
        "    total_features = df.shape[1]\n",
        "    response = rasgo.evaluate.feature_importance(\n",
        "        select_df, target, return_cli_only=True\n",
        "    )\n",
        "    auc = response[\"modelPerformance\"][\"AUC\"]\n",
        "    print(f\"{auc} with {select_df.shape[1]}\")\n",
        "    last_auc = auc\n",
        "\n",
        "    # Drop least important feature and recalculate model peformance\n",
        "    if max_features is None:\n",
        "        max_features = total_features - 1\n",
        "\n",
        "    for num_features in range(total_features - 1, 1, -1):\n",
        "        tmp_df = rasgo.prune.features(select_df, target, top_n=num_features)\n",
        "        response = rasgo.evaluate.feature_importance(\n",
        "            tmp_df, target, return_cli_only=True\n",
        "        )\n",
        "        auc = response[\"modelPerformance\"][\"AUC\"]\n",
        "        print(f\"{auc} with {tmp_df.shape[1]}\")\n",
        "        if (num_features < max_features) and (auc > last_auc):\n",
        "            # AUC increased, return last dataframe\n",
        "            return select_df\n",
        "        else:\n",
        "            # AUC improved, continue dropping features\n",
        "            last_auc = auc\n",
        "            select_df = tmp_df\n",
        "    return select_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJzzkxQxmEJ3"
      },
      "source": [
        "## Uploads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmlxNna0mG_w"
      },
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_subm = pd.read_csv(\"test.csv\")\n",
        "\n",
        "train_df = train_df.drop([\"id\"], axis=1)\n",
        "\n",
        "test_ids = test_subm[\"id\"]\n",
        "test_subm = test_subm.drop([\"id\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "sTy9G0FZIlqi",
        "outputId": "b25e956a-e674-4f57-f9dc-380b9ac7e920"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>2.165</td>\n",
              "      <td>0.681</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>1.309</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-2.246</td>\n",
              "      <td>1.825</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.177</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-1.927</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>1.763</td>\n",
              "      <td>1.449</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.686</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>-1.859</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.009</td>\n",
              "      <td>-2.296</td>\n",
              "      <td>0.385</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>1.528</td>\n",
              "      <td>-0.144</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>-0.403</td>\n",
              "      <td>0.005</td>\n",
              "      <td>1.405</td>\n",
              "      <td>-0.044</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>0.579</td>\n",
              "      <td>2.929</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.761</td>\n",
              "      <td>0.737</td>\n",
              "      <td>0.669</td>\n",
              "      <td>0.717</td>\n",
              "      <td>-1.542</td>\n",
              "      <td>-1.847</td>\n",
              "      <td>-0.445</td>\n",
              "      <td>1.238</td>\n",
              "      <td>-0.840</td>\n",
              "      <td>-1.891</td>\n",
              "      <td>-1.531</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>-0.927</td>\n",
              "      <td>2.072</td>\n",
              "      <td>0.946</td>\n",
              "      <td>-1.105</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.933</td>\n",
              "      <td>-1.410</td>\n",
              "      <td>-0.770</td>\n",
              "      <td>1.740</td>\n",
              "      <td>-1.504</td>\n",
              "      <td>-0.391</td>\n",
              "      <td>-1.551</td>\n",
              "      <td>-1.415</td>\n",
              "      <td>-0.974</td>\n",
              "      <td>0.796</td>\n",
              "      <td>-2.464</td>\n",
              "      <td>-1.424</td>\n",
              "      <td>1.230</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.130</td>\n",
              "      <td>-0.371</td>\n",
              "      <td>-0.930</td>\n",
              "      <td>1.851</td>\n",
              "      <td>1.292</td>\n",
              "      <td>-0.380</td>\n",
              "      <td>1.318</td>\n",
              "      <td>1.146</td>\n",
              "      <td>-0.399</td>\n",
              "      <td>2.227</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.870</td>\n",
              "      <td>1.420</td>\n",
              "      <td>-1.675</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.768</td>\n",
              "      <td>2.563</td>\n",
              "      <td>0.638</td>\n",
              "      <td>1.164</td>\n",
              "      <td>0.407</td>\n",
              "      <td>-1.556</td>\n",
              "      <td>-0.903</td>\n",
              "      <td>1.329</td>\n",
              "      <td>0.452</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>2.218</td>\n",
              "      <td>-1.844</td>\n",
              "      <td>0.158</td>\n",
              "      <td>-1.649</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>-1.167</td>\n",
              "      <td>-1.456</td>\n",
              "      <td>-0.778</td>\n",
              "      <td>0.098</td>\n",
              "      <td>-1.627</td>\n",
              "      <td>0.405</td>\n",
              "      <td>-0.082</td>\n",
              "      <td>-0.797</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>0.710</td>\n",
              "      <td>-0.252</td>\n",
              "      <td>1.920</td>\n",
              "      <td>0.706</td>\n",
              "      <td>-0.915</td>\n",
              "      <td>0.267</td>\n",
              "      <td>-0.607</td>\n",
              "      <td>0.966</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>-2.292</td>\n",
              "      <td>-1.366</td>\n",
              "      <td>-1.085</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.212</td>\n",
              "      <td>1.260</td>\n",
              "      <td>-1.276</td>\n",
              "      <td>-2.013</td>\n",
              "      <td>-1.101</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.661</td>\n",
              "      <td>1.232</td>\n",
              "      <td>-0.632</td>\n",
              "      <td>-0.805</td>\n",
              "      <td>1.236</td>\n",
              "      <td>-1.085</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.661</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>1.306</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.475</td>\n",
              "      <td>-0.613</td>\n",
              "      <td>-0.841</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>0.671</td>\n",
              "      <td>2.493</td>\n",
              "      <td>0.689</td>\n",
              "      <td>0.946</td>\n",
              "      <td>0.160</td>\n",
              "      <td>-0.607</td>\n",
              "      <td>-0.775</td>\n",
              "      <td>1.688</td>\n",
              "      <td>0.302</td>\n",
              "      <td>-1.156</td>\n",
              "      <td>-0.718</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.745</td>\n",
              "      <td>-0.287</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>0.646</td>\n",
              "      <td>-0.119</td>\n",
              "      <td>-0.675</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>-0.454</td>\n",
              "      <td>1.314</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.999</td>\n",
              "      <td>1.242</td>\n",
              "      <td>-0.339</td>\n",
              "      <td>0.403</td>\n",
              "      <td>-1.243</td>\n",
              "      <td>1.365</td>\n",
              "      <td>0.030</td>\n",
              "      <td>-0.475</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.036</td>\n",
              "      <td>1.313</td>\n",
              "      <td>-0.219</td>\n",
              "      <td>1.078</td>\n",
              "      <td>1.880</td>\n",
              "      <td>-0.317</td>\n",
              "      <td>-0.443</td>\n",
              "      <td>1.876</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.435</td>\n",
              "      <td>-0.226</td>\n",
              "      <td>0.311</td>\n",
              "      <td>0.139</td>\n",
              "      <td>-0.075</td>\n",
              "      <td>1.381</td>\n",
              "      <td>1.716</td>\n",
              "      <td>-2.017</td>\n",
              "      <td>-0.485</td>\n",
              "      <td>1.906</td>\n",
              "      <td>-0.119</td>\n",
              "      <td>0.609</td>\n",
              "      <td>-0.564</td>\n",
              "      <td>0.264</td>\n",
              "      <td>-0.604</td>\n",
              "      <td>-0.733</td>\n",
              "      <td>-2.352</td>\n",
              "      <td>-1.661</td>\n",
              "      <td>0.498</td>\n",
              "      <td>-0.841</td>\n",
              "      <td>0.907</td>\n",
              "      <td>-0.476</td>\n",
              "      <td>0.817</td>\n",
              "      <td>1.372</td>\n",
              "      <td>1.187</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.029</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>0.253</td>\n",
              "      <td>1.005</td>\n",
              "      <td>1.413</td>\n",
              "      <td>-0.133</td>\n",
              "      <td>0.655</td>\n",
              "      <td>-0.921</td>\n",
              "      <td>0.231</td>\n",
              "      <td>-1.902</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>-1.730</td>\n",
              "      <td>1.132</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.039</td>\n",
              "      <td>1.489</td>\n",
              "      <td>-0.328</td>\n",
              "      <td>0.966</td>\n",
              "      <td>-0.057</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>0.723</td>\n",
              "      <td>-0.313</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>-0.803</td>\n",
              "      <td>0.074</td>\n",
              "      <td>-2.851</td>\n",
              "      <td>-1.021</td>\n",
              "      <td>-0.894</td>\n",
              "      <td>0.967</td>\n",
              "      <td>0.218</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.514</td>\n",
              "      <td>0.754</td>\n",
              "      <td>-1.892</td>\n",
              "      <td>0.203</td>\n",
              "      <td>2.174</td>\n",
              "      <td>-0.755</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>-1.109</td>\n",
              "      <td>-0.681</td>\n",
              "      <td>1.250</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>2.457</td>\n",
              "      <td>0.771</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.569</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-2.145</td>\n",
              "      <td>-1.120</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.820</td>\n",
              "      <td>-1.049</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.617</td>\n",
              "      <td>1.253</td>\n",
              "      <td>1.248</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.802</td>\n",
              "      <td>-0.896</td>\n",
              "      <td>-1.793</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.867</td>\n",
              "      <td>1.347</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.649</td>\n",
              "      <td>0.672</td>\n",
              "      <td>-2.097</td>\n",
              "      <td>1.051</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>1.038</td>\n",
              "      <td>-1.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>-0.383</td>\n",
              "      <td>0.326</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.172</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>2.907</td>\n",
              "      <td>1.085</td>\n",
              "      <td>2.144</td>\n",
              "      <td>1.540</td>\n",
              "      <td>0.584</td>\n",
              "      <td>1.133</td>\n",
              "      <td>1.098</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-1.100</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>1.382</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-1.519</td>\n",
              "      <td>0.619</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.866</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.238</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-2.721</td>\n",
              "      <td>1.659</td>\n",
              "      <td>0.106</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>1.719</td>\n",
              "      <td>0.411</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.503</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>0.339</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>-0.947</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.695</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.188</td>\n",
              "      <td>-1.082</td>\n",
              "      <td>-0.872</td>\n",
              "      <td>0.660</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.303</td>\n",
              "      <td>-0.553</td>\n",
              "      <td>-0.771</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.472</td>\n",
              "      <td>1.315</td>\n",
              "      <td>-0.467</td>\n",
              "      <td>-0.064</td>\n",
              "      <td>1.808</td>\n",
              "      <td>0.633</td>\n",
              "      <td>1.221</td>\n",
              "      <td>1.112</td>\n",
              "      <td>1.133</td>\n",
              "      <td>-0.543</td>\n",
              "      <td>-2.144</td>\n",
              "      <td>0.151</td>\n",
              "      <td>-0.813</td>\n",
              "      <td>1.966</td>\n",
              "      <td>-1.190</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.195</td>\n",
              "      <td>-0.799</td>\n",
              "      <td>1.117</td>\n",
              "      <td>-0.759</td>\n",
              "      <td>-0.661</td>\n",
              "      <td>0.406</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>-0.035</td>\n",
              "      <td>-1.634</td>\n",
              "      <td>-0.011</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.610</td>\n",
              "      <td>-1.822</td>\n",
              "      <td>-0.030</td>\n",
              "      <td>1.188</td>\n",
              "      <td>-0.006</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>1.914</td>\n",
              "      <td>0.620</td>\n",
              "      <td>-1.495</td>\n",
              "      <td>1.787</td>\n",
              "      <td>-0.305</td>\n",
              "      <td>0.602</td>\n",
              "      <td>-1.208</td>\n",
              "      <td>0.893</td>\n",
              "      <td>0.379</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.581</td>\n",
              "      <td>-0.475</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>-0.691</td>\n",
              "      <td>-0.783</td>\n",
              "      <td>-1.485</td>\n",
              "      <td>1.911</td>\n",
              "      <td>-2.400</td>\n",
              "      <td>-2.372</td>\n",
              "      <td>-0.178</td>\n",
              "      <td>1.550</td>\n",
              "      <td>-0.228</td>\n",
              "      <td>0.674</td>\n",
              "      <td>0.987</td>\n",
              "      <td>1.373</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.229</td>\n",
              "      <td>-0.630</td>\n",
              "      <td>-0.175</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.074</td>\n",
              "      <td>-2.090</td>\n",
              "      <td>-0.625</td>\n",
              "      <td>-1.131</td>\n",
              "      <td>1.111</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>0.574</td>\n",
              "      <td>-0.660</td>\n",
              "      <td>-1.113</td>\n",
              "      <td>0.802</td>\n",
              "      <td>-0.093</td>\n",
              "      <td>1.302</td>\n",
              "      <td>-0.395</td>\n",
              "      <td>0.745</td>\n",
              "      <td>-0.384</td>\n",
              "      <td>0.066</td>\n",
              "      <td>-0.756</td>\n",
              "      <td>0.495</td>\n",
              "      <td>-0.822</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.883</td>\n",
              "      <td>0.211</td>\n",
              "      <td>-0.502</td>\n",
              "      <td>2.506</td>\n",
              "      <td>1.402</td>\n",
              "      <td>1.182</td>\n",
              "      <td>-1.382</td>\n",
              "      <td>0.448</td>\n",
              "      <td>-0.247</td>\n",
              "      <td>0.704</td>\n",
              "      <td>1.558</td>\n",
              "      <td>-0.075</td>\n",
              "      <td>0.609</td>\n",
              "      <td>1.255</td>\n",
              "      <td>-1.263</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.213</td>\n",
              "      <td>-1.395</td>\n",
              "      <td>0.613</td>\n",
              "      <td>-0.865</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.665</td>\n",
              "      <td>-1.081</td>\n",
              "      <td>0.940</td>\n",
              "      <td>0.415</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-0.616</td>\n",
              "      <td>0.987</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.762</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.832</td>\n",
              "      <td>0.395</td>\n",
              "      <td>1.113</td>\n",
              "      <td>-0.735</td>\n",
              "      <td>0.643</td>\n",
              "      <td>0.727</td>\n",
              "      <td>-0.607</td>\n",
              "      <td>-0.955</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.680</td>\n",
              "      <td>-0.192</td>\n",
              "      <td>0.859</td>\n",
              "      <td>-0.354</td>\n",
              "      <td>-1.178</td>\n",
              "      <td>1.039</td>\n",
              "      <td>-1.079</td>\n",
              "      <td>0.668</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.083</td>\n",
              "      <td>-1.411</td>\n",
              "      <td>-0.591</td>\n",
              "      <td>0.742</td>\n",
              "      <td>1.402</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>-0.551</td>\n",
              "      <td>0.003</td>\n",
              "      <td>-0.344</td>\n",
              "      <td>-1.194</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>-0.679</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.066</td>\n",
              "      <td>1.005</td>\n",
              "      <td>-0.822</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.329</td>\n",
              "      <td>1.213</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.584</td>\n",
              "      <td>-0.761</td>\n",
              "      <td>-0.151</td>\n",
              "      <td>-0.175</td>\n",
              "      <td>-0.603</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.354</td>\n",
              "      <td>-0.124</td>\n",
              "      <td>1.299</td>\n",
              "      <td>0.850</td>\n",
              "      <td>-0.318</td>\n",
              "      <td>-0.141</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-0.441</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>0.793</td>\n",
              "      <td>-1.470</td>\n",
              "      <td>0.386</td>\n",
              "      <td>-2.254</td>\n",
              "      <td>-0.463</td>\n",
              "      <td>0.366</td>\n",
              "      <td>-0.676</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.504</td>\n",
              "      <td>1.500</td>\n",
              "      <td>-1.160</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>-0.430</td>\n",
              "      <td>-1.151</td>\n",
              "      <td>1.764</td>\n",
              "      <td>1.307</td>\n",
              "      <td>-0.731</td>\n",
              "      <td>-1.234</td>\n",
              "      <td>0.960</td>\n",
              "      <td>1.470</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.483</td>\n",
              "      <td>-2.015</td>\n",
              "      <td>-1.258</td>\n",
              "      <td>0.630</td>\n",
              "      <td>1.158</td>\n",
              "      <td>0.971</td>\n",
              "      <td>-1.489</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.917</td>\n",
              "      <td>-0.094</td>\n",
              "      <td>-1.407</td>\n",
              "      <td>0.887</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>1.267</td>\n",
              "      <td>-1.667</td>\n",
              "      <td>-2.771</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.932</td>\n",
              "      <td>2.064</td>\n",
              "      <td>0.422</td>\n",
              "      <td>1.215</td>\n",
              "      <td>2.012</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.059</td>\n",
              "      <td>1.121</td>\n",
              "      <td>1.333</td>\n",
              "      <td>0.211</td>\n",
              "      <td>1.753</td>\n",
              "      <td>0.053</td>\n",
              "      <td>1.274</td>\n",
              "      <td>-0.612</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>-1.695</td>\n",
              "      <td>-1.257</td>\n",
              "      <td>1.359</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>-1.624</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>-1.099</td>\n",
              "      <td>-0.936</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-0.348</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.478</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>0.352</td>\n",
              "      <td>1.095</td>\n",
              "      <td>0.300</td>\n",
              "      <td>-1.044</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>0.144</td>\n",
              "      <td>-1.658</td>\n",
              "      <td>-0.946</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-0.772</td>\n",
              "      <td>1.786</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.103</td>\n",
              "      <td>-1.223</td>\n",
              "      <td>2.273</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-2.032</td>\n",
              "      <td>-0.452</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>1.896</td>\n",
              "      <td>-0.152</td>\n",
              "      <td>1.920</td>\n",
              "      <td>-1.244</td>\n",
              "      <td>-1.704</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.088</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.972</td>\n",
              "      <td>-1.554</td>\n",
              "      <td>0.218</td>\n",
              "      <td>-2.677</td>\n",
              "      <td>-1.528</td>\n",
              "      <td>0.613</td>\n",
              "      <td>-1.269</td>\n",
              "      <td>0.516</td>\n",
              "      <td>-0.714</td>\n",
              "      <td>-0.347</td>\n",
              "      <td>-1.025</td>\n",
              "      <td>1.340</td>\n",
              "      <td>0.923</td>\n",
              "      <td>-0.071</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.837</td>\n",
              "      <td>0.847</td>\n",
              "      <td>-0.807</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>1.424</td>\n",
              "      <td>0.943</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.593</td>\n",
              "      <td>-0.544</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-1.081</td>\n",
              "      <td>0.409</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>1.910</td>\n",
              "      <td>0.837</td>\n",
              "      <td>-1.252</td>\n",
              "      <td>1.492</td>\n",
              "      <td>-0.971</td>\n",
              "      <td>0.355</td>\n",
              "      <td>1.079</td>\n",
              "      <td>0.758</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-0.101</td>\n",
              "      <td>1.527</td>\n",
              "      <td>-0.942</td>\n",
              "      <td>-0.496</td>\n",
              "      <td>-0.572</td>\n",
              "      <td>0.533</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-1.488</td>\n",
              "      <td>0.696</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-1.476</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.636</td>\n",
              "      <td>0.857</td>\n",
              "      <td>-1.796</td>\n",
              "      <td>2.540</td>\n",
              "      <td>0.074</td>\n",
              "      <td>-0.768</td>\n",
              "      <td>-0.901</td>\n",
              "      <td>2.895</td>\n",
              "      <td>0.651</td>\n",
              "      <td>1.006</td>\n",
              "      <td>-0.587</td>\n",
              "      <td>0.208</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>0.231</td>\n",
              "      <td>-1.161</td>\n",
              "      <td>0.849</td>\n",
              "      <td>-0.199</td>\n",
              "      <td>0.882</td>\n",
              "      <td>0.523</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>0.655</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>0.080</td>\n",
              "      <td>-0.384</td>\n",
              "      <td>-0.590</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.622</td>\n",
              "      <td>-0.993</td>\n",
              "      <td>-0.070</td>\n",
              "      <td>-1.060</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.915</td>\n",
              "      <td>-0.517</td>\n",
              "      <td>-1.145</td>\n",
              "      <td>0.325</td>\n",
              "      <td>-1.581</td>\n",
              "      <td>1.838</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.114</td>\n",
              "      <td>-1.583</td>\n",
              "      <td>-1.462</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.817</td>\n",
              "      <td>-0.476</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.830</td>\n",
              "      <td>0.149</td>\n",
              "      <td>-1.661</td>\n",
              "      <td>1.319</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>-0.170</td>\n",
              "      <td>1.130</td>\n",
              "      <td>0.092</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.388</td>\n",
              "      <td>-0.882</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.654</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>-0.657</td>\n",
              "      <td>1.597</td>\n",
              "      <td>1.662</td>\n",
              "      <td>1.260</td>\n",
              "      <td>0.429</td>\n",
              "      <td>-0.558</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.602</td>\n",
              "      <td>0.187</td>\n",
              "      <td>-0.729</td>\n",
              "      <td>2.061</td>\n",
              "      <td>1.288</td>\n",
              "      <td>2.231</td>\n",
              "      <td>1.647</td>\n",
              "      <td>-0.956</td>\n",
              "      <td>0.349</td>\n",
              "      <td>1.189</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>-1.016</td>\n",
              "      <td>1.643</td>\n",
              "      <td>1.136</td>\n",
              "      <td>0.053</td>\n",
              "      <td>-1.081</td>\n",
              "      <td>-0.152</td>\n",
              "      <td>-1.710</td>\n",
              "      <td>-0.234</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-1.004</td>\n",
              "      <td>-2.788</td>\n",
              "      <td>-0.769</td>\n",
              "      <td>-0.530</td>\n",
              "      <td>2.117</td>\n",
              "      <td>-0.829</td>\n",
              "      <td>-0.702</td>\n",
              "      <td>0.422</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>0.488</td>\n",
              "      <td>-1.952</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>-1.468</td>\n",
              "      <td>0.516</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>0.288</td>\n",
              "      <td>0.968</td>\n",
              "      <td>-0.738</td>\n",
              "      <td>-1.636</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>-0.353</td>\n",
              "      <td>0.635</td>\n",
              "      <td>0.386</td>\n",
              "      <td>-1.081</td>\n",
              "      <td>0.161</td>\n",
              "      <td>-0.791</td>\n",
              "      <td>0.948</td>\n",
              "      <td>1.670</td>\n",
              "      <td>-0.309</td>\n",
              "      <td>1.662</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>0.307</td>\n",
              "      <td>-0.220</td>\n",
              "      <td>0.269</td>\n",
              "      <td>1.873</td>\n",
              "      <td>-0.395</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.163</td>\n",
              "      <td>-0.118</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.301</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>-1.181</td>\n",
              "      <td>-0.671</td>\n",
              "      <td>-0.303</td>\n",
              "      <td>-0.541</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>-0.226</td>\n",
              "      <td>0.751</td>\n",
              "      <td>-1.391</td>\n",
              "      <td>-0.906</td>\n",
              "      <td>0.933</td>\n",
              "      <td>0.773</td>\n",
              "      <td>-1.234</td>\n",
              "      <td>-0.967</td>\n",
              "      <td>-0.010</td>\n",
              "      <td>-0.815</td>\n",
              "      <td>1.000</td>\n",
              "      <td>-0.569</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>2.342</td>\n",
              "      <td>0.779</td>\n",
              "      <td>-0.548</td>\n",
              "      <td>-2.330</td>\n",
              "      <td>2.158</td>\n",
              "      <td>2.165</td>\n",
              "      <td>-0.945</td>\n",
              "      <td>-2.269</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.468</td>\n",
              "      <td>-0.405</td>\n",
              "      <td>1.059</td>\n",
              "      <td>0.483</td>\n",
              "      <td>2.470</td>\n",
              "      <td>1.459</td>\n",
              "      <td>-0.511</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>1.074</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>1.086</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>0.432</td>\n",
              "      <td>1.345</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>-1.602</td>\n",
              "      <td>-0.727</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.780</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-1.122</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>2.535</td>\n",
              "      <td>-1.045</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.373</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>1.381</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.444</td>\n",
              "      <td>-1.165</td>\n",
              "      <td>-1.544</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.800</td>\n",
              "      <td>-1.211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-1.637</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-1.035</td>\n",
              "      <td>0.834</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.335</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-1.442</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.836</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>0.716</td>\n",
              "      <td>-0.764</td>\n",
              "      <td>0.248</td>\n",
              "      <td>-1.308</td>\n",
              "      <td>2.127</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.296</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>1.854</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.999</td>\n",
              "      <td>-1.171</td>\n",
              "      <td>2.798</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-1.048</td>\n",
              "      <td>1.078</td>\n",
              "      <td>0.401</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>-0.732</td>\n",
              "      <td>-2.241</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.423</td>\n",
              "      <td>1.070</td>\n",
              "      <td>-0.861</td>\n",
              "      <td>1.320</td>\n",
              "      <td>-0.976</td>\n",
              "      <td>-1.096</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.508</td>\n",
              "      <td>-0.717</td>\n",
              "      <td>-1.133</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>0.645</td>\n",
              "      <td>-1.083</td>\n",
              "      <td>0.287</td>\n",
              "      <td>-0.396</td>\n",
              "      <td>0.178</td>\n",
              "      <td>-0.421</td>\n",
              "      <td>0.196</td>\n",
              "      <td>-0.706</td>\n",
              "      <td>-1.458</td>\n",
              "      <td>1.629</td>\n",
              "      <td>-1.112</td>\n",
              "      <td>-0.479</td>\n",
              "      <td>-0.264</td>\n",
              "      <td>0.205</td>\n",
              "      <td>1.092</td>\n",
              "      <td>0.606</td>\n",
              "      <td>-0.276</td>\n",
              "      <td>1.116</td>\n",
              "      <td>0.272</td>\n",
              "      <td>1.100</td>\n",
              "      <td>-0.811</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.312</td>\n",
              "      <td>1.848</td>\n",
              "      <td>0.455</td>\n",
              "      <td>-0.934</td>\n",
              "      <td>0.739</td>\n",
              "      <td>0.286</td>\n",
              "      <td>-0.860</td>\n",
              "      <td>0.290</td>\n",
              "      <td>1.188</td>\n",
              "      <td>-0.604</td>\n",
              "      <td>1.103</td>\n",
              "      <td>-1.823</td>\n",
              "      <td>0.863</td>\n",
              "      <td>-0.447</td>\n",
              "      <td>-1.108</td>\n",
              "      <td>-1.151</td>\n",
              "      <td>-0.919</td>\n",
              "      <td>-2.284</td>\n",
              "      <td>2.001</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>-0.418</td>\n",
              "      <td>0.281</td>\n",
              "      <td>-0.193</td>\n",
              "      <td>0.764</td>\n",
              "      <td>1.282</td>\n",
              "      <td>-1.478</td>\n",
              "      <td>1.182</td>\n",
              "      <td>2.152</td>\n",
              "      <td>-0.819</td>\n",
              "      <td>-0.520</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.416</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>0.444</td>\n",
              "      <td>-0.152</td>\n",
              "      <td>-0.471</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>0.618</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>0.314</td>\n",
              "      <td>-1.255</td>\n",
              "      <td>-0.334</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.852</td>\n",
              "      <td>1.077</td>\n",
              "      <td>1.738</td>\n",
              "      <td>-0.630</td>\n",
              "      <td>0.935</td>\n",
              "      <td>0.118</td>\n",
              "      <td>-0.195</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.763</td>\n",
              "      <td>0.071</td>\n",
              "      <td>-0.996</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.838</td>\n",
              "      <td>3.270</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>0.614</td>\n",
              "      <td>-0.268</td>\n",
              "      <td>1.652</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.199</td>\n",
              "      <td>-1.286</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.817</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.795</td>\n",
              "      <td>1.020</td>\n",
              "      <td>2.854</td>\n",
              "      <td>-2.404</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-1.334</td>\n",
              "      <td>0.036</td>\n",
              "      <td>-0.578</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.091</td>\n",
              "      <td>-0.824</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.152</td>\n",
              "      <td>1.024</td>\n",
              "      <td>-0.776</td>\n",
              "      <td>1.332</td>\n",
              "      <td>-0.703</td>\n",
              "      <td>1.826</td>\n",
              "      <td>0.005</td>\n",
              "      <td>1.291</td>\n",
              "      <td>0.221</td>\n",
              "      <td>-0.631</td>\n",
              "      <td>-0.958</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>0.216</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.289</td>\n",
              "      <td>0.855</td>\n",
              "      <td>-0.047</td>\n",
              "      <td>-0.173</td>\n",
              "      <td>0.084</td>\n",
              "      <td>-0.066</td>\n",
              "      <td>-1.322</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-1.056</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.092</td>\n",
              "      <td>-0.703</td>\n",
              "      <td>0.847</td>\n",
              "      <td>-0.902</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.862</td>\n",
              "      <td>1.026</td>\n",
              "      <td>0.706</td>\n",
              "      <td>1.089</td>\n",
              "      <td>1.477</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.186</td>\n",
              "      <td>-0.037</td>\n",
              "      <td>-1.730</td>\n",
              "      <td>0.786</td>\n",
              "      <td>0.656</td>\n",
              "      <td>1.259</td>\n",
              "      <td>0.469</td>\n",
              "      <td>-1.561</td>\n",
              "      <td>-0.719</td>\n",
              "      <td>-1.040</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.505</td>\n",
              "      <td>1.410</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.340</td>\n",
              "      <td>-1.029</td>\n",
              "      <td>-1.382</td>\n",
              "      <td>1.350</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.036</td>\n",
              "      <td>-0.640</td>\n",
              "      <td>0.168</td>\n",
              "      <td>1.069</td>\n",
              "      <td>-0.235</td>\n",
              "      <td>0.327</td>\n",
              "      <td>-1.878</td>\n",
              "      <td>0.900</td>\n",
              "      <td>1.059</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>1.006</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.507</td>\n",
              "      <td>0.526</td>\n",
              "      <td>0.899</td>\n",
              "      <td>1.496</td>\n",
              "      <td>-0.447</td>\n",
              "      <td>1.176</td>\n",
              "      <td>1.852</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>1.350</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.795</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>-0.497</td>\n",
              "      <td>0.814</td>\n",
              "      <td>-1.114</td>\n",
              "      <td>-0.800</td>\n",
              "      <td>1.495</td>\n",
              "      <td>-0.591</td>\n",
              "      <td>0.530</td>\n",
              "      <td>-0.528</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>1.251</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.215</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>2.725</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.845</td>\n",
              "      <td>-1.226</td>\n",
              "      <td>1.527</td>\n",
              "      <td>-1.701</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.150</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>1.282</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>-1.574</td>\n",
              "      <td>-1.618</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.640</td>\n",
              "      <td>-0.595</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-0.562</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2.347</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.511</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>1.225</td>\n",
              "      <td>1.594</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1.509</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>2.198</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.453</td>\n",
              "      <td>0.494</td>\n",
              "      <td>1.478</td>\n",
              "      <td>-1.412</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.312</td>\n",
              "      <td>-0.322</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.198</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>1.042</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>1.656</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>-1.437</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>-0.308</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>-1.739</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.336</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>2.371</td>\n",
              "      <td>0.554</td>\n",
              "      <td>1.173</td>\n",
              "      <td>-0.122</td>\n",
              "      <td>1.528</td>\n",
              "      <td>-1.220</td>\n",
              "      <td>2.054</td>\n",
              "      <td>-0.318</td>\n",
              "      <td>-0.445</td>\n",
              "      <td>0.344</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.830</td>\n",
              "      <td>-1.328</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.666</td>\n",
              "      <td>-0.212</td>\n",
              "      <td>-1.016</td>\n",
              "      <td>-0.312</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.807</td>\n",
              "      <td>0.301</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>1.556</td>\n",
              "      <td>1.138</td>\n",
              "      <td>2.066</td>\n",
              "      <td>-0.755</td>\n",
              "      <td>-1.172</td>\n",
              "      <td>0.679</td>\n",
              "      <td>-0.787</td>\n",
              "      <td>0.357</td>\n",
              "      <td>1.626</td>\n",
              "      <td>-0.142</td>\n",
              "      <td>1.717</td>\n",
              "      <td>-1.424</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.732</td>\n",
              "      <td>-0.433</td>\n",
              "      <td>-0.937</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>1.246</td>\n",
              "      <td>-0.930</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.083</td>\n",
              "      <td>-1.058</td>\n",
              "      <td>-0.187</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.054</td>\n",
              "      <td>-0.289</td>\n",
              "      <td>0.663</td>\n",
              "      <td>-1.218</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>1.333</td>\n",
              "      <td>-0.115</td>\n",
              "      <td>0.218</td>\n",
              "      <td>-1.906</td>\n",
              "      <td>0.892</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.527</td>\n",
              "      <td>1.438</td>\n",
              "      <td>0.749</td>\n",
              "      <td>-2.087</td>\n",
              "      <td>0.680</td>\n",
              "      <td>1.515</td>\n",
              "      <td>-0.617</td>\n",
              "      <td>-0.918</td>\n",
              "      <td>-0.243</td>\n",
              "      <td>0.689</td>\n",
              "      <td>-0.465</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.204</td>\n",
              "      <td>1.029</td>\n",
              "      <td>-0.364</td>\n",
              "      <td>-0.708</td>\n",
              "      <td>-0.392</td>\n",
              "      <td>0.011</td>\n",
              "      <td>1.206</td>\n",
              "      <td>1.740</td>\n",
              "      <td>0.040</td>\n",
              "      <td>1.027</td>\n",
              "      <td>2.883</td>\n",
              "      <td>1.037</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.579</td>\n",
              "      <td>-0.170</td>\n",
              "      <td>0.568</td>\n",
              "      <td>-0.731</td>\n",
              "      <td>0.142</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-1.474</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.062</td>\n",
              "      <td>-0.130</td>\n",
              "      <td>0.899</td>\n",
              "      <td>-1.606</td>\n",
              "      <td>-0.992</td>\n",
              "      <td>1.205</td>\n",
              "      <td>1.039</td>\n",
              "      <td>-0.224</td>\n",
              "      <td>1.544</td>\n",
              "      <td>0.936</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-0.465</td>\n",
              "      <td>-0.861</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.511</td>\n",
              "      <td>-1.413</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>-1.383</td>\n",
              "      <td>-1.225</td>\n",
              "      <td>0.850</td>\n",
              "      <td>-0.051</td>\n",
              "      <td>-1.302</td>\n",
              "      <td>0.913</td>\n",
              "      <td>-0.509</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.584</td>\n",
              "      <td>1.409</td>\n",
              "      <td>0.888</td>\n",
              "      <td>-1.372</td>\n",
              "      <td>0.891</td>\n",
              "      <td>1.133</td>\n",
              "      <td>0.360</td>\n",
              "      <td>-0.231</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.134</td>\n",
              "      <td>-0.386</td>\n",
              "      <td>0.597</td>\n",
              "      <td>1.774</td>\n",
              "      <td>0.330</td>\n",
              "      <td>-0.163</td>\n",
              "      <td>1.025</td>\n",
              "      <td>-0.461</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>0.180</td>\n",
              "      <td>-2.080</td>\n",
              "      <td>0.400</td>\n",
              "      <td>-0.557</td>\n",
              "      <td>0.712</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.842</td>\n",
              "      <td>-1.352</td>\n",
              "      <td>1.775</td>\n",
              "      <td>0.982</td>\n",
              "      <td>0.393</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.931</td>\n",
              "      <td>-0.509</td>\n",
              "      <td>0.119</td>\n",
              "      <td>-0.602</td>\n",
              "      <td>1.176</td>\n",
              "      <td>-0.130</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.139</td>\n",
              "      <td>-0.229</td>\n",
              "      <td>-0.456</td>\n",
              "      <td>0.084</td>\n",
              "      <td>-1.221</td>\n",
              "      <td>0.554</td>\n",
              "      <td>-0.137</td>\n",
              "      <td>-0.174</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.648</td>\n",
              "      <td>-0.739</td>\n",
              "      <td>-0.143</td>\n",
              "      <td>0.742</td>\n",
              "      <td>-0.572</td>\n",
              "      <td>-0.369</td>\n",
              "      <td>0.910</td>\n",
              "      <td>-1.806</td>\n",
              "      <td>-0.686</td>\n",
              "      <td>0.093</td>\n",
              "      <td>1.960</td>\n",
              "      <td>-0.413</td>\n",
              "      <td>0.110</td>\n",
              "      <td>-0.657</td>\n",
              "      <td>0.530</td>\n",
              "      <td>-1.003</td>\n",
              "      <td>0.222</td>\n",
              "      <td>1.210</td>\n",
              "      <td>2.099</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.796</td>\n",
              "      <td>0.507</td>\n",
              "      <td>-0.126</td>\n",
              "      <td>-0.660</td>\n",
              "      <td>-0.628</td>\n",
              "      <td>-0.453</td>\n",
              "      <td>0.953</td>\n",
              "      <td>-0.993</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.048</td>\n",
              "      <td>-0.693</td>\n",
              "      <td>-0.492</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>-0.233</td>\n",
              "      <td>-1.096</td>\n",
              "      <td>-0.728</td>\n",
              "      <td>0.842</td>\n",
              "      <td>1.914</td>\n",
              "      <td>1.490</td>\n",
              "      <td>-0.462</td>\n",
              "      <td>-0.767</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>0.169</td>\n",
              "      <td>1.273</td>\n",
              "      <td>-0.160</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.231</td>\n",
              "      <td>-0.906</td>\n",
              "      <td>0.348</td>\n",
              "      <td>-1.050</td>\n",
              "      <td>-0.347</td>\n",
              "      <td>0.904</td>\n",
              "      <td>-1.324</td>\n",
              "      <td>-0.849</td>\n",
              "      <td>3.432</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.174</td>\n",
              "      <td>-1.517</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-1.073</td>\n",
              "      <td>0.325</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-0.883</td>\n",
              "      <td>-1.830</td>\n",
              "      <td>1.408</td>\n",
              "      <td>2.319</td>\n",
              "      <td>1.704</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>1.014</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.096</td>\n",
              "      <td>-0.775</td>\n",
              "      <td>1.845</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.134</td>\n",
              "      <td>2.415</td>\n",
              "      <td>-0.996</td>\n",
              "      <td>-1.006</td>\n",
              "      <td>1.378</td>\n",
              "      <td>1.246</td>\n",
              "      <td>1.478</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target      0      1      2      3      4      5      6      7      8  \\\n",
              "0     1.0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276 -2.246   \n",
              "1     0.0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  0.004   \n",
              "2     1.0 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  0.137   \n",
              "3     1.0  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  0.503   \n",
              "4     1.0  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509 -0.012   \n",
              "\n",
              "       9     10     11     12     13     14     15     16     17     18  \\\n",
              "0  1.825 -0.912 -0.107  0.305  0.102  0.826  0.417  0.177 -0.673 -0.503   \n",
              "1 -0.291  2.907  1.085  2.144  1.540  0.584  1.133  1.098 -0.237 -0.498   \n",
              "2  0.183  0.459  0.478 -0.425  0.352  1.095  0.300 -1.044  0.270 -1.038   \n",
              "3  0.274  0.335 -1.148  0.067 -1.010  1.048 -1.442  0.210  0.836 -0.326   \n",
              "4  2.198  0.190  0.453  0.494  1.478 -1.412  0.270 -1.312 -0.322 -0.688   \n",
              "\n",
              "      19     20     21     22     23     24     25     26     27     28  \\\n",
              "0  1.864  0.410 -1.927  0.102 -0.931  1.763  1.449 -1.097 -0.686 -0.250   \n",
              "1  0.283 -1.100 -0.417  1.382 -0.515 -1.519  0.619 -0.128  0.866 -0.540   \n",
              "2  0.144 -1.658 -0.946  0.633 -0.772  1.786  0.136 -0.103 -1.223  2.273   \n",
              "3  0.716 -0.764  0.248 -1.308  2.127  0.365  0.296 -0.808  1.854  0.118   \n",
              "4 -0.198 -0.285  1.042 -0.315 -0.478  0.024 -0.190  1.656 -0.469 -1.437   \n",
              "\n",
              "      29     30     31     32     33     34     35     36     37     38  \\\n",
              "0 -1.859  1.125  1.009 -2.296  0.385 -0.876  1.528 -0.144 -1.078 -0.403   \n",
              "1  1.238 -0.227  0.269 -0.390 -2.721  1.659  0.106 -0.121  1.719  0.411   \n",
              "2  0.055 -2.032 -0.452  0.064  0.924 -0.692 -0.067 -0.917  1.896 -0.152   \n",
              "3  0.380  0.999 -1.171  2.798  0.394 -1.048  1.078  0.401 -0.486 -0.732   \n",
              "4 -0.581 -0.308 -0.837 -1.739  0.037  0.336 -1.102  2.371  0.554  1.173   \n",
              "\n",
              "      39     40     41     42     43     44     45     46     47     48  \\\n",
              "0  0.005  1.405 -0.044 -0.458  0.579  2.929  0.833  0.761  0.737  0.669   \n",
              "1 -0.303 -0.307  0.380  0.503 -1.320  0.339 -1.102 -0.947  0.267  0.695   \n",
              "2  1.920 -1.244 -1.704  0.167  1.088  0.068  0.972 -1.554  0.218 -2.677   \n",
              "3 -2.241 -0.193  0.336  0.009  0.423  1.070 -0.861  1.320 -0.976 -1.096   \n",
              "4 -0.122  1.528 -1.220  2.054 -0.318 -0.445  0.344  0.161  0.830 -1.328   \n",
              "\n",
              "      49     50     51     52     53     54     55     56     57     58  \\\n",
              "0  0.717 -1.542 -1.847 -0.445  1.238 -0.840 -1.891 -1.531 -0.396 -0.927   \n",
              "1  0.167  0.188 -1.082 -0.872  0.660  0.051  0.303 -0.553 -0.771  0.588   \n",
              "2 -1.528  0.613 -1.269  0.516 -0.714 -0.347 -1.025  1.340  0.923 -0.071   \n",
              "3 -0.912  0.548  0.924  0.053  0.570  0.508 -0.717 -1.133 -0.723  0.645   \n",
              "4  0.420  0.666 -0.212 -1.016 -0.312  0.620  0.807  0.301 -0.342  1.556   \n",
              "\n",
              "      59     60     61     62     63     64     65     66     67     68  \\\n",
              "0  2.072  0.946 -1.105  0.008  0.933 -1.410 -0.770  1.740 -1.504 -0.391   \n",
              "1  0.472  1.315 -0.467 -0.064  1.808  0.633  1.221  1.112  1.133 -0.543   \n",
              "2  0.552  0.837  0.847 -0.807 -0.091  1.424  0.943  0.333  0.593 -0.544   \n",
              "3 -1.083  0.287 -0.396  0.178 -0.421  0.196 -0.706 -1.458  1.629 -1.112   \n",
              "4  1.138  2.066 -0.755 -1.172  0.679 -0.787  0.357  1.626 -0.142  1.717   \n",
              "\n",
              "      69     70     71     72     73     74     75     76     77     78  \\\n",
              "0 -1.551 -1.415 -0.974  0.796 -2.464 -1.424  1.230  0.219  0.130 -0.371   \n",
              "1 -2.144  0.151 -0.813  1.966 -1.190  0.190 -0.473  0.002  1.195 -0.799   \n",
              "2  0.154 -1.081  0.409 -0.964  1.910  0.837 -1.252  1.492 -0.971  0.355   \n",
              "3 -0.479 -0.264  0.205  1.092  0.606 -0.276  1.116  0.272  1.100 -0.811   \n",
              "4 -1.424  0.432  0.732 -0.433 -0.937 -0.473  1.246 -0.930  0.350  0.083   \n",
              "\n",
              "      79     80     81     82     83     84     85     86     87     88  \\\n",
              "0 -0.930  1.851  1.292 -0.380  1.318  1.146 -0.399  2.227  0.447  0.870   \n",
              "1  1.117 -0.759 -0.661  0.406 -0.846 -0.035 -1.634 -0.011  0.503  0.610   \n",
              "2  1.079  0.758 -0.031 -0.101  1.527 -0.942 -0.496 -0.572  0.533  1.020   \n",
              "3  0.037  0.030  0.312  1.848  0.455 -0.934  0.739  0.286 -0.860  0.290   \n",
              "4 -1.058 -0.187 -0.932 -0.054 -0.289  0.663 -1.218 -0.134  1.333 -0.115   \n",
              "\n",
              "      89     90     91     92     93     94     95     96     97     98  \\\n",
              "0  1.420 -1.675  0.019  0.060  0.768  2.563  0.638  1.164  0.407 -1.556   \n",
              "1 -1.822 -0.030  1.188 -0.006 -0.279  1.914  0.620 -1.495  1.787 -0.305   \n",
              "2 -1.488  0.696  0.269 -1.476  0.545  0.636  0.857 -1.796  2.540  0.074   \n",
              "3  1.188 -0.604  1.103 -1.823  0.863 -0.447 -1.108 -1.151 -0.919 -2.284   \n",
              "4  0.218 -1.906  0.892  0.475  0.313  0.518  0.114  0.527  1.438  0.749   \n",
              "\n",
              "      99    100    101    102    103    104    105    106    107    108  \\\n",
              "0 -0.903  1.329  0.452 -0.704  2.218 -1.844  0.158 -1.649 -0.172 -1.167   \n",
              "1  0.602 -1.208  0.893  0.379  1.396  0.581 -0.475 -0.056 -0.691 -0.783   \n",
              "2 -0.768 -0.901  2.895  0.651  1.006 -0.587  0.208 -0.106  0.231 -1.161   \n",
              "3  2.001 -0.546 -1.125 -0.418  0.281 -0.193  0.764  1.282 -1.478  1.182   \n",
              "4 -2.087  0.680  1.515 -0.617 -0.918 -0.243  0.689 -0.465  0.353  0.204   \n",
              "\n",
              "     109    110    111    112    113    114    115    116    117    118  \\\n",
              "0 -1.456 -0.778  0.098 -1.627  0.405 -0.082 -0.797 -0.303  0.710 -0.252   \n",
              "1 -1.485  1.911 -2.400 -2.372 -0.178  1.550 -0.228  0.674  0.987  1.373   \n",
              "2  0.849 -0.199  0.882  0.523 -0.275  0.655 -0.707  0.080 -0.384 -0.590   \n",
              "3  2.152 -0.819 -0.520  0.434  0.589  0.416 -0.455  0.444 -0.152 -0.471   \n",
              "4  1.029 -0.364 -0.708 -0.392  0.011  1.206  1.740  0.040  1.027  2.883   \n",
              "\n",
              "     119    120    121    122    123    124    125    126    127    128  \\\n",
              "0  1.920  0.706 -0.915  0.267 -0.607  0.966 -0.337 -2.292 -1.366 -1.085   \n",
              "1 -0.373  0.629  0.229 -0.630 -0.175  0.548  0.074 -2.090 -0.625 -1.131   \n",
              "2  0.098  0.447  0.622 -0.993 -0.070 -1.060  0.078  0.915 -0.517 -1.145   \n",
              "3 -0.985  0.618 -0.181  0.314 -1.255 -0.334  0.342  0.852  1.077  1.738   \n",
              "4  1.037  0.190  0.579 -0.170  0.568 -0.731  0.142 -0.023 -1.474 -0.482   \n",
              "\n",
              "     129    130    131    132    133    134    135    136    137    138  \\\n",
              "0  0.278  0.212  1.260 -1.276 -2.013 -1.101  0.797  0.661  1.232 -0.632   \n",
              "1  1.111 -0.100  0.574 -0.660 -1.113  0.802 -0.093  1.302 -0.395  0.745   \n",
              "2  0.325 -1.581  1.838 -0.285  0.358  0.114 -1.583 -1.462  0.690  0.817   \n",
              "3 -0.630  0.935  0.118 -0.195  0.394 -0.763  0.071 -0.996  0.011  0.204   \n",
              "4 -0.062 -0.130  0.899 -1.606 -0.992  1.205  1.039 -0.224  1.544  0.936   \n",
              "\n",
              "     139    140    141    142    143    144    145    146    147    148  \\\n",
              "0 -0.805  1.236 -1.085 -0.067 -0.661 -0.745  1.306 -0.010 -0.475 -0.613   \n",
              "1 -0.384  0.066 -0.756  0.495 -0.822  0.135  0.883  0.211 -0.502  2.506   \n",
              "2 -0.476  0.367  0.830  0.149 -1.661  1.319 -1.064 -0.170  1.130  0.092   \n",
              "3  0.838  3.270 -0.912  0.614 -0.268  1.652  0.167  0.199 -1.286  0.334   \n",
              "4 -0.083 -0.465 -0.861  0.472  0.949  0.511 -1.413 -0.952 -1.383 -1.225   \n",
              "\n",
              "     149    150    151    152    153    154    155    156    157    158  \\\n",
              "0 -0.841 -0.837  0.671  2.493  0.689  0.946  0.160 -0.607 -0.775  1.688   \n",
              "1  1.402  1.182 -1.382  0.448 -0.247  0.704  1.558 -0.075  0.609  1.255   \n",
              "2 -0.191  0.247  0.388 -0.882  0.244  0.654 -0.748 -0.657  1.597  1.662   \n",
              "3  0.817  0.534  0.223 -0.795  1.020  2.854 -2.404  0.492  0.040  0.580   \n",
              "4  0.850 -0.051 -1.302  0.913 -0.509  0.289  0.584  1.409  0.888 -1.372   \n",
              "\n",
              "     159    160    161    162    163    164    165    166    167    168  \\\n",
              "0  0.302 -1.156 -0.718  0.126  0.745 -0.287 -0.565  0.646 -0.119 -0.675   \n",
              "1 -1.263  0.613  0.213 -1.395  0.613 -0.865  0.166  0.665 -1.081  0.940   \n",
              "2  1.260  0.429 -0.558 -0.673 -0.602  0.187 -0.729  2.061  1.288  2.231   \n",
              "3  0.148 -1.334  0.036 -0.578 -0.846  1.091 -0.824  0.855  0.152  1.024   \n",
              "4  0.891  1.133  0.360 -0.231  0.300  0.440  0.134 -0.386  0.597  1.774   \n",
              "\n",
              "     169    170    171    172    173    174    175    176    177    178  \\\n",
              "0 -0.479 -0.191 -0.454  1.314  0.740  0.999  1.242 -0.339  0.403 -1.243   \n",
              "1  0.415  0.578 -0.616  0.987  0.274  0.762  0.311  1.832  0.395  1.113   \n",
              "2  1.647 -0.956  0.349  1.189 -0.704 -1.016  1.643  1.136  0.053 -1.081   \n",
              "3 -0.776  1.332 -0.703  1.826  0.005  1.291  0.221 -0.631 -0.958 -0.522   \n",
              "4  0.330 -0.163  1.025 -0.461 -0.248  0.180 -2.080  0.400 -0.557  0.712   \n",
              "\n",
              "     179    180    181    182    183    184    185    186    187    188  \\\n",
              "0  1.365  0.030 -0.475  0.860  0.036  1.313 -0.219  1.078  1.880 -0.317   \n",
              "1 -0.735  0.643  0.727 -0.607 -0.955  0.009  0.680 -0.192  0.859 -0.354   \n",
              "2 -0.152 -1.710 -0.234 -0.083 -1.004 -2.788 -0.769 -0.530  2.117 -0.829   \n",
              "3  0.216 -0.482 -0.289  0.855 -0.047 -0.173  0.084 -0.066 -1.322 -0.455   \n",
              "4  0.054  0.842 -1.352  1.775  0.982  0.393 -0.464  0.931 -0.509  0.119   \n",
              "\n",
              "     189    190    191    192    193    194    195    196    197    198  \\\n",
              "0 -0.443  1.876 -0.611  0.892  1.435 -0.226  0.311  0.139 -0.075  1.381   \n",
              "1 -1.178  1.039 -1.079  0.668  0.995  0.083 -1.411 -0.591  0.742  1.402   \n",
              "2 -0.702  0.422 -0.246  0.488 -1.952 -0.500 -0.533 -1.468  0.516 -0.948   \n",
              "3 -1.056  0.153  0.092 -0.703  0.847 -0.902  0.569  0.862  1.026  0.706   \n",
              "4 -0.602  1.176 -0.130 -0.106  0.350  0.138  0.162  0.139 -0.229 -0.456   \n",
              "\n",
              "     199    200    201    202    203    204    205    206    207    208  \\\n",
              "0  1.716 -2.017 -0.485  1.906 -0.119  0.609 -0.564  0.264 -0.604 -0.733   \n",
              "1 -2.414 -0.551  0.003 -0.344 -1.194 -0.106 -0.679  0.009  0.372  0.025   \n",
              "2  0.288  0.968 -0.738 -1.636 -0.533 -0.353  0.635  0.386 -1.081  0.161   \n",
              "3  1.089  1.477  1.020  0.351  0.186 -0.037 -1.730  0.786  0.656  1.259   \n",
              "4  0.084 -1.221  0.554 -0.137 -0.174  0.567  0.648 -0.739 -0.143  0.742   \n",
              "\n",
              "     209    210    211    212    213    214    215    216    217    218  \\\n",
              "0 -2.352 -1.661  0.498 -0.841  0.907 -0.476  0.817  1.372  1.187  0.844   \n",
              "1  0.066  1.005 -0.822  0.468  0.413  0.004  0.329  1.213  0.216  0.584   \n",
              "2 -0.791  0.948  1.670 -0.309  1.662 -0.053  0.307 -0.220  0.269  1.873   \n",
              "3  0.469 -1.561 -0.719 -1.040  0.142  0.505  1.410  1.042  0.066  0.340   \n",
              "4 -0.572 -0.369  0.910 -1.806 -0.686  0.093  1.960 -0.413  0.110 -0.657   \n",
              "\n",
              "     219    220    221    222    223    224    225    226    227    228  \\\n",
              "0  0.028  0.029 -0.808  0.253  1.005  1.413 -0.133  0.655 -0.921  0.231   \n",
              "1 -0.761 -0.151 -0.175 -0.603  0.007  0.075 -0.354 -0.124  1.299  0.850   \n",
              "2 -0.395  0.186  0.163 -0.118  0.129  0.301 -0.125 -1.181 -0.671 -0.303   \n",
              "3 -1.029 -1.382  1.350  0.294  0.036 -0.640  0.168  1.069 -0.235  0.327   \n",
              "4  0.530 -1.003  0.222  1.210  2.099  0.527  0.128  0.204  0.796  0.507   \n",
              "\n",
              "     229    230    231    232    233    234    235    236    237    238  \\\n",
              "0 -1.902 -0.005 -1.730  1.132 -0.194  0.039  1.489 -0.328  0.966 -0.057   \n",
              "1 -0.318 -0.141  0.154 -0.441 -0.024  0.793 -1.470  0.386 -2.254 -0.463   \n",
              "2 -0.541 -0.285 -0.226  0.751 -1.391 -0.906  0.933  0.773 -1.234 -0.967   \n",
              "3 -1.878  0.900  1.059 -0.458  1.006  0.898  0.955  0.118  0.054  0.347   \n",
              "4 -0.126 -0.660 -0.628 -0.453  0.953 -0.993  0.518  0.055  0.159  0.625   \n",
              "\n",
              "     239    240    241    242    243    244    245    246    247    248  \\\n",
              "0 -0.181  0.723 -0.313 -0.165 -0.803  0.074 -2.851 -1.021 -0.894  0.967   \n",
              "1  0.366 -0.676  0.071  0.504  1.500 -1.160 -0.187 -0.430 -1.151  1.764   \n",
              "2 -0.010 -0.815  1.000 -0.569 -0.486  2.342  0.779 -0.548 -2.330  2.158   \n",
              "3  0.507  0.526  0.899  1.496 -0.447  1.176  1.852 -0.001 -0.414  1.350   \n",
              "4  0.024 -0.048 -0.693 -0.492 -0.670 -0.233 -1.096 -0.728  0.842  1.914   \n",
              "\n",
              "     249    250    251    252    253    254    255    256    257    258  \\\n",
              "0  0.218 -0.692 -0.514  0.754 -1.892  0.203  2.174 -0.755 -1.053 -0.516   \n",
              "1  1.307 -0.731 -1.234  0.960  1.470  0.652  0.483 -2.015 -1.258  0.630   \n",
              "2  2.165 -0.945 -2.269  0.678  0.468 -0.405  1.059  0.483  2.470  1.459   \n",
              "3  0.027  0.795 -0.056 -0.497  0.814 -1.114 -0.800  1.495 -0.591  0.530   \n",
              "4  1.490 -0.462 -0.767 -0.191  0.169  1.273 -0.160  0.393  0.231 -0.906   \n",
              "\n",
              "     259    260    261    262    263    264    265    266    267    268  \\\n",
              "0 -1.109 -0.681  1.250 -0.565 -1.318 -0.923  0.075 -0.704  2.457  0.771   \n",
              "1  1.158  0.971 -1.489  0.530  0.917 -0.094 -1.407  0.887 -0.104 -0.583   \n",
              "2 -0.511 -0.540 -0.299  1.074 -0.748  1.086 -0.766 -0.931  0.432  1.345   \n",
              "3 -0.528 -0.083 -0.831  1.251 -0.206 -0.933 -1.215  0.281  0.512 -0.424   \n",
              "4  0.348 -1.050 -0.347  0.904 -1.324 -0.849  3.432  0.222  0.416  0.174   \n",
              "\n",
              "     269    270    271    272    273    274    275    276    277    278  \\\n",
              "0 -0.460  0.569 -1.320 -1.516 -2.145 -1.120  0.156  0.820 -1.049 -1.125   \n",
              "1  1.267 -1.667 -2.771 -0.516  1.312  0.491  0.932  2.064  0.422  1.215   \n",
              "2 -0.491 -1.602 -0.727  0.346  0.780 -0.527 -1.122 -0.208 -0.730 -0.302   \n",
              "3  0.769  0.223 -0.710  2.725  0.176  0.845 -1.226  1.527 -1.701  0.597   \n",
              "4 -1.517 -0.337  0.055 -0.464  0.014 -1.073  0.325 -0.523 -0.692  0.190   \n",
              "\n",
              "     279    280    281    282    283    284    285    286    287    288  \\\n",
              "0  0.484  0.617  1.253  1.248  0.504 -0.802 -0.896 -1.793 -0.284 -0.601   \n",
              "1  2.012  0.043 -0.307 -0.059  1.121  1.333  0.211  1.753  0.053  1.274   \n",
              "2  2.535 -1.045  0.037  0.020  1.373  0.456 -0.277  1.381  1.843  0.749   \n",
              "3  0.150  1.864  0.322 -0.214  1.282  0.408 -0.910  1.020 -0.299 -1.574   \n",
              "4 -0.883 -1.830  1.408  2.319  1.704 -0.723  1.014  0.064  0.096 -0.775   \n",
              "\n",
              "     289    290    291    292    293    294    295    296    297    298    299  \n",
              "0  0.569  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
              "1 -0.612 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
              "2  0.202  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
              "3 -1.618 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
              "4  1.845  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "KMK5skT62VeV",
        "outputId": "16af6872-f389-4001-aaa5-8c19d7d74888"
      },
      "source": [
        "test_subm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.500</td>\n",
              "      <td>-1.033</td>\n",
              "      <td>-1.595</td>\n",
              "      <td>0.309</td>\n",
              "      <td>-0.714</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.535</td>\n",
              "      <td>-0.129</td>\n",
              "      <td>-0.687</td>\n",
              "      <td>1.291</td>\n",
              "      <td>0.507</td>\n",
              "      <td>-0.317</td>\n",
              "      <td>1.848</td>\n",
              "      <td>-0.232</td>\n",
              "      <td>-0.340</td>\n",
              "      <td>-0.051</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.764</td>\n",
              "      <td>1.860</td>\n",
              "      <td>0.262</td>\n",
              "      <td>1.112</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>-1.039</td>\n",
              "      <td>-0.492</td>\n",
              "      <td>0.183</td>\n",
              "      <td>-0.671</td>\n",
              "      <td>-1.313</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.244</td>\n",
              "      <td>1.072</td>\n",
              "      <td>-1.003</td>\n",
              "      <td>0.832</td>\n",
              "      <td>-1.075</td>\n",
              "      <td>1.988</td>\n",
              "      <td>1.201</td>\n",
              "      <td>-2.065</td>\n",
              "      <td>-0.826</td>\n",
              "      <td>-0.016</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.732</td>\n",
              "      <td>1.235</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>-0.616</td>\n",
              "      <td>0.340</td>\n",
              "      <td>0.788</td>\n",
              "      <td>-0.044</td>\n",
              "      <td>0.305</td>\n",
              "      <td>-0.819</td>\n",
              "      <td>-0.447</td>\n",
              "      <td>-1.625</td>\n",
              "      <td>-1.005</td>\n",
              "      <td>-0.653</td>\n",
              "      <td>-0.371</td>\n",
              "      <td>1.556</td>\n",
              "      <td>0.754</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.645</td>\n",
              "      <td>-0.222</td>\n",
              "      <td>-2.174</td>\n",
              "      <td>-0.610</td>\n",
              "      <td>-1.092</td>\n",
              "      <td>0.917</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>-1.021</td>\n",
              "      <td>-0.179</td>\n",
              "      <td>1.732</td>\n",
              "      <td>-0.366</td>\n",
              "      <td>-1.694</td>\n",
              "      <td>1.038</td>\n",
              "      <td>-0.721</td>\n",
              "      <td>0.112</td>\n",
              "      <td>-0.783</td>\n",
              "      <td>0.940</td>\n",
              "      <td>-1.803</td>\n",
              "      <td>1.295</td>\n",
              "      <td>-1.031</td>\n",
              "      <td>0.452</td>\n",
              "      <td>1.198</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>0.051</td>\n",
              "      <td>-1.055</td>\n",
              "      <td>1.740</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>-0.509</td>\n",
              "      <td>-0.987</td>\n",
              "      <td>-1.011</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.137</td>\n",
              "      <td>-1.585</td>\n",
              "      <td>0.532</td>\n",
              "      <td>-1.201</td>\n",
              "      <td>1.210</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>0.300</td>\n",
              "      <td>-0.110</td>\n",
              "      <td>-0.248</td>\n",
              "      <td>1.464</td>\n",
              "      <td>0.056</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>0.585</td>\n",
              "      <td>-1.263</td>\n",
              "      <td>0.993</td>\n",
              "      <td>-0.639</td>\n",
              "      <td>-1.337</td>\n",
              "      <td>0.225</td>\n",
              "      <td>1.640</td>\n",
              "      <td>-1.807</td>\n",
              "      <td>-1.983</td>\n",
              "      <td>-0.638</td>\n",
              "      <td>-0.432</td>\n",
              "      <td>1.890</td>\n",
              "      <td>0.506</td>\n",
              "      <td>1.091</td>\n",
              "      <td>-0.635</td>\n",
              "      <td>1.072</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-1.294</td>\n",
              "      <td>0.868</td>\n",
              "      <td>0.728</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.540</td>\n",
              "      <td>1.981</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>0.516</td>\n",
              "      <td>-0.373</td>\n",
              "      <td>0.954</td>\n",
              "      <td>-0.854</td>\n",
              "      <td>-1.241</td>\n",
              "      <td>-0.861</td>\n",
              "      <td>0.317</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>-0.179</td>\n",
              "      <td>-1.658</td>\n",
              "      <td>-1.397</td>\n",
              "      <td>-0.988</td>\n",
              "      <td>-0.316</td>\n",
              "      <td>1.808</td>\n",
              "      <td>0.930</td>\n",
              "      <td>1.098</td>\n",
              "      <td>0.997</td>\n",
              "      <td>0.901</td>\n",
              "      <td>1.095</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>2.592</td>\n",
              "      <td>0.846</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.676</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.047</td>\n",
              "      <td>1.265</td>\n",
              "      <td>1.218</td>\n",
              "      <td>1.003</td>\n",
              "      <td>1.219</td>\n",
              "      <td>-0.624</td>\n",
              "      <td>-1.316</td>\n",
              "      <td>0.921</td>\n",
              "      <td>3.472</td>\n",
              "      <td>-1.123</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.927</td>\n",
              "      <td>-0.695</td>\n",
              "      <td>-0.046</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.847</td>\n",
              "      <td>-1.072</td>\n",
              "      <td>0.315</td>\n",
              "      <td>2.522</td>\n",
              "      <td>0.376</td>\n",
              "      <td>-0.015</td>\n",
              "      <td>-1.283</td>\n",
              "      <td>-0.650</td>\n",
              "      <td>-0.106</td>\n",
              "      <td>0.747</td>\n",
              "      <td>0.612</td>\n",
              "      <td>-0.838</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.048</td>\n",
              "      <td>-1.073</td>\n",
              "      <td>0.657</td>\n",
              "      <td>-0.542</td>\n",
              "      <td>-0.269</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>0.329</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.118</td>\n",
              "      <td>-0.164</td>\n",
              "      <td>-0.538</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>1.375</td>\n",
              "      <td>1.194</td>\n",
              "      <td>-0.258</td>\n",
              "      <td>-0.298</td>\n",
              "      <td>1.578</td>\n",
              "      <td>-0.488</td>\n",
              "      <td>1.424</td>\n",
              "      <td>1.106</td>\n",
              "      <td>0.363</td>\n",
              "      <td>-2.007</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>0.551</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.422</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.378</td>\n",
              "      <td>-1.333</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>2.145</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.345</td>\n",
              "      <td>-0.904</td>\n",
              "      <td>0.425</td>\n",
              "      <td>-0.273</td>\n",
              "      <td>0.547</td>\n",
              "      <td>-0.184</td>\n",
              "      <td>0.458</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.540</td>\n",
              "      <td>-1.382</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.131</td>\n",
              "      <td>-0.068</td>\n",
              "      <td>-0.400</td>\n",
              "      <td>0.413</td>\n",
              "      <td>-0.030</td>\n",
              "      <td>0.890</td>\n",
              "      <td>1.000</td>\n",
              "      <td>-0.774</td>\n",
              "      <td>0.340</td>\n",
              "      <td>2.345</td>\n",
              "      <td>2.748</td>\n",
              "      <td>0.774</td>\n",
              "      <td>-0.355</td>\n",
              "      <td>0.574</td>\n",
              "      <td>0.027</td>\n",
              "      <td>1.437</td>\n",
              "      <td>-0.877</td>\n",
              "      <td>0.532</td>\n",
              "      <td>-0.348</td>\n",
              "      <td>0.926</td>\n",
              "      <td>1.308</td>\n",
              "      <td>-0.120</td>\n",
              "      <td>-1.460</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.426</td>\n",
              "      <td>1.667</td>\n",
              "      <td>-0.264</td>\n",
              "      <td>1.266</td>\n",
              "      <td>0.962</td>\n",
              "      <td>1.285</td>\n",
              "      <td>1.176</td>\n",
              "      <td>0.824</td>\n",
              "      <td>0.928</td>\n",
              "      <td>1.372</td>\n",
              "      <td>1.505</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.641</td>\n",
              "      <td>-1.132</td>\n",
              "      <td>1.009</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-1.634</td>\n",
              "      <td>1.046</td>\n",
              "      <td>0.114</td>\n",
              "      <td>-0.806</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.145</td>\n",
              "      <td>-0.684</td>\n",
              "      <td>0.794</td>\n",
              "      <td>-0.290</td>\n",
              "      <td>-1.688</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.140</td>\n",
              "      <td>0.447</td>\n",
              "      <td>-0.616</td>\n",
              "      <td>1.294</td>\n",
              "      <td>0.785</td>\n",
              "      <td>0.453</td>\n",
              "      <td>1.550</td>\n",
              "      <td>-0.866</td>\n",
              "      <td>1.007</td>\n",
              "      <td>-0.088</td>\n",
              "      <td>-2.628</td>\n",
              "      <td>-0.845</td>\n",
              "      <td>2.078</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>2.132</td>\n",
              "      <td>0.609</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>0.312</td>\n",
              "      <td>0.979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.776</td>\n",
              "      <td>0.914</td>\n",
              "      <td>-0.494</td>\n",
              "      <td>1.347</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-0.313</td>\n",
              "      <td>0.203</td>\n",
              "      <td>1.356</td>\n",
              "      <td>-1.086</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.876</td>\n",
              "      <td>-0.563</td>\n",
              "      <td>-1.394</td>\n",
              "      <td>0.385</td>\n",
              "      <td>1.891</td>\n",
              "      <td>-2.107</td>\n",
              "      <td>-0.636</td>\n",
              "      <td>-0.055</td>\n",
              "      <td>-0.843</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.557</td>\n",
              "      <td>0.475</td>\n",
              "      <td>-0.839</td>\n",
              "      <td>-1.146</td>\n",
              "      <td>1.210</td>\n",
              "      <td>1.427</td>\n",
              "      <td>0.347</td>\n",
              "      <td>1.077</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.894</td>\n",
              "      <td>1.190</td>\n",
              "      <td>0.342</td>\n",
              "      <td>-0.858</td>\n",
              "      <td>0.756</td>\n",
              "      <td>1.350</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>0.748</td>\n",
              "      <td>2.014</td>\n",
              "      <td>0.858</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1.343</td>\n",
              "      <td>0.784</td>\n",
              "      <td>-0.418</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>0.694</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>0.559</td>\n",
              "      <td>-0.799</td>\n",
              "      <td>-0.936</td>\n",
              "      <td>1.483</td>\n",
              "      <td>1.670</td>\n",
              "      <td>1.403</td>\n",
              "      <td>0.457</td>\n",
              "      <td>-1.564</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.550</td>\n",
              "      <td>-0.085</td>\n",
              "      <td>-0.561</td>\n",
              "      <td>-0.529</td>\n",
              "      <td>-1.563</td>\n",
              "      <td>-0.781</td>\n",
              "      <td>-0.532</td>\n",
              "      <td>0.375</td>\n",
              "      <td>-0.727</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-0.383</td>\n",
              "      <td>-0.123</td>\n",
              "      <td>1.573</td>\n",
              "      <td>-0.898</td>\n",
              "      <td>-0.070</td>\n",
              "      <td>0.811</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.720</td>\n",
              "      <td>1.691</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.421</td>\n",
              "      <td>-1.665</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.089</td>\n",
              "      <td>2.032</td>\n",
              "      <td>-1.132</td>\n",
              "      <td>-1.827</td>\n",
              "      <td>-0.017</td>\n",
              "      <td>-1.748</td>\n",
              "      <td>-0.717</td>\n",
              "      <td>2.004</td>\n",
              "      <td>1.216</td>\n",
              "      <td>1.547</td>\n",
              "      <td>1.322</td>\n",
              "      <td>0.481</td>\n",
              "      <td>1.819</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>0.617</td>\n",
              "      <td>-0.763</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>-0.847</td>\n",
              "      <td>-0.981</td>\n",
              "      <td>0.274</td>\n",
              "      <td>-1.856</td>\n",
              "      <td>0.808</td>\n",
              "      <td>-0.599</td>\n",
              "      <td>0.998</td>\n",
              "      <td>-0.286</td>\n",
              "      <td>0.649</td>\n",
              "      <td>0.612</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>1.258</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.252</td>\n",
              "      <td>1.093</td>\n",
              "      <td>-0.344</td>\n",
              "      <td>0.349</td>\n",
              "      <td>-1.386</td>\n",
              "      <td>-0.518</td>\n",
              "      <td>1.478</td>\n",
              "      <td>-1.261</td>\n",
              "      <td>2.206</td>\n",
              "      <td>-0.053</td>\n",
              "      <td>-1.191</td>\n",
              "      <td>-1.118</td>\n",
              "      <td>0.402</td>\n",
              "      <td>0.786</td>\n",
              "      <td>-0.345</td>\n",
              "      <td>-0.387</td>\n",
              "      <td>0.558</td>\n",
              "      <td>2.202</td>\n",
              "      <td>-2.056</td>\n",
              "      <td>1.502</td>\n",
              "      <td>1.191</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>-2.183</td>\n",
              "      <td>1.559</td>\n",
              "      <td>1.077</td>\n",
              "      <td>-0.659</td>\n",
              "      <td>0.339</td>\n",
              "      <td>1.113</td>\n",
              "      <td>0.286</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>-1.061</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>1.705</td>\n",
              "      <td>1.239</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.274</td>\n",
              "      <td>-1.454</td>\n",
              "      <td>1.763</td>\n",
              "      <td>-0.450</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>1.415</td>\n",
              "      <td>1.086</td>\n",
              "      <td>-0.007</td>\n",
              "      <td>-1.737</td>\n",
              "      <td>-1.348</td>\n",
              "      <td>0.598</td>\n",
              "      <td>1.876</td>\n",
              "      <td>0.968</td>\n",
              "      <td>-0.643</td>\n",
              "      <td>-0.035</td>\n",
              "      <td>-1.395</td>\n",
              "      <td>-2.041</td>\n",
              "      <td>0.485</td>\n",
              "      <td>-1.763</td>\n",
              "      <td>-0.636</td>\n",
              "      <td>-1.853</td>\n",
              "      <td>0.829</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>-0.573</td>\n",
              "      <td>0.479</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.254</td>\n",
              "      <td>-0.901</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>0.586</td>\n",
              "      <td>-1.795</td>\n",
              "      <td>0.939</td>\n",
              "      <td>-0.073</td>\n",
              "      <td>-0.930</td>\n",
              "      <td>-0.379</td>\n",
              "      <td>-1.023</td>\n",
              "      <td>0.149</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>0.849</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.724</td>\n",
              "      <td>-0.438</td>\n",
              "      <td>0.996</td>\n",
              "      <td>-1.843</td>\n",
              "      <td>-0.511</td>\n",
              "      <td>1.268</td>\n",
              "      <td>0.961</td>\n",
              "      <td>-1.270</td>\n",
              "      <td>-0.426</td>\n",
              "      <td>-1.236</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.860</td>\n",
              "      <td>-1.363</td>\n",
              "      <td>-0.279</td>\n",
              "      <td>-0.556</td>\n",
              "      <td>-2.017</td>\n",
              "      <td>-0.651</td>\n",
              "      <td>-1.192</td>\n",
              "      <td>-0.339</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.416</td>\n",
              "      <td>-0.039</td>\n",
              "      <td>2.421</td>\n",
              "      <td>0.953</td>\n",
              "      <td>1.059</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-0.616</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>1.502</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>-1.196</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.476</td>\n",
              "      <td>-0.271</td>\n",
              "      <td>0.869</td>\n",
              "      <td>-1.596</td>\n",
              "      <td>1.400</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.577</td>\n",
              "      <td>1.222</td>\n",
              "      <td>2.069</td>\n",
              "      <td>-0.820</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.089</td>\n",
              "      <td>-0.939</td>\n",
              "      <td>-0.643</td>\n",
              "      <td>-0.376</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.748</td>\n",
              "      <td>1.493</td>\n",
              "      <td>-2.634</td>\n",
              "      <td>0.368</td>\n",
              "      <td>-0.177</td>\n",
              "      <td>-0.143</td>\n",
              "      <td>0.835</td>\n",
              "      <td>-1.824</td>\n",
              "      <td>-1.452</td>\n",
              "      <td>-0.408</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>0.563</td>\n",
              "      <td>-0.161</td>\n",
              "      <td>-0.494</td>\n",
              "      <td>0.170</td>\n",
              "      <td>-0.257</td>\n",
              "      <td>-1.791</td>\n",
              "      <td>0.122</td>\n",
              "      <td>-0.669</td>\n",
              "      <td>-1.558</td>\n",
              "      <td>-0.244</td>\n",
              "      <td>2.583</td>\n",
              "      <td>-0.829</td>\n",
              "      <td>0.133</td>\n",
              "      <td>-2.746</td>\n",
              "      <td>0.341</td>\n",
              "      <td>-1.145</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.437</td>\n",
              "      <td>-0.628</td>\n",
              "      <td>0.271</td>\n",
              "      <td>2.639</td>\n",
              "      <td>0.481</td>\n",
              "      <td>-0.687</td>\n",
              "      <td>1.017</td>\n",
              "      <td>1.648</td>\n",
              "      <td>-1.272</td>\n",
              "      <td>-0.797</td>\n",
              "      <td>-0.870</td>\n",
              "      <td>-1.582</td>\n",
              "      <td>-1.987</td>\n",
              "      <td>-0.052</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.539</td>\n",
              "      <td>-1.788</td>\n",
              "      <td>-0.433</td>\n",
              "      <td>-0.683</td>\n",
              "      <td>-0.066</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.606</td>\n",
              "      <td>-0.353</td>\n",
              "      <td>-1.133</td>\n",
              "      <td>-3.138</td>\n",
              "      <td>0.281</td>\n",
              "      <td>-0.625</td>\n",
              "      <td>-0.761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.750</td>\n",
              "      <td>0.509</td>\n",
              "      <td>-0.057</td>\n",
              "      <td>0.835</td>\n",
              "      <td>-0.476</td>\n",
              "      <td>1.428</td>\n",
              "      <td>-0.701</td>\n",
              "      <td>-2.009</td>\n",
              "      <td>-1.378</td>\n",
              "      <td>0.167</td>\n",
              "      <td>-0.132</td>\n",
              "      <td>0.459</td>\n",
              "      <td>-0.341</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.184</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>-0.991</td>\n",
              "      <td>-1.039</td>\n",
              "      <td>0.992</td>\n",
              "      <td>1.036</td>\n",
              "      <td>1.552</td>\n",
              "      <td>-0.830</td>\n",
              "      <td>1.374</td>\n",
              "      <td>-0.914</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.327</td>\n",
              "      <td>1.117</td>\n",
              "      <td>0.871</td>\n",
              "      <td>-2.556</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>-0.081</td>\n",
              "      <td>0.744</td>\n",
              "      <td>-1.191</td>\n",
              "      <td>-1.784</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.489</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-1.063</td>\n",
              "      <td>-1.333</td>\n",
              "      <td>1.062</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.984</td>\n",
              "      <td>-0.542</td>\n",
              "      <td>1.295</td>\n",
              "      <td>-1.191</td>\n",
              "      <td>0.755</td>\n",
              "      <td>1.206</td>\n",
              "      <td>-0.558</td>\n",
              "      <td>-1.403</td>\n",
              "      <td>-0.852</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.835</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.640</td>\n",
              "      <td>-1.007</td>\n",
              "      <td>0.268</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>1.019</td>\n",
              "      <td>0.905</td>\n",
              "      <td>1.142</td>\n",
              "      <td>-0.529</td>\n",
              "      <td>0.738</td>\n",
              "      <td>-1.881</td>\n",
              "      <td>-0.857</td>\n",
              "      <td>-1.171</td>\n",
              "      <td>1.057</td>\n",
              "      <td>-2.476</td>\n",
              "      <td>2.686</td>\n",
              "      <td>-2.471</td>\n",
              "      <td>-0.153</td>\n",
              "      <td>0.190</td>\n",
              "      <td>1.063</td>\n",
              "      <td>0.117</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>-0.054</td>\n",
              "      <td>-0.608</td>\n",
              "      <td>-0.333</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.633</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>2.202</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.065</td>\n",
              "      <td>-1.104</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.906</td>\n",
              "      <td>-1.441</td>\n",
              "      <td>0.557</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.706</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.266</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>-0.445</td>\n",
              "      <td>-0.459</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-1.917</td>\n",
              "      <td>-0.202</td>\n",
              "      <td>1.119</td>\n",
              "      <td>-0.490</td>\n",
              "      <td>-0.513</td>\n",
              "      <td>-0.036</td>\n",
              "      <td>-3.083</td>\n",
              "      <td>-0.666</td>\n",
              "      <td>0.089</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>-0.908</td>\n",
              "      <td>0.347</td>\n",
              "      <td>-0.742</td>\n",
              "      <td>0.961</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.015</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.073</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>-0.171</td>\n",
              "      <td>1.598</td>\n",
              "      <td>-0.608</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.447</td>\n",
              "      <td>1.100</td>\n",
              "      <td>1.313</td>\n",
              "      <td>1.454</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>0.543</td>\n",
              "      <td>-0.360</td>\n",
              "      <td>0.294</td>\n",
              "      <td>1.890</td>\n",
              "      <td>1.530</td>\n",
              "      <td>-0.897</td>\n",
              "      <td>-0.139</td>\n",
              "      <td>0.498</td>\n",
              "      <td>1.641</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.888</td>\n",
              "      <td>-1.762</td>\n",
              "      <td>1.653</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.637</td>\n",
              "      <td>0.406</td>\n",
              "      <td>0.963</td>\n",
              "      <td>1.526</td>\n",
              "      <td>0.238</td>\n",
              "      <td>-0.852</td>\n",
              "      <td>-0.880</td>\n",
              "      <td>1.445</td>\n",
              "      <td>0.332</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>1.755</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.750</td>\n",
              "      <td>-0.720</td>\n",
              "      <td>0.086</td>\n",
              "      <td>-0.416</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>-1.114</td>\n",
              "      <td>-0.520</td>\n",
              "      <td>-0.557</td>\n",
              "      <td>1.850</td>\n",
              "      <td>-1.331</td>\n",
              "      <td>-0.166</td>\n",
              "      <td>-0.306</td>\n",
              "      <td>0.170</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>-2.914</td>\n",
              "      <td>0.167</td>\n",
              "      <td>-0.471</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.571</td>\n",
              "      <td>-0.716</td>\n",
              "      <td>0.342</td>\n",
              "      <td>-2.151</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>1.096</td>\n",
              "      <td>0.832</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>-0.149</td>\n",
              "      <td>0.463</td>\n",
              "      <td>1.980</td>\n",
              "      <td>-1.127</td>\n",
              "      <td>0.204</td>\n",
              "      <td>-0.152</td>\n",
              "      <td>0.329</td>\n",
              "      <td>-1.297</td>\n",
              "      <td>-0.847</td>\n",
              "      <td>-0.511</td>\n",
              "      <td>-0.181</td>\n",
              "      <td>-1.060</td>\n",
              "      <td>-0.205</td>\n",
              "      <td>-1.746</td>\n",
              "      <td>-0.371</td>\n",
              "      <td>0.878</td>\n",
              "      <td>-0.885</td>\n",
              "      <td>-1.128</td>\n",
              "      <td>-0.691</td>\n",
              "      <td>1.200</td>\n",
              "      <td>0.065</td>\n",
              "      <td>1.707</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.248</td>\n",
              "      <td>-1.201</td>\n",
              "      <td>-0.480</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.403</td>\n",
              "      <td>-0.228</td>\n",
              "      <td>-1.545</td>\n",
              "      <td>-0.085</td>\n",
              "      <td>0.554</td>\n",
              "      <td>-0.626</td>\n",
              "      <td>-0.751</td>\n",
              "      <td>-0.696</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.059</td>\n",
              "      <td>1.059</td>\n",
              "      <td>1.457</td>\n",
              "      <td>-0.452</td>\n",
              "      <td>-1.058</td>\n",
              "      <td>-0.393</td>\n",
              "      <td>-1.529</td>\n",
              "      <td>1.167</td>\n",
              "      <td>-1.070</td>\n",
              "      <td>-2.563</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.369</td>\n",
              "      <td>0.011</td>\n",
              "      <td>1.589</td>\n",
              "      <td>0.844</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>-0.572</td>\n",
              "      <td>0.558</td>\n",
              "      <td>-0.490</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>-1.651</td>\n",
              "      <td>0.460</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.982</td>\n",
              "      <td>0.123</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>0.034</td>\n",
              "      <td>1.661</td>\n",
              "      <td>-1.134</td>\n",
              "      <td>-0.643</td>\n",
              "      <td>-1.167</td>\n",
              "      <td>1.009</td>\n",
              "      <td>-0.180</td>\n",
              "      <td>-0.683</td>\n",
              "      <td>-1.383</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.268</td>\n",
              "      <td>-1.558</td>\n",
              "      <td>0.620</td>\n",
              "      <td>-0.489</td>\n",
              "      <td>-2.090</td>\n",
              "      <td>-0.977</td>\n",
              "      <td>1.672</td>\n",
              "      <td>-0.655</td>\n",
              "      <td>-0.801</td>\n",
              "      <td>-1.846</td>\n",
              "      <td>0.761</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.962</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>1.450</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.320</td>\n",
              "      <td>-0.951</td>\n",
              "      <td>-2.662</td>\n",
              "      <td>0.761</td>\n",
              "      <td>-0.665</td>\n",
              "      <td>-0.619</td>\n",
              "      <td>-0.645</td>\n",
              "      <td>-0.094</td>\n",
              "      <td>0.351</td>\n",
              "      <td>-0.607</td>\n",
              "      <td>-0.737</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>0.701</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.135</td>\n",
              "      <td>-1.327</td>\n",
              "      <td>2.463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.556</td>\n",
              "      <td>-1.855</td>\n",
              "      <td>-0.682</td>\n",
              "      <td>0.578</td>\n",
              "      <td>1.592</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-1.419</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.511</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.356</td>\n",
              "      <td>-0.060</td>\n",
              "      <td>0.767</td>\n",
              "      <td>-0.196</td>\n",
              "      <td>0.359</td>\n",
              "      <td>0.080</td>\n",
              "      <td>-0.956</td>\n",
              "      <td>0.857</td>\n",
              "      <td>-0.655</td>\n",
              "      <td>-0.090</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>-0.596</td>\n",
              "      <td>-0.413</td>\n",
              "      <td>-1.030</td>\n",
              "      <td>0.173</td>\n",
              "      <td>-0.969</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.790</td>\n",
              "      <td>-0.776</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>-1.995</td>\n",
              "      <td>0.572</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.547</td>\n",
              "      <td>0.307</td>\n",
              "      <td>-0.074</td>\n",
              "      <td>1.703</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>0.818</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.082</td>\n",
              "      <td>-0.374</td>\n",
              "      <td>-0.475</td>\n",
              "      <td>1.488</td>\n",
              "      <td>-0.556</td>\n",
              "      <td>1.975</td>\n",
              "      <td>0.812</td>\n",
              "      <td>-1.838</td>\n",
              "      <td>1.449</td>\n",
              "      <td>2.116</td>\n",
              "      <td>1.988</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>0.264</td>\n",
              "      <td>-0.232</td>\n",
              "      <td>0.974</td>\n",
              "      <td>-2.000</td>\n",
              "      <td>0.072</td>\n",
              "      <td>-1.553</td>\n",
              "      <td>1.145</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>-1.004</td>\n",
              "      <td>1.348</td>\n",
              "      <td>0.412</td>\n",
              "      <td>1.368</td>\n",
              "      <td>0.754</td>\n",
              "      <td>1.275</td>\n",
              "      <td>1.405</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>0.636</td>\n",
              "      <td>-1.180</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.932</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>1.051</td>\n",
              "      <td>-0.220</td>\n",
              "      <td>1.111</td>\n",
              "      <td>0.401</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.560</td>\n",
              "      <td>-0.569</td>\n",
              "      <td>-1.841</td>\n",
              "      <td>0.830</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.106</td>\n",
              "      <td>1.030</td>\n",
              "      <td>-1.244</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-1.649</td>\n",
              "      <td>0.405</td>\n",
              "      <td>-2.060</td>\n",
              "      <td>-0.870</td>\n",
              "      <td>1.206</td>\n",
              "      <td>1.490</td>\n",
              "      <td>-0.981</td>\n",
              "      <td>-0.828</td>\n",
              "      <td>-0.480</td>\n",
              "      <td>-1.525</td>\n",
              "      <td>0.321</td>\n",
              "      <td>-0.069</td>\n",
              "      <td>0.950</td>\n",
              "      <td>-0.571</td>\n",
              "      <td>0.267</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>0.387</td>\n",
              "      <td>-1.875</td>\n",
              "      <td>0.241</td>\n",
              "      <td>1.382</td>\n",
              "      <td>-0.062</td>\n",
              "      <td>1.260</td>\n",
              "      <td>-2.253</td>\n",
              "      <td>1.423</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.639</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.521</td>\n",
              "      <td>-1.518</td>\n",
              "      <td>-0.135</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.977</td>\n",
              "      <td>-0.684</td>\n",
              "      <td>-0.008</td>\n",
              "      <td>1.064</td>\n",
              "      <td>-0.395</td>\n",
              "      <td>0.433</td>\n",
              "      <td>-0.204</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.732</td>\n",
              "      <td>-1.865</td>\n",
              "      <td>0.531</td>\n",
              "      <td>-0.196</td>\n",
              "      <td>-0.885</td>\n",
              "      <td>0.696</td>\n",
              "      <td>-0.546</td>\n",
              "      <td>0.476</td>\n",
              "      <td>-0.157</td>\n",
              "      <td>0.116</td>\n",
              "      <td>1.211</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>0.040</td>\n",
              "      <td>-0.003</td>\n",
              "      <td>-1.189</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.437</td>\n",
              "      <td>-2.337</td>\n",
              "      <td>-0.191</td>\n",
              "      <td>-1.286</td>\n",
              "      <td>-0.531</td>\n",
              "      <td>-1.547</td>\n",
              "      <td>1.166</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>0.499</td>\n",
              "      <td>-0.141</td>\n",
              "      <td>-1.218</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.032</td>\n",
              "      <td>-0.262</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.565</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.157</td>\n",
              "      <td>2.227</td>\n",
              "      <td>0.080</td>\n",
              "      <td>-1.262</td>\n",
              "      <td>-1.249</td>\n",
              "      <td>0.214</td>\n",
              "      <td>-1.035</td>\n",
              "      <td>0.688</td>\n",
              "      <td>1.494</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>-1.506</td>\n",
              "      <td>-1.511</td>\n",
              "      <td>1.656</td>\n",
              "      <td>0.549</td>\n",
              "      <td>-1.825</td>\n",
              "      <td>1.321</td>\n",
              "      <td>-0.497</td>\n",
              "      <td>1.089</td>\n",
              "      <td>-1.172</td>\n",
              "      <td>0.465</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.871</td>\n",
              "      <td>-0.758</td>\n",
              "      <td>-0.100</td>\n",
              "      <td>0.617</td>\n",
              "      <td>2.248</td>\n",
              "      <td>-1.613</td>\n",
              "      <td>0.181</td>\n",
              "      <td>-0.399</td>\n",
              "      <td>1.485</td>\n",
              "      <td>0.871</td>\n",
              "      <td>-0.336</td>\n",
              "      <td>0.349</td>\n",
              "      <td>-1.032</td>\n",
              "      <td>0.728</td>\n",
              "      <td>-0.691</td>\n",
              "      <td>0.936</td>\n",
              "      <td>1.075</td>\n",
              "      <td>0.602</td>\n",
              "      <td>-1.773</td>\n",
              "      <td>-0.550</td>\n",
              "      <td>1.279</td>\n",
              "      <td>-0.793</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-0.394</td>\n",
              "      <td>0.121</td>\n",
              "      <td>-0.544</td>\n",
              "      <td>0.910</td>\n",
              "      <td>1.502</td>\n",
              "      <td>-0.817</td>\n",
              "      <td>0.453</td>\n",
              "      <td>-0.019</td>\n",
              "      <td>-1.556</td>\n",
              "      <td>-0.447</td>\n",
              "      <td>-0.076</td>\n",
              "      <td>-0.309</td>\n",
              "      <td>0.307</td>\n",
              "      <td>-1.386</td>\n",
              "      <td>0.637</td>\n",
              "      <td>-1.150</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.455</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-1.316</td>\n",
              "      <td>-0.274</td>\n",
              "      <td>-2.316</td>\n",
              "      <td>-0.652</td>\n",
              "      <td>-0.652</td>\n",
              "      <td>-0.611</td>\n",
              "      <td>1.744</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.051</td>\n",
              "      <td>-0.256</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>-1.297</td>\n",
              "      <td>-1.636</td>\n",
              "      <td>0.023</td>\n",
              "      <td>-0.872</td>\n",
              "      <td>0.243</td>\n",
              "      <td>1.110</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>-0.483</td>\n",
              "      <td>-0.189</td>\n",
              "      <td>-1.274</td>\n",
              "      <td>0.872</td>\n",
              "      <td>1.181</td>\n",
              "      <td>-0.627</td>\n",
              "      <td>0.827</td>\n",
              "      <td>-1.477</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.620</td>\n",
              "      <td>-1.029</td>\n",
              "      <td>-0.340</td>\n",
              "      <td>0.052</td>\n",
              "      <td>2.122</td>\n",
              "      <td>-0.136</td>\n",
              "      <td>-1.799</td>\n",
              "      <td>1.450</td>\n",
              "      <td>1.866</td>\n",
              "      <td>-0.273</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.207</td>\n",
              "      <td>-0.196</td>\n",
              "      <td>-1.106</td>\n",
              "      <td>-1.560</td>\n",
              "      <td>-0.934</td>\n",
              "      <td>2.167</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.583</td>\n",
              "      <td>1.480</td>\n",
              "      <td>-0.685</td>\n",
              "      <td>-0.473</td>\n",
              "      <td>-1.066</td>\n",
              "      <td>-0.271</td>\n",
              "      <td>0.506</td>\n",
              "      <td>-0.753</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-0.450</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>-1.221</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.336</td>\n",
              "      <td>-0.787</td>\n",
              "      <td>0.255</td>\n",
              "      <td>-0.031</td>\n",
              "      <td>-0.836</td>\n",
              "      <td>0.916</td>\n",
              "      <td>2.411</td>\n",
              "      <td>1.053</td>\n",
              "      <td>-1.601</td>\n",
              "      <td>-1.529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.754</td>\n",
              "      <td>-0.245</td>\n",
              "      <td>1.173</td>\n",
              "      <td>-1.623</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.781</td>\n",
              "      <td>-1.763</td>\n",
              "      <td>-1.432</td>\n",
              "      <td>-0.930</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.293</td>\n",
              "      <td>-0.259</td>\n",
              "      <td>0.030</td>\n",
              "      <td>-0.661</td>\n",
              "      <td>0.921</td>\n",
              "      <td>0.006</td>\n",
              "      <td>-0.631</td>\n",
              "      <td>1.284</td>\n",
              "      <td>-1.167</td>\n",
              "      <td>-0.744</td>\n",
              "      <td>-2.184</td>\n",
              "      <td>2.146</td>\n",
              "      <td>1.130</td>\n",
              "      <td>0.017</td>\n",
              "      <td>1.421</td>\n",
              "      <td>-0.590</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.579</td>\n",
              "      <td>0.521</td>\n",
              "      <td>0.635</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-0.892</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>-0.360</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.346</td>\n",
              "      <td>1.175</td>\n",
              "      <td>-0.252</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.654</td>\n",
              "      <td>0.339</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.751</td>\n",
              "      <td>0.611</td>\n",
              "      <td>-0.052</td>\n",
              "      <td>0.389</td>\n",
              "      <td>-0.426</td>\n",
              "      <td>1.950</td>\n",
              "      <td>1.168</td>\n",
              "      <td>-1.277</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>-1.829</td>\n",
              "      <td>1.521</td>\n",
              "      <td>2.195</td>\n",
              "      <td>0.012</td>\n",
              "      <td>1.258</td>\n",
              "      <td>-1.360</td>\n",
              "      <td>0.770</td>\n",
              "      <td>-0.916</td>\n",
              "      <td>-0.198</td>\n",
              "      <td>-1.210</td>\n",
              "      <td>1.643</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.048</td>\n",
              "      <td>-0.781</td>\n",
              "      <td>0.356</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.211</td>\n",
              "      <td>-1.321</td>\n",
              "      <td>1.749</td>\n",
              "      <td>0.563</td>\n",
              "      <td>0.020</td>\n",
              "      <td>-0.433</td>\n",
              "      <td>-0.742</td>\n",
              "      <td>1.269</td>\n",
              "      <td>-3.389</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>-1.216</td>\n",
              "      <td>-0.968</td>\n",
              "      <td>1.388</td>\n",
              "      <td>0.934</td>\n",
              "      <td>0.022</td>\n",
              "      <td>1.398</td>\n",
              "      <td>-0.571</td>\n",
              "      <td>-0.056</td>\n",
              "      <td>-0.033</td>\n",
              "      <td>-0.294</td>\n",
              "      <td>1.030</td>\n",
              "      <td>-0.972</td>\n",
              "      <td>-0.655</td>\n",
              "      <td>0.304</td>\n",
              "      <td>-0.028</td>\n",
              "      <td>1.155</td>\n",
              "      <td>1.376</td>\n",
              "      <td>1.061</td>\n",
              "      <td>-0.413</td>\n",
              "      <td>-0.174</td>\n",
              "      <td>1.402</td>\n",
              "      <td>-1.082</td>\n",
              "      <td>-1.603</td>\n",
              "      <td>-1.048</td>\n",
              "      <td>-0.087</td>\n",
              "      <td>-0.891</td>\n",
              "      <td>0.392</td>\n",
              "      <td>1.039</td>\n",
              "      <td>0.937</td>\n",
              "      <td>-0.101</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.215</td>\n",
              "      <td>-0.662</td>\n",
              "      <td>0.982</td>\n",
              "      <td>0.081</td>\n",
              "      <td>-0.081</td>\n",
              "      <td>-0.909</td>\n",
              "      <td>0.848</td>\n",
              "      <td>0.228</td>\n",
              "      <td>-0.259</td>\n",
              "      <td>-0.413</td>\n",
              "      <td>2.126</td>\n",
              "      <td>-0.749</td>\n",
              "      <td>0.933</td>\n",
              "      <td>-0.697</td>\n",
              "      <td>-1.659</td>\n",
              "      <td>-0.972</td>\n",
              "      <td>-0.024</td>\n",
              "      <td>1.922</td>\n",
              "      <td>1.842</td>\n",
              "      <td>0.586</td>\n",
              "      <td>1.135</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.643</td>\n",
              "      <td>-0.216</td>\n",
              "      <td>1.284</td>\n",
              "      <td>0.452</td>\n",
              "      <td>1.953</td>\n",
              "      <td>0.944</td>\n",
              "      <td>0.690</td>\n",
              "      <td>1.485</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>-0.213</td>\n",
              "      <td>1.519</td>\n",
              "      <td>0.900</td>\n",
              "      <td>1.384</td>\n",
              "      <td>1.018</td>\n",
              "      <td>0.019</td>\n",
              "      <td>-1.098</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-1.224</td>\n",
              "      <td>-0.901</td>\n",
              "      <td>1.161</td>\n",
              "      <td>1.626</td>\n",
              "      <td>0.786</td>\n",
              "      <td>-0.982</td>\n",
              "      <td>-0.154</td>\n",
              "      <td>-0.134</td>\n",
              "      <td>0.614</td>\n",
              "      <td>0.123</td>\n",
              "      <td>1.467</td>\n",
              "      <td>-1.152</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.134</td>\n",
              "      <td>-1.054</td>\n",
              "      <td>1.811</td>\n",
              "      <td>0.554</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-1.806</td>\n",
              "      <td>-0.746</td>\n",
              "      <td>0.090</td>\n",
              "      <td>2.212</td>\n",
              "      <td>-0.116</td>\n",
              "      <td>1.726</td>\n",
              "      <td>-0.157</td>\n",
              "      <td>1.357</td>\n",
              "      <td>1.235</td>\n",
              "      <td>-0.153</td>\n",
              "      <td>-1.116</td>\n",
              "      <td>1.421</td>\n",
              "      <td>1.472</td>\n",
              "      <td>-0.698</td>\n",
              "      <td>-0.457</td>\n",
              "      <td>-1.393</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.697</td>\n",
              "      <td>-0.076</td>\n",
              "      <td>-0.231</td>\n",
              "      <td>0.306</td>\n",
              "      <td>-0.815</td>\n",
              "      <td>-0.489</td>\n",
              "      <td>-0.493</td>\n",
              "      <td>-0.195</td>\n",
              "      <td>-1.456</td>\n",
              "      <td>-2.235</td>\n",
              "      <td>1.847</td>\n",
              "      <td>1.718</td>\n",
              "      <td>0.562</td>\n",
              "      <td>-0.162</td>\n",
              "      <td>-0.521</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>-1.888</td>\n",
              "      <td>-0.333</td>\n",
              "      <td>0.210</td>\n",
              "      <td>-0.110</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.832</td>\n",
              "      <td>-0.724</td>\n",
              "      <td>0.624</td>\n",
              "      <td>-0.496</td>\n",
              "      <td>-0.196</td>\n",
              "      <td>0.460</td>\n",
              "      <td>0.214</td>\n",
              "      <td>1.192</td>\n",
              "      <td>-0.090</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>0.811</td>\n",
              "      <td>1.154</td>\n",
              "      <td>1.663</td>\n",
              "      <td>-1.142</td>\n",
              "      <td>-1.592</td>\n",
              "      <td>-0.082</td>\n",
              "      <td>0.140</td>\n",
              "      <td>1.414</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.999</td>\n",
              "      <td>-0.270</td>\n",
              "      <td>0.234</td>\n",
              "      <td>-0.047</td>\n",
              "      <td>-1.567</td>\n",
              "      <td>-0.153</td>\n",
              "      <td>-0.273</td>\n",
              "      <td>-1.316</td>\n",
              "      <td>1.161</td>\n",
              "      <td>-1.568</td>\n",
              "      <td>-2.089</td>\n",
              "      <td>0.892</td>\n",
              "      <td>1.123</td>\n",
              "      <td>-0.862</td>\n",
              "      <td>-0.993</td>\n",
              "      <td>1.095</td>\n",
              "      <td>0.266</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>1.304</td>\n",
              "      <td>0.548</td>\n",
              "      <td>-0.654</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-0.073</td>\n",
              "      <td>-0.860</td>\n",
              "      <td>-0.585</td>\n",
              "      <td>2.169</td>\n",
              "      <td>0.141</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>-0.068</td>\n",
              "      <td>-0.534</td>\n",
              "      <td>-1.322</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-0.745</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-0.064</td>\n",
              "      <td>0.738</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>0.745</td>\n",
              "      <td>-0.588</td>\n",
              "      <td>-0.429</td>\n",
              "      <td>-0.588</td>\n",
              "      <td>0.154</td>\n",
              "      <td>-1.187</td>\n",
              "      <td>1.681</td>\n",
              "      <td>-0.832</td>\n",
              "      <td>-0.437</td>\n",
              "      <td>-0.038</td>\n",
              "      <td>-1.096</td>\n",
              "      <td>-0.156</td>\n",
              "      <td>3.565</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>-0.384</td>\n",
              "      <td>1.243</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>1.525</td>\n",
              "      <td>0.458</td>\n",
              "      <td>2.184</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>0.216</td>\n",
              "      <td>1.186</td>\n",
              "      <td>-0.143</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.068</td>\n",
              "      <td>-0.156</td>\n",
              "      <td>-1.153</td>\n",
              "      <td>0.825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0      1      2      3      4      5      6      7      8      9  \\\n",
              "0  0.500 -1.033 -1.595  0.309 -0.714  0.502  0.535 -0.129 -0.687  1.291   \n",
              "1  0.776  0.914 -0.494  1.347 -0.867  0.480  0.578 -0.313  0.203  1.356   \n",
              "2  1.750  0.509 -0.057  0.835 -0.476  1.428 -0.701 -2.009 -1.378  0.167   \n",
              "3 -0.556 -1.855 -0.682  0.578  1.592  0.512 -1.419  0.722  0.511  0.567   \n",
              "4  0.754 -0.245  1.173 -1.623  0.009  0.370  0.781 -1.763 -1.432 -0.930   \n",
              "\n",
              "      10     11     12     13     14     15     16     17     18     19  \\\n",
              "0  0.507 -0.317  1.848 -0.232 -0.340 -0.051  0.804  0.764  1.860  0.262   \n",
              "1 -1.086  0.322  0.876 -0.563 -1.394  0.385  1.891 -2.107 -0.636 -0.055   \n",
              "2 -0.132  0.459 -0.341  0.014  0.184 -0.460 -0.991 -1.039  0.992  1.036   \n",
              "3  0.356 -0.060  0.767 -0.196  0.359  0.080 -0.956  0.857 -0.655 -0.090   \n",
              "4 -0.098  0.896  0.293 -0.259  0.030 -0.661  0.921  0.006 -0.631  1.284   \n",
              "\n",
              "      20     21     22     23     24     25     26     27     28     29  \\\n",
              "0  1.112 -0.491 -1.039 -0.492  0.183 -0.671 -1.313  0.149  0.244  1.072   \n",
              "1 -0.843  0.041  0.253  0.557  0.475 -0.839 -1.146  1.210  1.427  0.347   \n",
              "2  1.552 -0.830  1.374 -0.914  0.427  0.027  0.327  1.117  0.871 -2.556   \n",
              "3 -0.008 -0.596 -0.413 -1.030  0.173 -0.969  0.998  0.079  0.790 -0.776   \n",
              "4 -1.167 -0.744 -2.184  2.146  1.130  0.017  1.421 -0.590  1.938 -0.194   \n",
              "\n",
              "      30     31     32     33     34     35     36     37     38     39  \\\n",
              "0 -1.003  0.832 -1.075  1.988  1.201 -2.065 -0.826 -0.016  0.490  0.191   \n",
              "1  1.077 -0.194  0.323  0.543  0.894  1.190  0.342 -0.858  0.756  1.350   \n",
              "2 -0.036 -0.081  0.744 -1.191 -1.784  0.239  0.500  0.437  0.746  0.999   \n",
              "3 -0.374 -1.995  0.572  0.542  0.547  0.307 -0.074  1.703 -0.003  0.818   \n",
              "4  0.794  0.579  0.521  0.635 -0.023 -0.892 -0.363 -0.360  0.405  0.222   \n",
              "\n",
              "      40     41     42     43     44     45     46     47     48     49  \\\n",
              "0  0.732  1.235 -0.867 -0.616  0.340  0.788 -0.044  0.305 -0.819 -0.447   \n",
              "1 -0.414  0.748  2.014  0.858  0.025  1.343  0.784 -0.418 -0.515  0.694   \n",
              "2  0.489  0.467 -1.063 -1.333  1.062  0.482  0.984 -0.542  1.295 -1.191   \n",
              "3  0.182  0.082 -0.374 -0.475  1.488 -0.556  1.975  0.812 -1.838  1.449   \n",
              "4  0.346  1.175 -0.252  0.767  0.654  0.339  0.481  0.751  0.611 -0.052   \n",
              "\n",
              "      50     51     52     53     54     55     56     57     58     59  \\\n",
              "0 -1.625 -1.005 -0.653 -0.371  1.556  0.754 -0.688  0.061  0.644  0.645   \n",
              "1 -1.097  0.559 -0.799 -0.936  1.483  1.670  1.403  0.457 -1.564  0.049   \n",
              "2  0.755  1.206 -0.558 -1.403 -0.852  0.025  0.835  0.716  0.640 -1.007   \n",
              "3  2.116  1.988 -1.516  0.264 -0.232  0.974 -2.000  0.072 -1.553  1.145   \n",
              "4  0.389 -0.426  1.950  1.168 -1.277 -0.154 -1.829  1.521  2.195  0.012   \n",
              "\n",
              "      60     61     62     63     64     65     66     67     68     69  \\\n",
              "0 -0.222 -2.174 -0.610 -1.092  0.917 -1.010 -1.021 -0.179  1.732 -0.366   \n",
              "1  0.550 -0.085 -0.561 -0.529 -1.563 -0.781 -0.532  0.375 -0.727 -0.053   \n",
              "2  0.268 -1.148  1.019  0.905  1.142 -0.529  0.738 -1.881 -0.857 -1.171   \n",
              "3 -1.038 -1.004  1.348  0.412  1.368  0.754  1.275  1.405 -0.024  0.636   \n",
              "4  1.258 -1.360  0.770 -0.916 -0.198 -1.210  1.643  0.068  0.048 -0.781   \n",
              "\n",
              "      70     71     72     73     74     75     76     77     78     79  \\\n",
              "0 -1.694  1.038 -0.721  0.112 -0.783  0.940 -1.803  1.295 -1.031  0.452   \n",
              "1 -0.383 -0.123  1.573 -0.898 -0.070  0.811 -0.036  0.720  1.691 -0.673   \n",
              "2  1.057 -2.476  2.686 -2.471 -0.153  0.190  1.063  0.117 -1.038 -0.134   \n",
              "3 -1.180  0.506  0.932 -0.246  1.051 -0.220  1.111  0.401  0.502  0.315   \n",
              "4  0.356  0.335  0.211 -1.321  1.749  0.563  0.020 -0.433 -0.742  1.269   \n",
              "\n",
              "      80     81     82     83     84     85     86     87     88     89  \\\n",
              "0  1.198 -0.206  0.051 -1.055  1.740 -0.910 -0.509 -0.987 -1.011  0.718   \n",
              "1 -0.421 -1.665  0.099  0.089  2.032 -1.132 -1.827 -0.017 -1.748 -0.717   \n",
              "2 -1.030 -0.054 -0.608 -0.333  0.184  0.633  0.024 -0.056  2.202  0.434   \n",
              "3  0.560 -0.569 -1.841  0.830  0.543  0.090  0.062  0.106  1.030 -1.244   \n",
              "4 -3.389 -0.291 -1.216 -0.968  1.388  0.934  0.022  1.398 -0.571 -0.056   \n",
              "\n",
              "      90     91     92     93     94     95     96     97     98     99  \\\n",
              "0  0.375  0.101  0.137 -1.585  0.532 -1.201  1.210 -0.374  0.300 -0.110   \n",
              "1  2.004  1.216  1.547  1.322  0.481  1.819 -0.809  0.617 -0.763 -0.154   \n",
              "2  0.065 -1.104 -0.455  0.290  0.906 -1.441  0.557  0.243  0.706 -1.097   \n",
              "3 -0.237 -1.649  0.405 -2.060 -0.870  1.206  1.490 -0.981 -0.828 -0.480   \n",
              "4 -0.033 -0.294  1.030 -0.972 -0.655  0.304 -0.028  1.155  1.376  1.061   \n",
              "\n",
              "     100    101    102    103    104    105    106    107    108    109  \\\n",
              "0 -0.248  1.464  0.056 -0.038 -0.482  0.585 -1.263  0.993 -0.639 -1.337   \n",
              "1 -0.847 -0.981  0.274 -1.856  0.808 -0.599  0.998 -0.286  0.649  0.612   \n",
              "2  0.748  0.266 -0.952 -0.445 -0.459  0.322 -1.917 -0.202  1.119 -0.490   \n",
              "3 -1.525  0.321 -0.069  0.950 -0.571  0.267 -0.191  0.387 -1.875  0.241   \n",
              "4 -0.413 -0.174  1.402 -1.082 -1.603 -1.048 -0.087 -0.891  0.392  1.039   \n",
              "\n",
              "     110    111    112    113    114    115    116    117    118    119  \\\n",
              "0  0.225  1.640 -1.807 -1.983 -0.638 -0.432  1.890  0.506  1.091 -0.635   \n",
              "1 -1.002  1.258  0.149  0.252  1.093 -0.344  0.349 -1.386 -0.518  1.478   \n",
              "2 -0.513 -0.036 -3.083 -0.666  0.089 -0.798 -0.908  0.347 -0.742  0.961   \n",
              "3  1.382 -0.062  1.260 -2.253  1.423  0.524  0.639  0.292  0.521 -1.518   \n",
              "4  0.937 -0.101  0.400  0.215 -0.662  0.982  0.081 -0.081 -0.909  0.848   \n",
              "\n",
              "     120    121    122    123    124    125    126    127    128    129  \\\n",
              "0  1.072  0.262  0.418 -1.294  0.868  0.728  0.618  0.540  1.981 -0.246   \n",
              "1 -1.261  2.206 -0.053 -1.191 -1.118  0.402  0.786 -0.345 -0.387  0.558   \n",
              "2  0.860  0.084  0.015 -0.692  0.195  0.514  0.073 -0.798 -0.171  1.598   \n",
              "3 -0.135  0.210  0.977 -0.684 -0.008  1.064 -0.395  0.433 -0.204 -1.222   \n",
              "4  0.228 -0.259 -0.413  2.126 -0.749  0.933 -0.697 -1.659 -0.972 -0.024   \n",
              "\n",
              "     130    131    132    133    134    135    136    137    138    139  \\\n",
              "0  0.516 -0.373  0.954 -0.854 -1.241 -0.861  0.317 -0.611 -0.179 -1.658   \n",
              "1  2.202 -2.056  1.502  1.191 -0.404 -2.183  1.559  1.077 -0.659  0.339   \n",
              "2 -0.608  0.827  0.447  1.100  1.313  1.454 -1.022  0.543 -0.360  0.294   \n",
              "3  0.732 -1.865  0.531 -0.196 -0.885  0.696 -0.546  0.476 -0.157  0.116   \n",
              "4  1.922  1.842  0.586  1.135  0.050  0.667  0.643 -0.216  1.284  0.452   \n",
              "\n",
              "     140    141    142    143    144    145    146    147    148    149  \\\n",
              "0 -1.397 -0.988 -0.316  1.808  0.930  1.098  0.997  0.901  1.095 -0.464   \n",
              "1  1.113  0.286 -0.424 -1.061 -0.748 -1.148  1.705  1.239 -0.912 -0.692   \n",
              "2  1.890  1.530 -0.897 -0.139  0.498  1.641  0.346  0.888 -1.762  1.653   \n",
              "3  1.211 -0.516  0.040 -0.003 -1.189  0.759  0.326  0.437 -2.337 -0.191   \n",
              "4  1.953  0.944  0.690  1.485 -0.291 -0.213  1.519  0.900  1.384  1.018   \n",
              "\n",
              "     150    151    152    153    154    155    156    157    158    159  \\\n",
              "0  2.592  0.846  1.060 -0.676  0.376  0.047  1.265  1.218  1.003  1.219   \n",
              "1 -0.274 -1.454  1.763 -0.450 -0.725 -0.194  1.415  1.086 -0.007 -1.737   \n",
              "2  0.075 -0.056 -0.469  0.583  0.067  0.637  0.406  0.963  1.526  0.238   \n",
              "3 -1.286 -0.531 -1.547  1.166 -0.730 -0.299  0.499 -0.141 -1.218  0.103   \n",
              "4  0.019 -1.098 -0.725 -1.224 -0.901  1.161  1.626  0.786 -0.982 -0.154   \n",
              "\n",
              "     160    161    162    163    164    165    166    167    168    169  \\\n",
              "0 -0.624 -1.316  0.921  3.472 -1.123  0.037  0.927 -0.695 -0.046  0.270   \n",
              "1 -1.348  0.598  1.876  0.968 -0.643 -0.035 -1.395 -2.041  0.485 -1.763   \n",
              "2 -0.852 -0.880  1.445  0.332 -0.154  1.755  0.479  0.750 -0.720  0.086   \n",
              "3  0.437  0.032 -0.262  0.813  0.565  0.172  0.157  2.227  0.080 -1.262   \n",
              "4 -0.134  0.614  0.123  1.467 -1.152  0.141  0.543  0.726  1.134 -1.054   \n",
              "\n",
              "     170    171    172    173    174    175    176    177    178    179  \\\n",
              "0  0.847 -1.072  0.315  2.522  0.376 -0.015 -1.283 -0.650 -0.106  0.747   \n",
              "1 -0.636 -1.853  0.829 -0.704 -0.573  0.479 -0.670  0.320  0.462  0.254   \n",
              "2 -0.416 -0.300 -1.114 -0.520 -0.557  1.850 -1.331 -0.166 -0.306  0.170   \n",
              "3 -1.249  0.214 -1.035  0.688  1.494 -0.056 -1.506 -1.511  1.656  0.549   \n",
              "4  1.811  0.554 -0.221 -1.806 -0.746  0.090  2.212 -0.116  1.726 -0.157   \n",
              "\n",
              "     180    181    182    183    184    185    186    187    188    189  \\\n",
              "0  0.612 -0.838  0.713  0.048 -1.073  0.657 -0.542 -0.269 -0.482  0.329   \n",
              "1 -0.901 -0.953  0.586 -1.795  0.939 -0.073 -0.930 -0.379 -1.023  0.149   \n",
              "2 -0.256 -2.914  0.167 -0.471  0.185  0.571 -0.716  0.342 -2.151 -0.172   \n",
              "3 -1.825  1.321 -0.497  1.089 -1.172  0.465 -0.056  1.864  0.358  0.871   \n",
              "4  1.357  1.235 -0.153 -1.116  1.421  1.472 -0.698 -0.457 -1.393  0.752   \n",
              "\n",
              "     190    191    192    193    194    195    196    197    198    199  \\\n",
              "0  0.534  0.337  0.118 -0.164 -0.538 -0.285  1.375  1.194 -0.258 -0.298   \n",
              "1 -0.254  0.849  0.629  0.724 -0.438  0.996 -1.843 -0.511  1.268  0.961   \n",
              "2  1.096  0.832 -0.876 -0.149  0.463  1.980 -1.127  0.204 -0.152  0.329   \n",
              "3 -0.758 -0.100  0.617  2.248 -1.613  0.181 -0.399  1.485  0.871 -0.336   \n",
              "4  0.697 -0.076 -0.231  0.306 -0.815 -0.489 -0.493 -0.195 -1.456 -2.235   \n",
              "\n",
              "     200    201    202    203    204    205    206    207    208    209  \\\n",
              "0  1.578 -0.488  1.424  1.106  0.363 -2.007 -0.091  0.551  0.388  0.422   \n",
              "1 -1.270 -0.426 -1.236 -0.036  0.187  0.860 -1.363 -0.279 -0.556 -2.017   \n",
              "2 -1.297 -0.847 -0.511 -0.181 -1.060 -0.205 -1.746 -0.371  0.878 -0.885   \n",
              "3  0.349 -1.032  0.728 -0.691  0.936  1.075  0.602 -1.773 -0.550  1.279   \n",
              "4  1.847  1.718  0.562 -0.162 -0.521 -0.425 -1.888 -0.333  0.210 -0.110   \n",
              "\n",
              "     210    211    212    213    214    215    216    217    218    219  \\\n",
              "0  0.099  0.378 -1.333 -1.102  2.145  0.745  0.345 -0.904  0.425 -0.273   \n",
              "1 -0.651 -1.192 -0.339  0.363  0.416 -0.039  2.421  0.953  1.059  0.512   \n",
              "2 -1.128 -0.691  1.200  0.065  1.707 -0.846  1.248 -1.201 -0.480 -0.953   \n",
              "3 -0.793  0.680  0.263 -0.394  0.121 -0.544  0.910  1.502 -0.817  0.453   \n",
              "4  0.827  0.102 -0.832 -0.724  0.624 -0.496 -0.196  0.460  0.214  1.192   \n",
              "\n",
              "     220    221    222    223    224    225    226    227    228    229  \\\n",
              "0  0.547 -0.184  0.458  0.182  0.592  0.966  0.540 -1.382  0.069  0.131   \n",
              "1 -0.616 -0.172  1.502 -1.078 -1.196  0.042  0.476 -0.271  0.869 -1.596   \n",
              "2  1.403 -0.228 -1.545 -0.085  0.554 -0.626 -0.751 -0.696  0.248  0.059   \n",
              "3 -0.019 -1.556 -0.447 -0.076 -0.309  0.307 -1.386  0.637 -1.150  0.540   \n",
              "4 -0.090 -0.089  0.811  1.154  1.663 -1.142 -1.592 -0.082  0.140  1.414   \n",
              "\n",
              "     230    231    232    233    234    235    236    237    238    239  \\\n",
              "0 -0.068 -0.400  0.413 -0.030  0.890  1.000 -0.774  0.340  2.345  2.748   \n",
              "1  1.400  0.148  0.577  1.222  2.069 -0.820  0.443  0.025  0.089 -0.939   \n",
              "2  1.059  1.457 -0.452 -1.058 -0.393 -1.529  1.167 -1.070 -2.563  0.427   \n",
              "3  0.455 -0.948 -1.316 -0.274 -2.316 -0.652 -0.652 -0.611  1.744  0.260   \n",
              "4  0.047  0.343  0.062  0.999 -0.270  0.234 -0.047 -1.567 -0.153 -0.273   \n",
              "\n",
              "     240    241    242    243    244    245    246    247    248    249  \\\n",
              "0  0.774 -0.355  0.574  0.027  1.437 -0.877  0.532 -0.348  0.926  1.308   \n",
              "1 -0.643 -0.376  0.297  0.352  0.748  1.493 -2.634  0.368 -0.177 -0.143   \n",
              "2  0.369  0.011  1.589  0.844 -0.425 -0.572  0.558 -0.490 -0.424 -1.651   \n",
              "3  0.051 -0.256 -0.296 -1.297 -1.636  0.023 -0.872  0.243  1.110 -0.104   \n",
              "4 -1.316  1.161 -1.568 -2.089  0.892  1.123 -0.862 -0.993  1.095  0.266   \n",
              "\n",
              "     250    251    252    253    254    255    256    257    258    259  \\\n",
              "0 -0.120 -1.460  0.755  0.426  1.667 -0.264  1.266  0.962  1.285  1.176   \n",
              "1  0.835 -1.824 -1.452 -0.408 -0.417  0.563 -0.161 -0.494  0.170 -0.257   \n",
              "2  0.460 -0.581  0.259  0.982  0.123 -0.723  0.034  1.661 -1.134 -0.643   \n",
              "3 -0.483 -0.189 -1.274  0.872  1.181 -0.627  0.827 -1.477  0.322 -0.620   \n",
              "4 -0.455  1.304  0.548 -0.654  0.276 -0.073 -0.860 -0.585  2.169  0.141   \n",
              "\n",
              "     260    261    262    263    264    265    266    267    268    269  \\\n",
              "0  0.824  0.928  1.372  1.505  0.645  0.641 -1.132  1.009  0.998  0.210   \n",
              "1 -1.791  0.122 -0.669 -1.558 -0.244  2.583 -0.829  0.133 -2.746  0.341   \n",
              "2 -1.167  1.009 -0.180 -0.683 -1.383  1.020  0.268 -1.558  0.620 -0.489   \n",
              "3 -1.029 -0.340  0.052  2.122 -0.136 -1.799  1.450  1.866 -0.273 -0.237   \n",
              "4 -0.486 -0.068 -0.534 -1.322  0.500  0.263 -0.745  0.578 -0.064  0.738   \n",
              "\n",
              "     270    271    272    273    274    275    276    277    278    279  \\\n",
              "0 -1.634  1.046  0.114 -0.806  0.301  0.145 -0.684  0.794 -0.290 -1.688   \n",
              "1 -1.145  0.492  0.437 -0.628  0.271  2.639  0.481 -0.687  1.017  1.648   \n",
              "2 -2.090 -0.977  1.672 -0.655 -0.801 -1.846  0.761 -0.846  0.181  0.962   \n",
              "3 -0.207 -0.196 -1.106 -1.560 -0.934  2.167  0.323  0.583  1.480 -0.685   \n",
              "4 -0.280  0.745 -0.588 -0.429 -0.588  0.154 -1.187  1.681 -0.832 -0.437   \n",
              "\n",
              "     280    281    282    283    284    285    286    287    288    289  \\\n",
              "0  0.313  1.140  0.447 -0.616  1.294  0.785  0.453  1.550 -0.866  1.007   \n",
              "1 -1.272 -0.797 -0.870 -1.582 -1.987 -0.052 -0.194  0.539 -1.788 -0.433   \n",
              "2 -0.611  1.450  0.021  0.320 -0.951 -2.662  0.761 -0.665 -0.619 -0.645   \n",
              "3 -0.473 -1.066 -0.271  0.506 -0.753  1.048 -0.450 -0.300 -1.221  0.235   \n",
              "4 -0.038 -1.096 -0.156  3.565 -0.428 -0.384  1.243 -0.966  1.525  0.458   \n",
              "\n",
              "     290    291    292    293    294    295    296    297    298    299  \n",
              "0 -0.088 -2.628 -0.845  2.078 -0.277  2.132  0.609 -0.104  0.312  0.979  \n",
              "1 -0.683 -0.066  0.025  0.606 -0.353 -1.133 -3.138  0.281 -0.625 -0.761  \n",
              "2 -0.094  0.351 -0.607 -0.737 -0.031  0.701  0.976  0.135 -1.327  2.463  \n",
              "3 -0.336 -0.787  0.255 -0.031 -0.836  0.916  2.411  1.053 -1.601 -1.529  \n",
              "4  2.184 -1.090  0.216  1.186 -0.143  0.322 -0.068 -0.156 -1.153  0.825  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ1ZtJRnmJRU",
        "outputId": "51d5c0d8-4853-4511-e74b-c2381cca74fc"
      },
      "source": [
        "print(\n",
        "    \"The train data size (target variable is yet to be dropped): {} \".format(\n",
        "        train_df.shape\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"The test data: {} \".format(test_subm.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train data size (target variable is yet to be dropped): (250, 301) \n",
            "The test data: (19750, 300) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRUwaEiDmLU0",
        "outputId": "edf5bb8f-71d0-46e3-f12c-3a390b405444"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 250 entries, 0 to 249\n",
            "Columns: 301 entries, target to 299\n",
            "dtypes: float64(301)\n",
            "memory usage: 588.0 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELyq1m0LmN46",
        "outputId": "7082a261-6a0f-4da2-e660-6a4994f5a218"
      },
      "source": [
        "test_subm.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19750 entries, 0 to 19749\n",
            "Columns: 300 entries, 0 to 299\n",
            "dtypes: float64(300)\n",
            "memory usage: 45.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj1Qgn0J3z_C"
      },
      "source": [
        "As we can see we have 250 samples in the training set and 197500 samples in the test set, both of which has 300 predictor variables. Moreover, we only have variables of a single data type: float64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "DYDLGw-lmPVU",
        "outputId": "24bb6c1c-6359-4304-aa10-7231de133b5f"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.00000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.00000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.00000</td>\n",
              "      <td>250.00000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.00000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>250.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.023292</td>\n",
              "      <td>-0.026872</td>\n",
              "      <td>0.167404</td>\n",
              "      <td>0.001904</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>-0.007304</td>\n",
              "      <td>0.032052</td>\n",
              "      <td>0.078412</td>\n",
              "      <td>-0.036920</td>\n",
              "      <td>0.035448</td>\n",
              "      <td>-0.005032</td>\n",
              "      <td>0.110248</td>\n",
              "      <td>0.019808</td>\n",
              "      <td>-0.001108</td>\n",
              "      <td>-0.016280</td>\n",
              "      <td>-0.039644</td>\n",
              "      <td>0.017260</td>\n",
              "      <td>-0.106856</td>\n",
              "      <td>0.036184</td>\n",
              "      <td>-0.043296</td>\n",
              "      <td>-0.110832</td>\n",
              "      <td>0.072680</td>\n",
              "      <td>0.017296</td>\n",
              "      <td>-0.030728</td>\n",
              "      <td>-0.128252</td>\n",
              "      <td>0.154736</td>\n",
              "      <td>0.083408</td>\n",
              "      <td>0.039552</td>\n",
              "      <td>-0.091784</td>\n",
              "      <td>0.054636</td>\n",
              "      <td>-0.048288</td>\n",
              "      <td>-0.017296</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>-0.134460</td>\n",
              "      <td>0.093852</td>\n",
              "      <td>-0.020588</td>\n",
              "      <td>-0.002492</td>\n",
              "      <td>-0.141400</td>\n",
              "      <td>-0.061500</td>\n",
              "      <td>-0.043576</td>\n",
              "      <td>0.009136</td>\n",
              "      <td>-0.056824</td>\n",
              "      <td>-0.025968</td>\n",
              "      <td>0.046644</td>\n",
              "      <td>-0.029320</td>\n",
              "      <td>0.048420</td>\n",
              "      <td>-0.030300</td>\n",
              "      <td>-0.054828</td>\n",
              "      <td>0.085188</td>\n",
              "      <td>0.021192</td>\n",
              "      <td>0.031700</td>\n",
              "      <td>0.065468</td>\n",
              "      <td>0.047296</td>\n",
              "      <td>-0.051608</td>\n",
              "      <td>0.045292</td>\n",
              "      <td>-0.038224</td>\n",
              "      <td>0.016220</td>\n",
              "      <td>-0.044704</td>\n",
              "      <td>0.070112</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>0.088100</td>\n",
              "      <td>-0.028832</td>\n",
              "      <td>0.034528</td>\n",
              "      <td>0.064612</td>\n",
              "      <td>-0.074768</td>\n",
              "      <td>0.003796</td>\n",
              "      <td>-0.114680</td>\n",
              "      <td>0.034240</td>\n",
              "      <td>0.006828</td>\n",
              "      <td>-0.059696</td>\n",
              "      <td>0.033832</td>\n",
              "      <td>-0.033364</td>\n",
              "      <td>0.055336</td>\n",
              "      <td>0.005072</td>\n",
              "      <td>-0.117244</td>\n",
              "      <td>0.085096</td>\n",
              "      <td>0.071112</td>\n",
              "      <td>-0.140400</td>\n",
              "      <td>-0.072248</td>\n",
              "      <td>0.021096</td>\n",
              "      <td>0.023664</td>\n",
              "      <td>-0.018056</td>\n",
              "      <td>-0.071508</td>\n",
              "      <td>-0.074868</td>\n",
              "      <td>-0.050976</td>\n",
              "      <td>0.045988</td>\n",
              "      <td>0.023304</td>\n",
              "      <td>-0.013688</td>\n",
              "      <td>0.029780</td>\n",
              "      <td>-0.088448</td>\n",
              "      <td>0.003468</td>\n",
              "      <td>-0.067724</td>\n",
              "      <td>-0.162788</td>\n",
              "      <td>-0.136320</td>\n",
              "      <td>-0.015532</td>\n",
              "      <td>-0.039068</td>\n",
              "      <td>-0.005716</td>\n",
              "      <td>0.006416</td>\n",
              "      <td>-0.027120</td>\n",
              "      <td>0.037784</td>\n",
              "      <td>-0.081212</td>\n",
              "      <td>0.048188</td>\n",
              "      <td>-0.129152</td>\n",
              "      <td>0.084436</td>\n",
              "      <td>0.040980</td>\n",
              "      <td>0.005156</td>\n",
              "      <td>0.082088</td>\n",
              "      <td>-0.018372</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>-0.106732</td>\n",
              "      <td>-0.044920</td>\n",
              "      <td>0.018000</td>\n",
              "      <td>-0.044172</td>\n",
              "      <td>0.050436</td>\n",
              "      <td>0.058124</td>\n",
              "      <td>0.137328</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.007536</td>\n",
              "      <td>-0.002052</td>\n",
              "      <td>-0.116228</td>\n",
              "      <td>0.050988</td>\n",
              "      <td>0.011496</td>\n",
              "      <td>0.055308</td>\n",
              "      <td>0.061228</td>\n",
              "      <td>0.048288</td>\n",
              "      <td>-0.039312</td>\n",
              "      <td>-0.080952</td>\n",
              "      <td>0.002332</td>\n",
              "      <td>-0.014024</td>\n",
              "      <td>-0.146060</td>\n",
              "      <td>-0.066192</td>\n",
              "      <td>0.025044</td>\n",
              "      <td>-0.037212</td>\n",
              "      <td>-0.072808</td>\n",
              "      <td>0.063464</td>\n",
              "      <td>0.008856</td>\n",
              "      <td>-0.036892</td>\n",
              "      <td>-0.034632</td>\n",
              "      <td>-0.017148</td>\n",
              "      <td>0.022340</td>\n",
              "      <td>0.042056</td>\n",
              "      <td>0.018392</td>\n",
              "      <td>0.04880</td>\n",
              "      <td>0.054828</td>\n",
              "      <td>-0.006960</td>\n",
              "      <td>-0.078104</td>\n",
              "      <td>-0.074484</td>\n",
              "      <td>-0.113868</td>\n",
              "      <td>-0.006252</td>\n",
              "      <td>0.089764</td>\n",
              "      <td>-0.057740</td>\n",
              "      <td>-0.060148</td>\n",
              "      <td>0.045800</td>\n",
              "      <td>-0.028772</td>\n",
              "      <td>0.015844</td>\n",
              "      <td>0.014892</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>-0.148604</td>\n",
              "      <td>0.034092</td>\n",
              "      <td>-0.060968</td>\n",
              "      <td>-0.046732</td>\n",
              "      <td>-0.034764</td>\n",
              "      <td>0.042196</td>\n",
              "      <td>-0.011332</td>\n",
              "      <td>-0.042468</td>\n",
              "      <td>-0.022832</td>\n",
              "      <td>-0.008856</td>\n",
              "      <td>0.143128</td>\n",
              "      <td>0.012248</td>\n",
              "      <td>-0.059912</td>\n",
              "      <td>-0.076204</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>-0.011724</td>\n",
              "      <td>0.003372</td>\n",
              "      <td>0.046936</td>\n",
              "      <td>-0.113800</td>\n",
              "      <td>-0.040764</td>\n",
              "      <td>-0.000448</td>\n",
              "      <td>0.017132</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>0.11202</td>\n",
              "      <td>0.056900</td>\n",
              "      <td>0.019344</td>\n",
              "      <td>0.005128</td>\n",
              "      <td>-0.028700</td>\n",
              "      <td>0.016536</td>\n",
              "      <td>0.095228</td>\n",
              "      <td>0.040540</td>\n",
              "      <td>-0.086368</td>\n",
              "      <td>0.018672</td>\n",
              "      <td>-0.005040</td>\n",
              "      <td>-0.027916</td>\n",
              "      <td>0.018764</td>\n",
              "      <td>0.089004</td>\n",
              "      <td>0.069460</td>\n",
              "      <td>-0.002348</td>\n",
              "      <td>-0.046272</td>\n",
              "      <td>-0.004520</td>\n",
              "      <td>-0.093012</td>\n",
              "      <td>-0.079264</td>\n",
              "      <td>-0.030500</td>\n",
              "      <td>-0.007548</td>\n",
              "      <td>0.038812</td>\n",
              "      <td>0.082516</td>\n",
              "      <td>0.046000</td>\n",
              "      <td>0.076528</td>\n",
              "      <td>-0.061012</td>\n",
              "      <td>0.011580</td>\n",
              "      <td>-0.041820</td>\n",
              "      <td>0.025816</td>\n",
              "      <td>-0.053976</td>\n",
              "      <td>-0.043388</td>\n",
              "      <td>0.087544</td>\n",
              "      <td>0.029264</td>\n",
              "      <td>0.032324</td>\n",
              "      <td>0.014004</td>\n",
              "      <td>0.011668</td>\n",
              "      <td>0.041676</td>\n",
              "      <td>-0.070524</td>\n",
              "      <td>0.079772</td>\n",
              "      <td>-0.030328</td>\n",
              "      <td>-0.035720</td>\n",
              "      <td>-0.057568</td>\n",
              "      <td>0.018484</td>\n",
              "      <td>-0.086640</td>\n",
              "      <td>0.038140</td>\n",
              "      <td>0.086800</td>\n",
              "      <td>0.016068</td>\n",
              "      <td>-0.200064</td>\n",
              "      <td>-0.008108</td>\n",
              "      <td>0.019748</td>\n",
              "      <td>-0.058592</td>\n",
              "      <td>0.030492</td>\n",
              "      <td>0.092460</td>\n",
              "      <td>0.07190</td>\n",
              "      <td>-0.02140</td>\n",
              "      <td>0.027644</td>\n",
              "      <td>-0.096620</td>\n",
              "      <td>0.027476</td>\n",
              "      <td>-0.044340</td>\n",
              "      <td>-0.05460</td>\n",
              "      <td>-0.089084</td>\n",
              "      <td>0.022528</td>\n",
              "      <td>0.189628</td>\n",
              "      <td>0.048204</td>\n",
              "      <td>-0.034680</td>\n",
              "      <td>0.114060</td>\n",
              "      <td>0.023388</td>\n",
              "      <td>0.082356</td>\n",
              "      <td>0.120912</td>\n",
              "      <td>0.037056</td>\n",
              "      <td>0.092040</td>\n",
              "      <td>0.023136</td>\n",
              "      <td>-0.078856</td>\n",
              "      <td>-0.026748</td>\n",
              "      <td>-0.019820</td>\n",
              "      <td>-0.025568</td>\n",
              "      <td>-0.033840</td>\n",
              "      <td>-0.037772</td>\n",
              "      <td>0.042652</td>\n",
              "      <td>0.005780</td>\n",
              "      <td>-0.102304</td>\n",
              "      <td>-0.013796</td>\n",
              "      <td>0.089384</td>\n",
              "      <td>0.036368</td>\n",
              "      <td>0.016276</td>\n",
              "      <td>-0.069448</td>\n",
              "      <td>-0.113236</td>\n",
              "      <td>0.035696</td>\n",
              "      <td>0.034484</td>\n",
              "      <td>-0.066236</td>\n",
              "      <td>-0.057988</td>\n",
              "      <td>0.091556</td>\n",
              "      <td>-0.029896</td>\n",
              "      <td>0.115648</td>\n",
              "      <td>0.007372</td>\n",
              "      <td>0.033552</td>\n",
              "      <td>0.090524</td>\n",
              "      <td>0.001576</td>\n",
              "      <td>-0.007784</td>\n",
              "      <td>0.043184</td>\n",
              "      <td>0.082696</td>\n",
              "      <td>0.098476</td>\n",
              "      <td>0.055356</td>\n",
              "      <td>0.111708</td>\n",
              "      <td>-0.015688</td>\n",
              "      <td>0.035992</td>\n",
              "      <td>0.026452</td>\n",
              "      <td>-0.059152</td>\n",
              "      <td>0.077272</td>\n",
              "      <td>0.044652</td>\n",
              "      <td>0.126344</td>\n",
              "      <td>0.018436</td>\n",
              "      <td>-0.012092</td>\n",
              "      <td>-0.065720</td>\n",
              "      <td>-0.106112</td>\n",
              "      <td>0.046472</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>0.009372</td>\n",
              "      <td>-0.128952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.480963</td>\n",
              "      <td>0.998354</td>\n",
              "      <td>1.009314</td>\n",
              "      <td>1.021709</td>\n",
              "      <td>1.011751</td>\n",
              "      <td>1.035411</td>\n",
              "      <td>0.955700</td>\n",
              "      <td>1.006657</td>\n",
              "      <td>0.939731</td>\n",
              "      <td>0.963688</td>\n",
              "      <td>1.019689</td>\n",
              "      <td>1.085089</td>\n",
              "      <td>1.036265</td>\n",
              "      <td>1.050041</td>\n",
              "      <td>1.024305</td>\n",
              "      <td>0.926789</td>\n",
              "      <td>0.955915</td>\n",
              "      <td>1.025655</td>\n",
              "      <td>1.012777</td>\n",
              "      <td>0.945099</td>\n",
              "      <td>1.055935</td>\n",
              "      <td>1.003178</td>\n",
              "      <td>1.039556</td>\n",
              "      <td>0.988482</td>\n",
              "      <td>0.945902</td>\n",
              "      <td>0.997026</td>\n",
              "      <td>0.997894</td>\n",
              "      <td>1.040371</td>\n",
              "      <td>0.922270</td>\n",
              "      <td>1.047282</td>\n",
              "      <td>1.041432</td>\n",
              "      <td>1.010971</td>\n",
              "      <td>0.992464</td>\n",
              "      <td>0.986350</td>\n",
              "      <td>1.015563</td>\n",
              "      <td>1.117898</td>\n",
              "      <td>0.958191</td>\n",
              "      <td>0.948855</td>\n",
              "      <td>1.042429</td>\n",
              "      <td>1.024573</td>\n",
              "      <td>0.996280</td>\n",
              "      <td>0.993367</td>\n",
              "      <td>0.977879</td>\n",
              "      <td>1.035532</td>\n",
              "      <td>0.991244</td>\n",
              "      <td>0.921214</td>\n",
              "      <td>0.973115</td>\n",
              "      <td>0.998897</td>\n",
              "      <td>1.038618</td>\n",
              "      <td>0.984001</td>\n",
              "      <td>1.048054</td>\n",
              "      <td>0.988012</td>\n",
              "      <td>1.055504</td>\n",
              "      <td>1.033105</td>\n",
              "      <td>0.948249</td>\n",
              "      <td>1.010267</td>\n",
              "      <td>1.033381</td>\n",
              "      <td>0.909847</td>\n",
              "      <td>1.033891</td>\n",
              "      <td>0.992619</td>\n",
              "      <td>1.053395</td>\n",
              "      <td>0.973836</td>\n",
              "      <td>0.968131</td>\n",
              "      <td>0.960685</td>\n",
              "      <td>0.924178</td>\n",
              "      <td>1.079216</td>\n",
              "      <td>0.983879</td>\n",
              "      <td>1.070762</td>\n",
              "      <td>0.993845</td>\n",
              "      <td>1.021970</td>\n",
              "      <td>1.016206</td>\n",
              "      <td>0.979236</td>\n",
              "      <td>1.062990</td>\n",
              "      <td>1.021214</td>\n",
              "      <td>1.021526</td>\n",
              "      <td>0.929744</td>\n",
              "      <td>1.015468</td>\n",
              "      <td>0.967391</td>\n",
              "      <td>1.058324</td>\n",
              "      <td>1.035092</td>\n",
              "      <td>0.904238</td>\n",
              "      <td>1.032585</td>\n",
              "      <td>0.951960</td>\n",
              "      <td>0.897700</td>\n",
              "      <td>1.004530</td>\n",
              "      <td>0.936543</td>\n",
              "      <td>1.023601</td>\n",
              "      <td>0.972569</td>\n",
              "      <td>0.962015</td>\n",
              "      <td>0.997142</td>\n",
              "      <td>1.021435</td>\n",
              "      <td>0.968743</td>\n",
              "      <td>1.001998</td>\n",
              "      <td>0.990242</td>\n",
              "      <td>1.018165</td>\n",
              "      <td>1.058821</td>\n",
              "      <td>0.930971</td>\n",
              "      <td>1.025627</td>\n",
              "      <td>1.014966</td>\n",
              "      <td>1.000413</td>\n",
              "      <td>1.029598</td>\n",
              "      <td>0.962005</td>\n",
              "      <td>1.058964</td>\n",
              "      <td>1.008200</td>\n",
              "      <td>1.028253</td>\n",
              "      <td>0.986306</td>\n",
              "      <td>0.980061</td>\n",
              "      <td>1.007852</td>\n",
              "      <td>1.057517</td>\n",
              "      <td>1.012250</td>\n",
              "      <td>0.972992</td>\n",
              "      <td>1.048435</td>\n",
              "      <td>1.094542</td>\n",
              "      <td>0.998305</td>\n",
              "      <td>1.018891</td>\n",
              "      <td>0.972842</td>\n",
              "      <td>1.062220</td>\n",
              "      <td>1.034974</td>\n",
              "      <td>1.074398</td>\n",
              "      <td>1.023247</td>\n",
              "      <td>1.053193</td>\n",
              "      <td>1.003299</td>\n",
              "      <td>0.990817</td>\n",
              "      <td>0.972459</td>\n",
              "      <td>0.936815</td>\n",
              "      <td>1.022176</td>\n",
              "      <td>1.020982</td>\n",
              "      <td>1.013309</td>\n",
              "      <td>1.056450</td>\n",
              "      <td>0.972288</td>\n",
              "      <td>0.912586</td>\n",
              "      <td>1.000926</td>\n",
              "      <td>1.009362</td>\n",
              "      <td>1.071714</td>\n",
              "      <td>1.000910</td>\n",
              "      <td>0.961140</td>\n",
              "      <td>0.996527</td>\n",
              "      <td>0.963481</td>\n",
              "      <td>1.033276</td>\n",
              "      <td>1.013884</td>\n",
              "      <td>0.977589</td>\n",
              "      <td>1.044865</td>\n",
              "      <td>1.067007</td>\n",
              "      <td>0.96140</td>\n",
              "      <td>0.986516</td>\n",
              "      <td>0.975603</td>\n",
              "      <td>1.007537</td>\n",
              "      <td>0.954159</td>\n",
              "      <td>0.989752</td>\n",
              "      <td>0.957323</td>\n",
              "      <td>1.060648</td>\n",
              "      <td>1.053604</td>\n",
              "      <td>0.974275</td>\n",
              "      <td>1.038575</td>\n",
              "      <td>1.056589</td>\n",
              "      <td>1.009334</td>\n",
              "      <td>0.920187</td>\n",
              "      <td>0.974076</td>\n",
              "      <td>1.046082</td>\n",
              "      <td>1.043446</td>\n",
              "      <td>0.989547</td>\n",
              "      <td>0.928692</td>\n",
              "      <td>0.975756</td>\n",
              "      <td>1.068247</td>\n",
              "      <td>0.998016</td>\n",
              "      <td>0.945895</td>\n",
              "      <td>1.037263</td>\n",
              "      <td>1.005934</td>\n",
              "      <td>0.985518</td>\n",
              "      <td>0.889416</td>\n",
              "      <td>0.968491</td>\n",
              "      <td>1.043838</td>\n",
              "      <td>0.986582</td>\n",
              "      <td>0.978644</td>\n",
              "      <td>1.061376</td>\n",
              "      <td>1.020330</td>\n",
              "      <td>0.996019</td>\n",
              "      <td>0.987487</td>\n",
              "      <td>0.977769</td>\n",
              "      <td>1.028095</td>\n",
              "      <td>0.899947</td>\n",
              "      <td>1.02915</td>\n",
              "      <td>0.930588</td>\n",
              "      <td>0.904465</td>\n",
              "      <td>0.969452</td>\n",
              "      <td>1.030067</td>\n",
              "      <td>0.910916</td>\n",
              "      <td>1.006043</td>\n",
              "      <td>1.069322</td>\n",
              "      <td>0.945655</td>\n",
              "      <td>0.991395</td>\n",
              "      <td>1.021503</td>\n",
              "      <td>1.056613</td>\n",
              "      <td>1.042937</td>\n",
              "      <td>0.950353</td>\n",
              "      <td>0.967331</td>\n",
              "      <td>1.003769</td>\n",
              "      <td>1.060352</td>\n",
              "      <td>0.967092</td>\n",
              "      <td>0.989061</td>\n",
              "      <td>1.039407</td>\n",
              "      <td>0.956564</td>\n",
              "      <td>1.057546</td>\n",
              "      <td>0.987352</td>\n",
              "      <td>0.963755</td>\n",
              "      <td>0.932865</td>\n",
              "      <td>1.056818</td>\n",
              "      <td>0.962153</td>\n",
              "      <td>1.005787</td>\n",
              "      <td>0.937099</td>\n",
              "      <td>1.009785</td>\n",
              "      <td>0.945263</td>\n",
              "      <td>0.919409</td>\n",
              "      <td>1.032664</td>\n",
              "      <td>1.043265</td>\n",
              "      <td>1.007421</td>\n",
              "      <td>0.992384</td>\n",
              "      <td>0.962415</td>\n",
              "      <td>1.016528</td>\n",
              "      <td>0.954239</td>\n",
              "      <td>1.002974</td>\n",
              "      <td>0.989978</td>\n",
              "      <td>0.925272</td>\n",
              "      <td>0.989053</td>\n",
              "      <td>1.028179</td>\n",
              "      <td>1.019368</td>\n",
              "      <td>0.996159</td>\n",
              "      <td>0.931904</td>\n",
              "      <td>1.103765</td>\n",
              "      <td>0.987543</td>\n",
              "      <td>1.065948</td>\n",
              "      <td>1.010590</td>\n",
              "      <td>0.948761</td>\n",
              "      <td>0.979871</td>\n",
              "      <td>1.011076</td>\n",
              "      <td>0.98353</td>\n",
              "      <td>0.99673</td>\n",
              "      <td>0.920217</td>\n",
              "      <td>0.994774</td>\n",
              "      <td>1.072805</td>\n",
              "      <td>0.969193</td>\n",
              "      <td>1.01953</td>\n",
              "      <td>0.962420</td>\n",
              "      <td>1.001536</td>\n",
              "      <td>1.077355</td>\n",
              "      <td>1.020979</td>\n",
              "      <td>0.897634</td>\n",
              "      <td>1.039991</td>\n",
              "      <td>1.059544</td>\n",
              "      <td>1.052645</td>\n",
              "      <td>0.949140</td>\n",
              "      <td>1.066471</td>\n",
              "      <td>1.002997</td>\n",
              "      <td>1.029485</td>\n",
              "      <td>1.044701</td>\n",
              "      <td>1.050109</td>\n",
              "      <td>0.973342</td>\n",
              "      <td>1.008486</td>\n",
              "      <td>0.986381</td>\n",
              "      <td>0.990875</td>\n",
              "      <td>1.060948</td>\n",
              "      <td>0.994761</td>\n",
              "      <td>1.094494</td>\n",
              "      <td>1.026025</td>\n",
              "      <td>0.963489</td>\n",
              "      <td>1.026373</td>\n",
              "      <td>1.008207</td>\n",
              "      <td>0.989451</td>\n",
              "      <td>1.002857</td>\n",
              "      <td>0.944743</td>\n",
              "      <td>1.023709</td>\n",
              "      <td>0.985451</td>\n",
              "      <td>0.951879</td>\n",
              "      <td>1.027877</td>\n",
              "      <td>0.966882</td>\n",
              "      <td>1.037173</td>\n",
              "      <td>1.004543</td>\n",
              "      <td>1.006219</td>\n",
              "      <td>1.037119</td>\n",
              "      <td>1.024067</td>\n",
              "      <td>1.056086</td>\n",
              "      <td>1.012516</td>\n",
              "      <td>1.068741</td>\n",
              "      <td>0.934163</td>\n",
              "      <td>0.988100</td>\n",
              "      <td>1.043230</td>\n",
              "      <td>1.010720</td>\n",
              "      <td>1.058982</td>\n",
              "      <td>0.896318</td>\n",
              "      <td>1.113760</td>\n",
              "      <td>0.972530</td>\n",
              "      <td>1.011416</td>\n",
              "      <td>0.972567</td>\n",
              "      <td>0.954229</td>\n",
              "      <td>0.960630</td>\n",
              "      <td>1.057414</td>\n",
              "      <td>1.038389</td>\n",
              "      <td>0.967661</td>\n",
              "      <td>0.998984</td>\n",
              "      <td>1.008099</td>\n",
              "      <td>0.971219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.319000</td>\n",
              "      <td>-2.931000</td>\n",
              "      <td>-2.477000</td>\n",
              "      <td>-2.359000</td>\n",
              "      <td>-2.566000</td>\n",
              "      <td>-2.845000</td>\n",
              "      <td>-2.976000</td>\n",
              "      <td>-3.444000</td>\n",
              "      <td>-2.768000</td>\n",
              "      <td>-2.361000</td>\n",
              "      <td>-3.302000</td>\n",
              "      <td>-2.851000</td>\n",
              "      <td>-2.681000</td>\n",
              "      <td>-2.596000</td>\n",
              "      <td>-3.275000</td>\n",
              "      <td>-3.512000</td>\n",
              "      <td>-2.476000</td>\n",
              "      <td>-3.619000</td>\n",
              "      <td>-2.428000</td>\n",
              "      <td>-3.229000</td>\n",
              "      <td>-3.024000</td>\n",
              "      <td>-2.775000</td>\n",
              "      <td>-2.962000</td>\n",
              "      <td>-2.490000</td>\n",
              "      <td>-3.107000</td>\n",
              "      <td>-2.943000</td>\n",
              "      <td>-2.933000</td>\n",
              "      <td>-2.942000</td>\n",
              "      <td>-2.957000</td>\n",
              "      <td>-2.911000</td>\n",
              "      <td>-2.568000</td>\n",
              "      <td>-2.649000</td>\n",
              "      <td>-3.031000</td>\n",
              "      <td>-2.913000</td>\n",
              "      <td>-3.265000</td>\n",
              "      <td>-2.372000</td>\n",
              "      <td>-3.037000</td>\n",
              "      <td>-3.340000</td>\n",
              "      <td>-2.699000</td>\n",
              "      <td>-3.398000</td>\n",
              "      <td>-2.717000</td>\n",
              "      <td>-2.698000</td>\n",
              "      <td>-2.728000</td>\n",
              "      <td>-2.838000</td>\n",
              "      <td>-2.741000</td>\n",
              "      <td>-2.335000</td>\n",
              "      <td>-2.946000</td>\n",
              "      <td>-2.815000</td>\n",
              "      <td>-2.677000</td>\n",
              "      <td>-2.760000</td>\n",
              "      <td>-2.582000</td>\n",
              "      <td>-2.553000</td>\n",
              "      <td>-2.736000</td>\n",
              "      <td>-3.559000</td>\n",
              "      <td>-3.041000</td>\n",
              "      <td>-3.415000</td>\n",
              "      <td>-2.748000</td>\n",
              "      <td>-2.586000</td>\n",
              "      <td>-2.473000</td>\n",
              "      <td>-2.672000</td>\n",
              "      <td>-2.724000</td>\n",
              "      <td>-3.058000</td>\n",
              "      <td>-2.736000</td>\n",
              "      <td>-2.607000</td>\n",
              "      <td>-2.797000</td>\n",
              "      <td>-2.799000</td>\n",
              "      <td>-2.697000</td>\n",
              "      <td>-3.020000</td>\n",
              "      <td>-2.628000</td>\n",
              "      <td>-2.869000</td>\n",
              "      <td>-2.903000</td>\n",
              "      <td>-2.912000</td>\n",
              "      <td>-3.177000</td>\n",
              "      <td>-2.868000</td>\n",
              "      <td>-2.570000</td>\n",
              "      <td>-2.308000</td>\n",
              "      <td>-3.044000</td>\n",
              "      <td>-3.098000</td>\n",
              "      <td>-3.130000</td>\n",
              "      <td>-1.870000</td>\n",
              "      <td>-2.301000</td>\n",
              "      <td>-2.325000</td>\n",
              "      <td>-2.102000</td>\n",
              "      <td>-2.445000</td>\n",
              "      <td>-2.269000</td>\n",
              "      <td>-2.961000</td>\n",
              "      <td>-3.196000</td>\n",
              "      <td>-2.656000</td>\n",
              "      <td>-2.690000</td>\n",
              "      <td>-2.755000</td>\n",
              "      <td>-3.109000</td>\n",
              "      <td>-2.909000</td>\n",
              "      <td>-2.681000</td>\n",
              "      <td>-3.054000</td>\n",
              "      <td>-2.225000</td>\n",
              "      <td>-2.107000</td>\n",
              "      <td>-2.986000</td>\n",
              "      <td>-2.521000</td>\n",
              "      <td>-2.567000</td>\n",
              "      <td>-2.694000</td>\n",
              "      <td>-3.263000</td>\n",
              "      <td>-2.865000</td>\n",
              "      <td>-3.455000</td>\n",
              "      <td>-3.465000</td>\n",
              "      <td>-2.260000</td>\n",
              "      <td>-2.680000</td>\n",
              "      <td>-2.982000</td>\n",
              "      <td>-2.855000</td>\n",
              "      <td>-2.275000</td>\n",
              "      <td>-3.900000</td>\n",
              "      <td>-2.608000</td>\n",
              "      <td>-3.889000</td>\n",
              "      <td>-2.832000</td>\n",
              "      <td>-3.140000</td>\n",
              "      <td>-2.295000</td>\n",
              "      <td>-2.925000</td>\n",
              "      <td>-2.706000</td>\n",
              "      <td>-4.270000</td>\n",
              "      <td>-2.770000</td>\n",
              "      <td>-2.573000</td>\n",
              "      <td>-2.960000</td>\n",
              "      <td>-2.488000</td>\n",
              "      <td>-2.842000</td>\n",
              "      <td>-2.607000</td>\n",
              "      <td>-2.582000</td>\n",
              "      <td>-2.922000</td>\n",
              "      <td>-4.016000</td>\n",
              "      <td>-2.752000</td>\n",
              "      <td>-2.879000</td>\n",
              "      <td>-3.411000</td>\n",
              "      <td>-2.664000</td>\n",
              "      <td>-2.806000</td>\n",
              "      <td>-2.466000</td>\n",
              "      <td>-3.279000</td>\n",
              "      <td>-2.470000</td>\n",
              "      <td>-2.856000</td>\n",
              "      <td>-2.687000</td>\n",
              "      <td>-2.813000</td>\n",
              "      <td>-3.033000</td>\n",
              "      <td>-2.332000</td>\n",
              "      <td>-2.428000</td>\n",
              "      <td>-3.181000</td>\n",
              "      <td>-2.80600</td>\n",
              "      <td>-2.411000</td>\n",
              "      <td>-2.430000</td>\n",
              "      <td>-3.110000</td>\n",
              "      <td>-2.944000</td>\n",
              "      <td>-2.901000</td>\n",
              "      <td>-2.394000</td>\n",
              "      <td>-3.230000</td>\n",
              "      <td>-3.024000</td>\n",
              "      <td>-2.658000</td>\n",
              "      <td>-3.382000</td>\n",
              "      <td>-2.755000</td>\n",
              "      <td>-2.712000</td>\n",
              "      <td>-3.008000</td>\n",
              "      <td>-2.815000</td>\n",
              "      <td>-3.118000</td>\n",
              "      <td>-2.138000</td>\n",
              "      <td>-2.964000</td>\n",
              "      <td>-2.370000</td>\n",
              "      <td>-2.711000</td>\n",
              "      <td>-2.746000</td>\n",
              "      <td>-2.509000</td>\n",
              "      <td>-2.238000</td>\n",
              "      <td>-3.516000</td>\n",
              "      <td>-3.205000</td>\n",
              "      <td>-2.524000</td>\n",
              "      <td>-2.312000</td>\n",
              "      <td>-2.858000</td>\n",
              "      <td>-2.823000</td>\n",
              "      <td>-3.032000</td>\n",
              "      <td>-2.449000</td>\n",
              "      <td>-3.251000</td>\n",
              "      <td>-2.922000</td>\n",
              "      <td>-2.983000</td>\n",
              "      <td>-2.945000</td>\n",
              "      <td>-2.292000</td>\n",
              "      <td>-2.352000</td>\n",
              "      <td>-2.454000</td>\n",
              "      <td>-2.93000</td>\n",
              "      <td>-2.782000</td>\n",
              "      <td>-2.471000</td>\n",
              "      <td>-3.118000</td>\n",
              "      <td>-3.155000</td>\n",
              "      <td>-2.045000</td>\n",
              "      <td>-3.315000</td>\n",
              "      <td>-3.059000</td>\n",
              "      <td>-2.360000</td>\n",
              "      <td>-2.813000</td>\n",
              "      <td>-3.701000</td>\n",
              "      <td>-3.692000</td>\n",
              "      <td>-2.761000</td>\n",
              "      <td>-2.583000</td>\n",
              "      <td>-2.485000</td>\n",
              "      <td>-2.683000</td>\n",
              "      <td>-3.218000</td>\n",
              "      <td>-2.776000</td>\n",
              "      <td>-2.699000</td>\n",
              "      <td>-3.521000</td>\n",
              "      <td>-2.706000</td>\n",
              "      <td>-2.667000</td>\n",
              "      <td>-3.085000</td>\n",
              "      <td>-2.508000</td>\n",
              "      <td>-2.717000</td>\n",
              "      <td>-2.693000</td>\n",
              "      <td>-3.230000</td>\n",
              "      <td>-2.677000</td>\n",
              "      <td>-2.699000</td>\n",
              "      <td>-2.541000</td>\n",
              "      <td>-2.733000</td>\n",
              "      <td>-2.771000</td>\n",
              "      <td>-2.466000</td>\n",
              "      <td>-2.886000</td>\n",
              "      <td>-2.818000</td>\n",
              "      <td>-2.516000</td>\n",
              "      <td>-2.811000</td>\n",
              "      <td>-2.678000</td>\n",
              "      <td>-2.730000</td>\n",
              "      <td>-2.510000</td>\n",
              "      <td>-2.829000</td>\n",
              "      <td>-3.827000</td>\n",
              "      <td>-2.735000</td>\n",
              "      <td>-2.769000</td>\n",
              "      <td>-2.500000</td>\n",
              "      <td>-3.464000</td>\n",
              "      <td>-2.783000</td>\n",
              "      <td>-3.604000</td>\n",
              "      <td>-2.395000</td>\n",
              "      <td>-2.566000</td>\n",
              "      <td>-2.979000</td>\n",
              "      <td>-2.617000</td>\n",
              "      <td>-3.133000</td>\n",
              "      <td>-2.967000</td>\n",
              "      <td>-3.06200</td>\n",
              "      <td>-3.02700</td>\n",
              "      <td>-2.531000</td>\n",
              "      <td>-3.450000</td>\n",
              "      <td>-2.788000</td>\n",
              "      <td>-2.530000</td>\n",
              "      <td>-2.59500</td>\n",
              "      <td>-2.480000</td>\n",
              "      <td>-2.648000</td>\n",
              "      <td>-2.403000</td>\n",
              "      <td>-3.921000</td>\n",
              "      <td>-2.851000</td>\n",
              "      <td>-2.526000</td>\n",
              "      <td>-3.202000</td>\n",
              "      <td>-2.421000</td>\n",
              "      <td>-2.143000</td>\n",
              "      <td>-3.186000</td>\n",
              "      <td>-2.269000</td>\n",
              "      <td>-2.422000</td>\n",
              "      <td>-2.865000</td>\n",
              "      <td>-3.629000</td>\n",
              "      <td>-2.601000</td>\n",
              "      <td>-2.731000</td>\n",
              "      <td>-3.005000</td>\n",
              "      <td>-2.773000</td>\n",
              "      <td>-2.510000</td>\n",
              "      <td>-2.512000</td>\n",
              "      <td>-2.873000</td>\n",
              "      <td>-2.549000</td>\n",
              "      <td>-2.721000</td>\n",
              "      <td>-2.578000</td>\n",
              "      <td>-2.239000</td>\n",
              "      <td>-3.046000</td>\n",
              "      <td>-2.755000</td>\n",
              "      <td>-2.507000</td>\n",
              "      <td>-3.369000</td>\n",
              "      <td>-2.448000</td>\n",
              "      <td>-2.771000</td>\n",
              "      <td>-2.903000</td>\n",
              "      <td>-2.522000</td>\n",
              "      <td>-2.759000</td>\n",
              "      <td>-2.915000</td>\n",
              "      <td>-2.618000</td>\n",
              "      <td>-3.623000</td>\n",
              "      <td>-2.673000</td>\n",
              "      <td>-3.229000</td>\n",
              "      <td>-2.537000</td>\n",
              "      <td>-2.748000</td>\n",
              "      <td>-2.850000</td>\n",
              "      <td>-2.577000</td>\n",
              "      <td>-2.973000</td>\n",
              "      <td>-2.709000</td>\n",
              "      <td>-3.605000</td>\n",
              "      <td>-2.357000</td>\n",
              "      <td>-2.904000</td>\n",
              "      <td>-2.734000</td>\n",
              "      <td>-2.804000</td>\n",
              "      <td>-2.443000</td>\n",
              "      <td>-2.757000</td>\n",
              "      <td>-2.466000</td>\n",
              "      <td>-3.287000</td>\n",
              "      <td>-3.072000</td>\n",
              "      <td>-2.634000</td>\n",
              "      <td>-2.776000</td>\n",
              "      <td>-3.211000</td>\n",
              "      <td>-3.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.644750</td>\n",
              "      <td>-0.739750</td>\n",
              "      <td>-0.425250</td>\n",
              "      <td>-0.686500</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.643750</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.550750</td>\n",
              "      <td>-0.689500</td>\n",
              "      <td>-0.643500</td>\n",
              "      <td>-0.693500</td>\n",
              "      <td>-0.524000</td>\n",
              "      <td>-0.708500</td>\n",
              "      <td>-0.692000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.634500</td>\n",
              "      <td>-0.683500</td>\n",
              "      <td>-0.801500</td>\n",
              "      <td>-0.574250</td>\n",
              "      <td>-0.758000</td>\n",
              "      <td>-0.870500</td>\n",
              "      <td>-0.596000</td>\n",
              "      <td>-0.725750</td>\n",
              "      <td>-0.652000</td>\n",
              "      <td>-0.779500</td>\n",
              "      <td>-0.424250</td>\n",
              "      <td>-0.585750</td>\n",
              "      <td>-0.625000</td>\n",
              "      <td>-0.751250</td>\n",
              "      <td>-0.582500</td>\n",
              "      <td>-0.713500</td>\n",
              "      <td>-0.750000</td>\n",
              "      <td>-0.588000</td>\n",
              "      <td>-0.829000</td>\n",
              "      <td>-0.648500</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.614000</td>\n",
              "      <td>-0.816750</td>\n",
              "      <td>-0.819500</td>\n",
              "      <td>-0.665250</td>\n",
              "      <td>-0.529500</td>\n",
              "      <td>-0.655500</td>\n",
              "      <td>-0.751250</td>\n",
              "      <td>-0.679750</td>\n",
              "      <td>-0.714500</td>\n",
              "      <td>-0.681750</td>\n",
              "      <td>-0.729250</td>\n",
              "      <td>-0.798500</td>\n",
              "      <td>-0.477000</td>\n",
              "      <td>-0.662250</td>\n",
              "      <td>-0.619000</td>\n",
              "      <td>-0.651250</td>\n",
              "      <td>-0.666750</td>\n",
              "      <td>-0.565500</td>\n",
              "      <td>-0.606500</td>\n",
              "      <td>-0.696750</td>\n",
              "      <td>-0.627750</td>\n",
              "      <td>-0.776250</td>\n",
              "      <td>-0.547250</td>\n",
              "      <td>-0.713750</td>\n",
              "      <td>-0.473750</td>\n",
              "      <td>-0.680750</td>\n",
              "      <td>-0.596500</td>\n",
              "      <td>-0.510250</td>\n",
              "      <td>-0.889250</td>\n",
              "      <td>-0.636000</td>\n",
              "      <td>-0.796250</td>\n",
              "      <td>-0.604500</td>\n",
              "      <td>-0.691750</td>\n",
              "      <td>-0.740250</td>\n",
              "      <td>-0.637500</td>\n",
              "      <td>-0.719750</td>\n",
              "      <td>-0.711250</td>\n",
              "      <td>-0.717250</td>\n",
              "      <td>-0.789250</td>\n",
              "      <td>-0.701250</td>\n",
              "      <td>-0.608500</td>\n",
              "      <td>-0.793750</td>\n",
              "      <td>-0.713000</td>\n",
              "      <td>-0.614500</td>\n",
              "      <td>-0.762750</td>\n",
              "      <td>-0.695000</td>\n",
              "      <td>-0.681750</td>\n",
              "      <td>-0.750000</td>\n",
              "      <td>-0.694000</td>\n",
              "      <td>-0.600750</td>\n",
              "      <td>-0.617250</td>\n",
              "      <td>-0.747000</td>\n",
              "      <td>-0.693500</td>\n",
              "      <td>-0.688500</td>\n",
              "      <td>-0.650750</td>\n",
              "      <td>-0.660000</td>\n",
              "      <td>-0.898500</td>\n",
              "      <td>-0.723750</td>\n",
              "      <td>-0.782250</td>\n",
              "      <td>-0.836500</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.678250</td>\n",
              "      <td>-0.583000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.689500</td>\n",
              "      <td>-0.627500</td>\n",
              "      <td>-0.793500</td>\n",
              "      <td>-0.494500</td>\n",
              "      <td>-0.611000</td>\n",
              "      <td>-0.630750</td>\n",
              "      <td>-0.596750</td>\n",
              "      <td>-0.641500</td>\n",
              "      <td>-0.746750</td>\n",
              "      <td>-0.751750</td>\n",
              "      <td>-0.684250</td>\n",
              "      <td>-0.613500</td>\n",
              "      <td>-0.787500</td>\n",
              "      <td>-0.642000</td>\n",
              "      <td>-0.717000</td>\n",
              "      <td>-0.522000</td>\n",
              "      <td>-0.701500</td>\n",
              "      <td>-0.733500</td>\n",
              "      <td>-0.717750</td>\n",
              "      <td>-0.898750</td>\n",
              "      <td>-0.583500</td>\n",
              "      <td>-0.687000</td>\n",
              "      <td>-0.649500</td>\n",
              "      <td>-0.515000</td>\n",
              "      <td>-0.594750</td>\n",
              "      <td>-0.714500</td>\n",
              "      <td>-0.638500</td>\n",
              "      <td>-0.683250</td>\n",
              "      <td>-0.661250</td>\n",
              "      <td>-0.708750</td>\n",
              "      <td>-0.695250</td>\n",
              "      <td>-0.642250</td>\n",
              "      <td>-0.825500</td>\n",
              "      <td>-0.734000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.677500</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.738500</td>\n",
              "      <td>-0.757500</td>\n",
              "      <td>-0.706500</td>\n",
              "      <td>-0.745000</td>\n",
              "      <td>-0.819500</td>\n",
              "      <td>-0.59350</td>\n",
              "      <td>-0.581500</td>\n",
              "      <td>-0.752750</td>\n",
              "      <td>-0.801250</td>\n",
              "      <td>-0.689750</td>\n",
              "      <td>-0.753250</td>\n",
              "      <td>-0.611750</td>\n",
              "      <td>-0.618000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.703250</td>\n",
              "      <td>-0.704500</td>\n",
              "      <td>-0.764750</td>\n",
              "      <td>-0.644500</td>\n",
              "      <td>-0.571750</td>\n",
              "      <td>-0.677250</td>\n",
              "      <td>-0.870000</td>\n",
              "      <td>-0.787750</td>\n",
              "      <td>-0.719000</td>\n",
              "      <td>-0.693000</td>\n",
              "      <td>-0.588250</td>\n",
              "      <td>-0.628000</td>\n",
              "      <td>-0.771750</td>\n",
              "      <td>-0.752250</td>\n",
              "      <td>-0.785750</td>\n",
              "      <td>-0.728750</td>\n",
              "      <td>-0.554500</td>\n",
              "      <td>-0.617000</td>\n",
              "      <td>-0.692500</td>\n",
              "      <td>-0.831500</td>\n",
              "      <td>-0.683750</td>\n",
              "      <td>-0.713750</td>\n",
              "      <td>-0.724500</td>\n",
              "      <td>-0.692250</td>\n",
              "      <td>-0.703500</td>\n",
              "      <td>-0.703000</td>\n",
              "      <td>-0.629250</td>\n",
              "      <td>-0.720250</td>\n",
              "      <td>-0.623500</td>\n",
              "      <td>-0.53750</td>\n",
              "      <td>-0.534750</td>\n",
              "      <td>-0.618250</td>\n",
              "      <td>-0.710500</td>\n",
              "      <td>-0.641250</td>\n",
              "      <td>-0.597750</td>\n",
              "      <td>-0.500250</td>\n",
              "      <td>-0.649250</td>\n",
              "      <td>-0.733250</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.560250</td>\n",
              "      <td>-0.713500</td>\n",
              "      <td>-0.726000</td>\n",
              "      <td>-0.604000</td>\n",
              "      <td>-0.594250</td>\n",
              "      <td>-0.692250</td>\n",
              "      <td>-0.818750</td>\n",
              "      <td>-0.674500</td>\n",
              "      <td>-0.748000</td>\n",
              "      <td>-0.853750</td>\n",
              "      <td>-0.692000</td>\n",
              "      <td>-0.701500</td>\n",
              "      <td>-0.542500</td>\n",
              "      <td>-0.549750</td>\n",
              "      <td>-0.532750</td>\n",
              "      <td>-0.640750</td>\n",
              "      <td>-0.699250</td>\n",
              "      <td>-0.686500</td>\n",
              "      <td>-0.652750</td>\n",
              "      <td>-0.650500</td>\n",
              "      <td>-0.745750</td>\n",
              "      <td>-0.610750</td>\n",
              "      <td>-0.663500</td>\n",
              "      <td>-0.697750</td>\n",
              "      <td>-0.703750</td>\n",
              "      <td>-0.674750</td>\n",
              "      <td>-0.643000</td>\n",
              "      <td>-0.661500</td>\n",
              "      <td>-0.656500</td>\n",
              "      <td>-0.627250</td>\n",
              "      <td>-0.743500</td>\n",
              "      <td>-0.577250</td>\n",
              "      <td>-0.708250</td>\n",
              "      <td>-0.718250</td>\n",
              "      <td>-0.791500</td>\n",
              "      <td>-0.644750</td>\n",
              "      <td>-0.496000</td>\n",
              "      <td>-0.655750</td>\n",
              "      <td>-0.871500</td>\n",
              "      <td>-0.779000</td>\n",
              "      <td>-0.767500</td>\n",
              "      <td>-0.671500</td>\n",
              "      <td>-0.570500</td>\n",
              "      <td>-0.613000</td>\n",
              "      <td>-0.58175</td>\n",
              "      <td>-0.62500</td>\n",
              "      <td>-0.598500</td>\n",
              "      <td>-0.756000</td>\n",
              "      <td>-0.746750</td>\n",
              "      <td>-0.656000</td>\n",
              "      <td>-0.75700</td>\n",
              "      <td>-0.675750</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.553000</td>\n",
              "      <td>-0.628500</td>\n",
              "      <td>-0.683500</td>\n",
              "      <td>-0.577500</td>\n",
              "      <td>-0.741750</td>\n",
              "      <td>-0.638500</td>\n",
              "      <td>-0.553000</td>\n",
              "      <td>-0.675750</td>\n",
              "      <td>-0.629500</td>\n",
              "      <td>-0.644500</td>\n",
              "      <td>-0.778250</td>\n",
              "      <td>-0.727750</td>\n",
              "      <td>-0.713500</td>\n",
              "      <td>-0.734000</td>\n",
              "      <td>-0.637750</td>\n",
              "      <td>-0.731500</td>\n",
              "      <td>-0.643250</td>\n",
              "      <td>-0.622750</td>\n",
              "      <td>-1.009250</td>\n",
              "      <td>-0.693250</td>\n",
              "      <td>-0.567750</td>\n",
              "      <td>-0.696500</td>\n",
              "      <td>-0.684000</td>\n",
              "      <td>-0.703750</td>\n",
              "      <td>-0.771250</td>\n",
              "      <td>-0.624500</td>\n",
              "      <td>-0.653000</td>\n",
              "      <td>-0.786750</td>\n",
              "      <td>-0.701000</td>\n",
              "      <td>-0.543250</td>\n",
              "      <td>-0.672750</td>\n",
              "      <td>-0.626750</td>\n",
              "      <td>-0.730250</td>\n",
              "      <td>-0.649750</td>\n",
              "      <td>-0.589500</td>\n",
              "      <td>-0.725750</td>\n",
              "      <td>-0.667750</td>\n",
              "      <td>-0.605000</td>\n",
              "      <td>-0.637750</td>\n",
              "      <td>-0.458250</td>\n",
              "      <td>-0.553500</td>\n",
              "      <td>-0.566750</td>\n",
              "      <td>-0.778250</td>\n",
              "      <td>-0.693250</td>\n",
              "      <td>-0.596750</td>\n",
              "      <td>-0.789000</td>\n",
              "      <td>-0.671250</td>\n",
              "      <td>-0.617000</td>\n",
              "      <td>-0.510500</td>\n",
              "      <td>-0.535750</td>\n",
              "      <td>-0.657000</td>\n",
              "      <td>-0.818500</td>\n",
              "      <td>-0.821000</td>\n",
              "      <td>-0.605500</td>\n",
              "      <td>-0.751250</td>\n",
              "      <td>-0.550000</td>\n",
              "      <td>-0.754250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.015500</td>\n",
              "      <td>0.057000</td>\n",
              "      <td>0.184000</td>\n",
              "      <td>-0.016500</td>\n",
              "      <td>-0.023000</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.060500</td>\n",
              "      <td>0.183500</td>\n",
              "      <td>-0.012500</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.066000</td>\n",
              "      <td>0.115500</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>-0.119000</td>\n",
              "      <td>-0.164500</td>\n",
              "      <td>-0.009500</td>\n",
              "      <td>-0.018000</td>\n",
              "      <td>-0.161500</td>\n",
              "      <td>0.048000</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>-0.016000</td>\n",
              "      <td>-0.165500</td>\n",
              "      <td>0.125500</td>\n",
              "      <td>0.036500</td>\n",
              "      <td>0.045000</td>\n",
              "      <td>-0.026000</td>\n",
              "      <td>0.045000</td>\n",
              "      <td>-0.130000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.023500</td>\n",
              "      <td>-0.216500</td>\n",
              "      <td>0.231500</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>-0.012500</td>\n",
              "      <td>-0.234000</td>\n",
              "      <td>-0.013000</td>\n",
              "      <td>-0.099000</td>\n",
              "      <td>0.006500</td>\n",
              "      <td>-0.117000</td>\n",
              "      <td>-0.063000</td>\n",
              "      <td>0.139000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.054500</td>\n",
              "      <td>-0.054000</td>\n",
              "      <td>-0.027500</td>\n",
              "      <td>0.116000</td>\n",
              "      <td>0.085500</td>\n",
              "      <td>0.083000</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.103500</td>\n",
              "      <td>-0.090500</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>-0.010500</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>-0.020000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>-0.034000</td>\n",
              "      <td>0.147000</td>\n",
              "      <td>-0.026000</td>\n",
              "      <td>-0.014000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>-0.082000</td>\n",
              "      <td>0.022000</td>\n",
              "      <td>-0.101500</td>\n",
              "      <td>-0.034500</td>\n",
              "      <td>-0.023000</td>\n",
              "      <td>-0.050000</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>-0.023000</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>0.054500</td>\n",
              "      <td>-0.129000</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>-0.011500</td>\n",
              "      <td>-0.151000</td>\n",
              "      <td>-0.078000</td>\n",
              "      <td>0.060500</td>\n",
              "      <td>-0.032500</td>\n",
              "      <td>-0.053500</td>\n",
              "      <td>-0.157500</td>\n",
              "      <td>-0.018000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>-0.028500</td>\n",
              "      <td>-0.030000</td>\n",
              "      <td>0.038500</td>\n",
              "      <td>-0.108500</td>\n",
              "      <td>-0.034000</td>\n",
              "      <td>-0.043500</td>\n",
              "      <td>-0.144500</td>\n",
              "      <td>-0.119500</td>\n",
              "      <td>-0.086000</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>-0.060500</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>-0.035000</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>-0.053500</td>\n",
              "      <td>0.076500</td>\n",
              "      <td>-0.168500</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>-0.010000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.038000</td>\n",
              "      <td>0.027500</td>\n",
              "      <td>-0.020500</td>\n",
              "      <td>-0.061500</td>\n",
              "      <td>0.041000</td>\n",
              "      <td>0.051500</td>\n",
              "      <td>-0.092500</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>-0.029500</td>\n",
              "      <td>0.085500</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.011500</td>\n",
              "      <td>-0.054500</td>\n",
              "      <td>-0.214000</td>\n",
              "      <td>0.065500</td>\n",
              "      <td>0.045500</td>\n",
              "      <td>0.069000</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>-0.016500</td>\n",
              "      <td>-0.070500</td>\n",
              "      <td>0.023500</td>\n",
              "      <td>-0.164000</td>\n",
              "      <td>-0.098000</td>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.048500</td>\n",
              "      <td>-0.106000</td>\n",
              "      <td>0.123000</td>\n",
              "      <td>0.036500</td>\n",
              "      <td>-0.091000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>0.076000</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.10150</td>\n",
              "      <td>0.070500</td>\n",
              "      <td>-0.064500</td>\n",
              "      <td>-0.051500</td>\n",
              "      <td>-0.026500</td>\n",
              "      <td>-0.208000</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>-0.035000</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.025500</td>\n",
              "      <td>0.067500</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>-0.206500</td>\n",
              "      <td>0.083000</td>\n",
              "      <td>-0.081500</td>\n",
              "      <td>-0.108500</td>\n",
              "      <td>-0.080000</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>-0.026500</td>\n",
              "      <td>-0.063500</td>\n",
              "      <td>-0.139000</td>\n",
              "      <td>-0.126000</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>-0.073000</td>\n",
              "      <td>-0.135000</td>\n",
              "      <td>-0.116500</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>-0.057000</td>\n",
              "      <td>-0.054000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.124000</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>-0.083000</td>\n",
              "      <td>0.10800</td>\n",
              "      <td>0.101000</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>-0.013500</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>-0.012500</td>\n",
              "      <td>0.106500</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>-0.110500</td>\n",
              "      <td>-0.075500</td>\n",
              "      <td>0.048000</td>\n",
              "      <td>-0.064000</td>\n",
              "      <td>0.073000</td>\n",
              "      <td>0.132000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>-0.019000</td>\n",
              "      <td>-0.119500</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>-0.062000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.032500</td>\n",
              "      <td>-0.001500</td>\n",
              "      <td>-0.014500</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>-0.024000</td>\n",
              "      <td>-0.026000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.042000</td>\n",
              "      <td>-0.020000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.095500</td>\n",
              "      <td>-0.016500</td>\n",
              "      <td>0.022500</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>-0.135500</td>\n",
              "      <td>0.050500</td>\n",
              "      <td>0.032000</td>\n",
              "      <td>-0.057500</td>\n",
              "      <td>-0.111000</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>-0.120500</td>\n",
              "      <td>0.079500</td>\n",
              "      <td>0.157000</td>\n",
              "      <td>-0.029000</td>\n",
              "      <td>-0.170000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.004500</td>\n",
              "      <td>-0.146500</td>\n",
              "      <td>0.025500</td>\n",
              "      <td>0.103500</td>\n",
              "      <td>0.14900</td>\n",
              "      <td>-0.01850</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>-0.078500</td>\n",
              "      <td>-0.058000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.00250</td>\n",
              "      <td>-0.111500</td>\n",
              "      <td>0.019500</td>\n",
              "      <td>0.129500</td>\n",
              "      <td>0.072000</td>\n",
              "      <td>-0.031000</td>\n",
              "      <td>0.084500</td>\n",
              "      <td>0.068500</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.115500</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>-0.057500</td>\n",
              "      <td>-0.101500</td>\n",
              "      <td>-0.001500</td>\n",
              "      <td>-0.056000</td>\n",
              "      <td>0.015500</td>\n",
              "      <td>-0.041500</td>\n",
              "      <td>-0.016500</td>\n",
              "      <td>0.058000</td>\n",
              "      <td>-0.048500</td>\n",
              "      <td>-0.134000</td>\n",
              "      <td>-0.065500</td>\n",
              "      <td>0.097000</td>\n",
              "      <td>-0.088000</td>\n",
              "      <td>0.019500</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>-0.107500</td>\n",
              "      <td>0.045500</td>\n",
              "      <td>-0.023500</td>\n",
              "      <td>-0.101000</td>\n",
              "      <td>-0.109000</td>\n",
              "      <td>0.050500</td>\n",
              "      <td>-0.081500</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>0.035000</td>\n",
              "      <td>0.086500</td>\n",
              "      <td>0.126500</td>\n",
              "      <td>0.053500</td>\n",
              "      <td>0.063500</td>\n",
              "      <td>0.052500</td>\n",
              "      <td>0.103000</td>\n",
              "      <td>0.135500</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>0.093500</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>-0.007500</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>-0.122500</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>0.067500</td>\n",
              "      <td>0.091000</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>-0.021000</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>-0.079500</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>-0.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.620750</td>\n",
              "      <td>0.805000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.735000</td>\n",
              "      <td>0.660500</td>\n",
              "      <td>0.783250</td>\n",
              "      <td>0.766250</td>\n",
              "      <td>0.635000</td>\n",
              "      <td>0.733000</td>\n",
              "      <td>0.694250</td>\n",
              "      <td>0.786250</td>\n",
              "      <td>0.805250</td>\n",
              "      <td>0.654000</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>0.578000</td>\n",
              "      <td>0.699000</td>\n",
              "      <td>0.496500</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.698000</td>\n",
              "      <td>0.560500</td>\n",
              "      <td>0.797000</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.619250</td>\n",
              "      <td>0.579250</td>\n",
              "      <td>0.719500</td>\n",
              "      <td>0.798250</td>\n",
              "      <td>0.721750</td>\n",
              "      <td>0.678500</td>\n",
              "      <td>0.728750</td>\n",
              "      <td>0.670500</td>\n",
              "      <td>0.698750</td>\n",
              "      <td>0.602750</td>\n",
              "      <td>0.557750</td>\n",
              "      <td>0.736250</td>\n",
              "      <td>0.599250</td>\n",
              "      <td>0.612000</td>\n",
              "      <td>0.557000</td>\n",
              "      <td>0.634500</td>\n",
              "      <td>0.616750</td>\n",
              "      <td>0.731500</td>\n",
              "      <td>0.641000</td>\n",
              "      <td>0.699500</td>\n",
              "      <td>0.781750</td>\n",
              "      <td>0.578000</td>\n",
              "      <td>0.677250</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.609750</td>\n",
              "      <td>0.728250</td>\n",
              "      <td>0.729250</td>\n",
              "      <td>0.627000</td>\n",
              "      <td>0.811500</td>\n",
              "      <td>0.799250</td>\n",
              "      <td>0.550750</td>\n",
              "      <td>0.656500</td>\n",
              "      <td>0.703750</td>\n",
              "      <td>0.592000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.703750</td>\n",
              "      <td>0.819000</td>\n",
              "      <td>0.750250</td>\n",
              "      <td>0.581750</td>\n",
              "      <td>0.741750</td>\n",
              "      <td>0.671500</td>\n",
              "      <td>0.721250</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.644750</td>\n",
              "      <td>0.640750</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.629250</td>\n",
              "      <td>0.655500</td>\n",
              "      <td>0.585500</td>\n",
              "      <td>0.859750</td>\n",
              "      <td>0.705250</td>\n",
              "      <td>0.519250</td>\n",
              "      <td>0.763000</td>\n",
              "      <td>0.728500</td>\n",
              "      <td>0.505000</td>\n",
              "      <td>0.521500</td>\n",
              "      <td>0.714750</td>\n",
              "      <td>0.681750</td>\n",
              "      <td>0.592500</td>\n",
              "      <td>0.528500</td>\n",
              "      <td>0.616750</td>\n",
              "      <td>0.556250</td>\n",
              "      <td>0.705750</td>\n",
              "      <td>0.728250</td>\n",
              "      <td>0.617000</td>\n",
              "      <td>0.645250</td>\n",
              "      <td>0.670750</td>\n",
              "      <td>0.666750</td>\n",
              "      <td>0.518500</td>\n",
              "      <td>0.599250</td>\n",
              "      <td>0.567500</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.660250</td>\n",
              "      <td>0.577250</td>\n",
              "      <td>0.684750</td>\n",
              "      <td>0.614250</td>\n",
              "      <td>0.703500</td>\n",
              "      <td>0.540000</td>\n",
              "      <td>0.796500</td>\n",
              "      <td>0.600500</td>\n",
              "      <td>0.699500</td>\n",
              "      <td>0.649500</td>\n",
              "      <td>0.687750</td>\n",
              "      <td>0.905750</td>\n",
              "      <td>0.728250</td>\n",
              "      <td>0.708000</td>\n",
              "      <td>0.628500</td>\n",
              "      <td>0.569250</td>\n",
              "      <td>0.757000</td>\n",
              "      <td>0.555500</td>\n",
              "      <td>0.815500</td>\n",
              "      <td>0.758750</td>\n",
              "      <td>0.859500</td>\n",
              "      <td>0.736500</td>\n",
              "      <td>0.805750</td>\n",
              "      <td>0.778500</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>0.689000</td>\n",
              "      <td>0.619750</td>\n",
              "      <td>0.751500</td>\n",
              "      <td>0.671750</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.570750</td>\n",
              "      <td>0.787000</td>\n",
              "      <td>0.634500</td>\n",
              "      <td>0.471000</td>\n",
              "      <td>0.556500</td>\n",
              "      <td>0.763250</td>\n",
              "      <td>0.628750</td>\n",
              "      <td>0.631000</td>\n",
              "      <td>0.770500</td>\n",
              "      <td>0.785750</td>\n",
              "      <td>0.617250</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.681750</td>\n",
              "      <td>0.743000</td>\n",
              "      <td>0.748750</td>\n",
              "      <td>0.821750</td>\n",
              "      <td>0.71375</td>\n",
              "      <td>0.647500</td>\n",
              "      <td>0.754750</td>\n",
              "      <td>0.647250</td>\n",
              "      <td>0.609250</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.688250</td>\n",
              "      <td>0.750250</td>\n",
              "      <td>0.571500</td>\n",
              "      <td>0.582250</td>\n",
              "      <td>0.732250</td>\n",
              "      <td>0.710250</td>\n",
              "      <td>0.702000</td>\n",
              "      <td>0.675250</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.582750</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.646250</td>\n",
              "      <td>0.613000</td>\n",
              "      <td>0.585250</td>\n",
              "      <td>0.807750</td>\n",
              "      <td>0.631750</td>\n",
              "      <td>0.622500</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.841500</td>\n",
              "      <td>0.639000</td>\n",
              "      <td>0.633500</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.595000</td>\n",
              "      <td>0.695750</td>\n",
              "      <td>0.787000</td>\n",
              "      <td>0.485500</td>\n",
              "      <td>0.699750</td>\n",
              "      <td>0.614000</td>\n",
              "      <td>0.756750</td>\n",
              "      <td>0.678750</td>\n",
              "      <td>0.83600</td>\n",
              "      <td>0.619250</td>\n",
              "      <td>0.674500</td>\n",
              "      <td>0.588750</td>\n",
              "      <td>0.538500</td>\n",
              "      <td>0.631000</td>\n",
              "      <td>0.811250</td>\n",
              "      <td>0.812000</td>\n",
              "      <td>0.565000</td>\n",
              "      <td>0.694500</td>\n",
              "      <td>0.608500</td>\n",
              "      <td>0.692750</td>\n",
              "      <td>0.689000</td>\n",
              "      <td>0.724250</td>\n",
              "      <td>0.687750</td>\n",
              "      <td>0.696750</td>\n",
              "      <td>0.691000</td>\n",
              "      <td>0.715750</td>\n",
              "      <td>0.585250</td>\n",
              "      <td>0.674750</td>\n",
              "      <td>0.641250</td>\n",
              "      <td>0.654500</td>\n",
              "      <td>0.599500</td>\n",
              "      <td>0.630750</td>\n",
              "      <td>0.633000</td>\n",
              "      <td>0.886750</td>\n",
              "      <td>0.520750</td>\n",
              "      <td>0.710750</td>\n",
              "      <td>0.511000</td>\n",
              "      <td>0.701250</td>\n",
              "      <td>0.602500</td>\n",
              "      <td>0.536750</td>\n",
              "      <td>0.771000</td>\n",
              "      <td>0.740250</td>\n",
              "      <td>0.648250</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.636500</td>\n",
              "      <td>0.693000</td>\n",
              "      <td>0.522250</td>\n",
              "      <td>0.700500</td>\n",
              "      <td>0.722250</td>\n",
              "      <td>0.537000</td>\n",
              "      <td>0.658000</td>\n",
              "      <td>0.702250</td>\n",
              "      <td>0.645250</td>\n",
              "      <td>0.728500</td>\n",
              "      <td>0.689000</td>\n",
              "      <td>0.812750</td>\n",
              "      <td>0.502000</td>\n",
              "      <td>0.732750</td>\n",
              "      <td>0.769250</td>\n",
              "      <td>0.568250</td>\n",
              "      <td>0.651000</td>\n",
              "      <td>0.815000</td>\n",
              "      <td>0.75500</td>\n",
              "      <td>0.67275</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.587750</td>\n",
              "      <td>0.726000</td>\n",
              "      <td>0.574250</td>\n",
              "      <td>0.66525</td>\n",
              "      <td>0.460250</td>\n",
              "      <td>0.772250</td>\n",
              "      <td>0.869250</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.600500</td>\n",
              "      <td>0.765750</td>\n",
              "      <td>0.751500</td>\n",
              "      <td>0.783250</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.769000</td>\n",
              "      <td>0.758000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.604000</td>\n",
              "      <td>0.667250</td>\n",
              "      <td>0.616250</td>\n",
              "      <td>0.608250</td>\n",
              "      <td>0.642750</td>\n",
              "      <td>0.616250</td>\n",
              "      <td>0.778500</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.708750</td>\n",
              "      <td>0.726000</td>\n",
              "      <td>0.739250</td>\n",
              "      <td>0.856750</td>\n",
              "      <td>0.709250</td>\n",
              "      <td>0.616000</td>\n",
              "      <td>0.569500</td>\n",
              "      <td>0.655750</td>\n",
              "      <td>0.634750</td>\n",
              "      <td>0.525750</td>\n",
              "      <td>0.612000</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.630250</td>\n",
              "      <td>0.836750</td>\n",
              "      <td>0.718000</td>\n",
              "      <td>0.791500</td>\n",
              "      <td>0.726000</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.665750</td>\n",
              "      <td>0.603750</td>\n",
              "      <td>0.705500</td>\n",
              "      <td>0.688250</td>\n",
              "      <td>0.759000</td>\n",
              "      <td>0.704750</td>\n",
              "      <td>0.673750</td>\n",
              "      <td>0.748750</td>\n",
              "      <td>0.604250</td>\n",
              "      <td>0.650500</td>\n",
              "      <td>0.772500</td>\n",
              "      <td>0.797250</td>\n",
              "      <td>0.804250</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.650250</td>\n",
              "      <td>0.739500</td>\n",
              "      <td>0.493000</td>\n",
              "      <td>0.683000</td>\n",
              "      <td>0.794250</td>\n",
              "      <td>0.654250</td>\n",
              "      <td>0.503250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.567000</td>\n",
              "      <td>2.419000</td>\n",
              "      <td>3.392000</td>\n",
              "      <td>2.771000</td>\n",
              "      <td>2.901000</td>\n",
              "      <td>2.793000</td>\n",
              "      <td>2.546000</td>\n",
              "      <td>2.846000</td>\n",
              "      <td>2.512000</td>\n",
              "      <td>2.959000</td>\n",
              "      <td>3.271000</td>\n",
              "      <td>2.998000</td>\n",
              "      <td>2.729000</td>\n",
              "      <td>2.651000</td>\n",
              "      <td>2.913000</td>\n",
              "      <td>2.508000</td>\n",
              "      <td>3.286000</td>\n",
              "      <td>2.430000</td>\n",
              "      <td>2.557000</td>\n",
              "      <td>2.868000</td>\n",
              "      <td>2.703000</td>\n",
              "      <td>2.691000</td>\n",
              "      <td>2.604000</td>\n",
              "      <td>2.362000</td>\n",
              "      <td>2.927000</td>\n",
              "      <td>2.976000</td>\n",
              "      <td>2.581000</td>\n",
              "      <td>2.305000</td>\n",
              "      <td>2.489000</td>\n",
              "      <td>2.895000</td>\n",
              "      <td>2.457000</td>\n",
              "      <td>2.407000</td>\n",
              "      <td>2.882000</td>\n",
              "      <td>2.649000</td>\n",
              "      <td>2.914000</td>\n",
              "      <td>2.995000</td>\n",
              "      <td>2.382000</td>\n",
              "      <td>2.481000</td>\n",
              "      <td>2.704000</td>\n",
              "      <td>3.585000</td>\n",
              "      <td>3.127000</td>\n",
              "      <td>2.835000</td>\n",
              "      <td>3.847000</td>\n",
              "      <td>2.541000</td>\n",
              "      <td>2.929000</td>\n",
              "      <td>3.506000</td>\n",
              "      <td>2.343000</td>\n",
              "      <td>2.919000</td>\n",
              "      <td>2.902000</td>\n",
              "      <td>3.046000</td>\n",
              "      <td>2.983000</td>\n",
              "      <td>3.302000</td>\n",
              "      <td>2.313000</td>\n",
              "      <td>2.606000</td>\n",
              "      <td>2.813000</td>\n",
              "      <td>2.514000</td>\n",
              "      <td>2.701000</td>\n",
              "      <td>2.691000</td>\n",
              "      <td>3.510000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>2.959000</td>\n",
              "      <td>2.479000</td>\n",
              "      <td>3.224000</td>\n",
              "      <td>2.767000</td>\n",
              "      <td>3.755000</td>\n",
              "      <td>3.163000</td>\n",
              "      <td>3.064000</td>\n",
              "      <td>2.785000</td>\n",
              "      <td>2.965000</td>\n",
              "      <td>2.898000</td>\n",
              "      <td>2.544000</td>\n",
              "      <td>3.521000</td>\n",
              "      <td>2.586000</td>\n",
              "      <td>2.151000</td>\n",
              "      <td>2.587000</td>\n",
              "      <td>2.976000</td>\n",
              "      <td>2.645000</td>\n",
              "      <td>3.298000</td>\n",
              "      <td>2.768000</td>\n",
              "      <td>2.443000</td>\n",
              "      <td>2.862000</td>\n",
              "      <td>2.794000</td>\n",
              "      <td>2.547000</td>\n",
              "      <td>3.171000</td>\n",
              "      <td>2.575000</td>\n",
              "      <td>2.799000</td>\n",
              "      <td>2.710000</td>\n",
              "      <td>2.504000</td>\n",
              "      <td>2.818000</td>\n",
              "      <td>2.788000</td>\n",
              "      <td>3.456000</td>\n",
              "      <td>2.493000</td>\n",
              "      <td>2.313000</td>\n",
              "      <td>2.905000</td>\n",
              "      <td>3.306000</td>\n",
              "      <td>1.984000</td>\n",
              "      <td>3.427000</td>\n",
              "      <td>2.540000</td>\n",
              "      <td>3.875000</td>\n",
              "      <td>3.266000</td>\n",
              "      <td>2.726000</td>\n",
              "      <td>3.885000</td>\n",
              "      <td>3.035000</td>\n",
              "      <td>3.581000</td>\n",
              "      <td>3.022000</td>\n",
              "      <td>2.188000</td>\n",
              "      <td>2.791000</td>\n",
              "      <td>2.245000</td>\n",
              "      <td>2.949000</td>\n",
              "      <td>2.411000</td>\n",
              "      <td>3.220000</td>\n",
              "      <td>3.399000</td>\n",
              "      <td>2.478000</td>\n",
              "      <td>2.974000</td>\n",
              "      <td>2.774000</td>\n",
              "      <td>3.056000</td>\n",
              "      <td>3.479000</td>\n",
              "      <td>3.100000</td>\n",
              "      <td>2.883000</td>\n",
              "      <td>3.053000</td>\n",
              "      <td>2.636000</td>\n",
              "      <td>3.190000</td>\n",
              "      <td>2.869000</td>\n",
              "      <td>2.724000</td>\n",
              "      <td>3.267000</td>\n",
              "      <td>3.174000</td>\n",
              "      <td>3.408000</td>\n",
              "      <td>2.689000</td>\n",
              "      <td>2.980000</td>\n",
              "      <td>2.382000</td>\n",
              "      <td>2.874000</td>\n",
              "      <td>2.395000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>2.945000</td>\n",
              "      <td>2.848000</td>\n",
              "      <td>2.447000</td>\n",
              "      <td>2.649000</td>\n",
              "      <td>2.748000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.775000</td>\n",
              "      <td>3.270000</td>\n",
              "      <td>2.816000</td>\n",
              "      <td>3.20100</td>\n",
              "      <td>2.752000</td>\n",
              "      <td>3.048000</td>\n",
              "      <td>2.720000</td>\n",
              "      <td>2.137000</td>\n",
              "      <td>2.298000</td>\n",
              "      <td>2.506000</td>\n",
              "      <td>3.523000</td>\n",
              "      <td>2.974000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>2.996000</td>\n",
              "      <td>3.431000</td>\n",
              "      <td>2.854000</td>\n",
              "      <td>1.959000</td>\n",
              "      <td>3.555000</td>\n",
              "      <td>2.930000</td>\n",
              "      <td>2.728000</td>\n",
              "      <td>3.016000</td>\n",
              "      <td>2.250000</td>\n",
              "      <td>2.889000</td>\n",
              "      <td>2.522000</td>\n",
              "      <td>2.747000</td>\n",
              "      <td>3.154000</td>\n",
              "      <td>2.805000</td>\n",
              "      <td>2.520000</td>\n",
              "      <td>2.621000</td>\n",
              "      <td>2.364000</td>\n",
              "      <td>2.844000</td>\n",
              "      <td>2.483000</td>\n",
              "      <td>2.589000</td>\n",
              "      <td>2.394000</td>\n",
              "      <td>2.975000</td>\n",
              "      <td>2.375000</td>\n",
              "      <td>2.546000</td>\n",
              "      <td>2.214000</td>\n",
              "      <td>3.160000</td>\n",
              "      <td>2.879000</td>\n",
              "      <td>2.553000</td>\n",
              "      <td>2.65600</td>\n",
              "      <td>2.562000</td>\n",
              "      <td>2.312000</td>\n",
              "      <td>2.943000</td>\n",
              "      <td>2.839000</td>\n",
              "      <td>2.977000</td>\n",
              "      <td>2.366000</td>\n",
              "      <td>2.892000</td>\n",
              "      <td>2.152000</td>\n",
              "      <td>2.534000</td>\n",
              "      <td>2.893000</td>\n",
              "      <td>2.936000</td>\n",
              "      <td>3.763000</td>\n",
              "      <td>2.430000</td>\n",
              "      <td>2.651000</td>\n",
              "      <td>2.777000</td>\n",
              "      <td>2.510000</td>\n",
              "      <td>2.387000</td>\n",
              "      <td>2.344000</td>\n",
              "      <td>2.185000</td>\n",
              "      <td>3.084000</td>\n",
              "      <td>3.022000</td>\n",
              "      <td>3.435000</td>\n",
              "      <td>2.597000</td>\n",
              "      <td>3.112000</td>\n",
              "      <td>2.424000</td>\n",
              "      <td>2.835000</td>\n",
              "      <td>2.654000</td>\n",
              "      <td>2.519000</td>\n",
              "      <td>2.411000</td>\n",
              "      <td>3.132000</td>\n",
              "      <td>2.541000</td>\n",
              "      <td>3.302000</td>\n",
              "      <td>2.646000</td>\n",
              "      <td>2.443000</td>\n",
              "      <td>3.402000</td>\n",
              "      <td>2.933000</td>\n",
              "      <td>2.743000</td>\n",
              "      <td>2.493000</td>\n",
              "      <td>3.602000</td>\n",
              "      <td>2.149000</td>\n",
              "      <td>2.045000</td>\n",
              "      <td>2.628000</td>\n",
              "      <td>2.696000</td>\n",
              "      <td>2.347000</td>\n",
              "      <td>2.856000</td>\n",
              "      <td>3.364000</td>\n",
              "      <td>2.557000</td>\n",
              "      <td>1.967000</td>\n",
              "      <td>2.614000</td>\n",
              "      <td>2.723000</td>\n",
              "      <td>2.451000</td>\n",
              "      <td>3.140000</td>\n",
              "      <td>2.410000</td>\n",
              "      <td>2.91600</td>\n",
              "      <td>2.33800</td>\n",
              "      <td>2.544000</td>\n",
              "      <td>2.194000</td>\n",
              "      <td>3.092000</td>\n",
              "      <td>2.979000</td>\n",
              "      <td>2.93700</td>\n",
              "      <td>3.069000</td>\n",
              "      <td>2.560000</td>\n",
              "      <td>3.334000</td>\n",
              "      <td>2.528000</td>\n",
              "      <td>2.523000</td>\n",
              "      <td>3.083000</td>\n",
              "      <td>2.634000</td>\n",
              "      <td>2.377000</td>\n",
              "      <td>2.716000</td>\n",
              "      <td>2.589000</td>\n",
              "      <td>2.538000</td>\n",
              "      <td>3.617000</td>\n",
              "      <td>3.113000</td>\n",
              "      <td>2.842000</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>2.019000</td>\n",
              "      <td>2.581000</td>\n",
              "      <td>2.774000</td>\n",
              "      <td>2.708000</td>\n",
              "      <td>2.612000</td>\n",
              "      <td>2.680000</td>\n",
              "      <td>2.964000</td>\n",
              "      <td>2.663000</td>\n",
              "      <td>2.406000</td>\n",
              "      <td>3.457000</td>\n",
              "      <td>2.496000</td>\n",
              "      <td>2.501000</td>\n",
              "      <td>2.832000</td>\n",
              "      <td>2.897000</td>\n",
              "      <td>3.753000</td>\n",
              "      <td>2.498000</td>\n",
              "      <td>2.725000</td>\n",
              "      <td>2.680000</td>\n",
              "      <td>3.445000</td>\n",
              "      <td>2.846000</td>\n",
              "      <td>2.315000</td>\n",
              "      <td>2.780000</td>\n",
              "      <td>2.364000</td>\n",
              "      <td>2.908000</td>\n",
              "      <td>2.926000</td>\n",
              "      <td>3.441000</td>\n",
              "      <td>2.319000</td>\n",
              "      <td>2.842000</td>\n",
              "      <td>3.343000</td>\n",
              "      <td>3.266000</td>\n",
              "      <td>3.061000</td>\n",
              "      <td>2.146000</td>\n",
              "      <td>2.853000</td>\n",
              "      <td>3.026000</td>\n",
              "      <td>2.865000</td>\n",
              "      <td>2.801000</td>\n",
              "      <td>2.736000</td>\n",
              "      <td>2.596000</td>\n",
              "      <td>2.226000</td>\n",
              "      <td>3.131000</td>\n",
              "      <td>3.236000</td>\n",
              "      <td>2.626000</td>\n",
              "      <td>3.530000</td>\n",
              "      <td>2.771000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           target           0           1           2           3           4  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.640000    0.023292   -0.026872    0.167404    0.001904    0.001588   \n",
              "std      0.480963    0.998354    1.009314    1.021709    1.011751    1.035411   \n",
              "min      0.000000   -2.319000   -2.931000   -2.477000   -2.359000   -2.566000   \n",
              "25%      0.000000   -0.644750   -0.739750   -0.425250   -0.686500   -0.659000   \n",
              "50%      1.000000   -0.015500    0.057000    0.184000   -0.016500   -0.023000   \n",
              "75%      1.000000    0.677000    0.620750    0.805000    0.720000    0.735000   \n",
              "max      1.000000    2.567000    2.419000    3.392000    2.771000    2.901000   \n",
              "\n",
              "                5           6           7           8           9          10  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.007304    0.032052    0.078412   -0.036920    0.035448   -0.005032   \n",
              "std      0.955700    1.006657    0.939731    0.963688    1.019689    1.085089   \n",
              "min     -2.845000   -2.976000   -3.444000   -2.768000   -2.361000   -3.302000   \n",
              "25%     -0.643750   -0.675000   -0.550750   -0.689500   -0.643500   -0.693500   \n",
              "50%      0.037500    0.060500    0.183500   -0.012500    0.052000    0.066000   \n",
              "75%      0.660500    0.783250    0.766250    0.635000    0.733000    0.694250   \n",
              "max      2.793000    2.546000    2.846000    2.512000    2.959000    3.271000   \n",
              "\n",
              "               11          12          13          14          15          16  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.110248    0.019808   -0.001108   -0.016280   -0.039644    0.017260   \n",
              "std      1.036265    1.050041    1.024305    0.926789    0.955915    1.025655   \n",
              "min     -2.851000   -2.681000   -2.596000   -3.275000   -3.512000   -2.476000   \n",
              "25%     -0.524000   -0.708500   -0.692000   -0.677000   -0.634500   -0.683500   \n",
              "50%      0.115500    0.090000    0.016000    0.009500    0.010000   -0.119000   \n",
              "75%      0.786250    0.805250    0.654000    0.611000    0.578000    0.699000   \n",
              "max      2.998000    2.729000    2.651000    2.913000    2.508000    3.286000   \n",
              "\n",
              "               17          18          19          20          21          22  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.106856    0.036184   -0.043296   -0.110832    0.072680    0.017296   \n",
              "std      1.012777    0.945099    1.055935    1.003178    1.039556    0.988482   \n",
              "min     -3.619000   -2.428000   -3.229000   -3.024000   -2.775000   -2.962000   \n",
              "25%     -0.801500   -0.574250   -0.758000   -0.870500   -0.596000   -0.725750   \n",
              "50%     -0.164500   -0.009500   -0.018000   -0.161500    0.048000    0.135000   \n",
              "75%      0.496500    0.686000    0.698000    0.560500    0.797000    0.631500   \n",
              "max      2.430000    2.557000    2.868000    2.703000    2.691000    2.604000   \n",
              "\n",
              "               23          24          25          26          27          28  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.030728   -0.128252    0.154736    0.083408    0.039552   -0.091784   \n",
              "std      0.945902    0.997026    0.997894    1.040371    0.922270    1.047282   \n",
              "min     -2.490000   -3.107000   -2.943000   -2.933000   -2.942000   -2.957000   \n",
              "25%     -0.652000   -0.779500   -0.424250   -0.585750   -0.625000   -0.751250   \n",
              "50%     -0.016000   -0.165500    0.125500    0.036500    0.045000   -0.026000   \n",
              "75%      0.619250    0.579250    0.719500    0.798250    0.721750    0.678500   \n",
              "max      2.362000    2.927000    2.976000    2.581000    2.305000    2.489000   \n",
              "\n",
              "               29          30          31          32          33          34  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.054636   -0.048288   -0.017296    0.007708   -0.134460    0.093852   \n",
              "std      1.041432    1.010971    0.992464    0.986350    1.015563    1.117898   \n",
              "min     -2.911000   -2.568000   -2.649000   -3.031000   -2.913000   -3.265000   \n",
              "25%     -0.582500   -0.713500   -0.750000   -0.588000   -0.829000   -0.648500   \n",
              "50%      0.045000   -0.130000    0.016000    0.023500   -0.216500    0.231500   \n",
              "75%      0.728750    0.670500    0.698750    0.602750    0.557750    0.736250   \n",
              "max      2.895000    2.457000    2.407000    2.882000    2.649000    2.914000   \n",
              "\n",
              "               35          36          37          38          39          40  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.020588   -0.002492   -0.141400   -0.061500   -0.043576    0.009136   \n",
              "std      0.958191    0.948855    1.042429    1.024573    0.996280    0.993367   \n",
              "min     -2.372000   -3.037000   -3.340000   -2.699000   -3.398000   -2.717000   \n",
              "25%     -0.659750   -0.614000   -0.816750   -0.819500   -0.665250   -0.529500   \n",
              "50%      0.014000   -0.012500   -0.234000   -0.013000   -0.099000    0.006500   \n",
              "75%      0.599250    0.612000    0.557000    0.634500    0.616750    0.731500   \n",
              "max      2.995000    2.382000    2.481000    2.704000    3.585000    3.127000   \n",
              "\n",
              "               41          42          43          44          45          46  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.056824   -0.025968    0.046644   -0.029320    0.048420   -0.030300   \n",
              "std      0.977879    1.035532    0.991244    0.921214    0.973115    0.998897   \n",
              "min     -2.698000   -2.728000   -2.838000   -2.741000   -2.335000   -2.946000   \n",
              "25%     -0.655500   -0.751250   -0.679750   -0.714500   -0.681750   -0.729250   \n",
              "50%     -0.117000   -0.063000    0.139000    0.000500    0.054500   -0.054000   \n",
              "75%      0.641000    0.699500    0.781750    0.578000    0.677250    0.690000   \n",
              "max      2.835000    3.847000    2.541000    2.929000    3.506000    2.343000   \n",
              "\n",
              "               47          48          49          50          51          52  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.054828    0.085188    0.021192    0.031700    0.065468    0.047296   \n",
              "std      1.038618    0.984001    1.048054    0.988012    1.055504    1.033105   \n",
              "min     -2.815000   -2.677000   -2.760000   -2.582000   -2.553000   -2.736000   \n",
              "25%     -0.798500   -0.477000   -0.662250   -0.619000   -0.651250   -0.666750   \n",
              "50%     -0.027500    0.116000    0.085500    0.083000    0.049500    0.103500   \n",
              "75%      0.609750    0.728250    0.729250    0.627000    0.811500    0.799250   \n",
              "max      2.919000    2.902000    3.046000    2.983000    3.302000    2.313000   \n",
              "\n",
              "               53          54          55          56          57          58  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.051608    0.045292   -0.038224    0.016220   -0.044704    0.070112   \n",
              "std      0.948249    1.010267    1.033381    0.909847    1.033891    0.992619   \n",
              "min     -3.559000   -3.041000   -3.415000   -2.748000   -2.586000   -2.473000   \n",
              "25%     -0.565500   -0.606500   -0.696750   -0.627750   -0.776250   -0.547250   \n",
              "50%     -0.090500    0.040000   -0.010500    0.085000   -0.020000    0.016000   \n",
              "75%      0.550750    0.656500    0.703750    0.592000    0.687500    0.703750   \n",
              "max      2.606000    2.813000    2.514000    2.701000    2.691000    3.510000   \n",
              "\n",
              "               59          60          61          62          63          64  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.047628    0.088100   -0.028832    0.034528    0.064612   -0.074768   \n",
              "std      1.053395    0.973836    0.968131    0.960685    0.924178    1.079216   \n",
              "min     -2.672000   -2.724000   -3.058000   -2.736000   -2.607000   -2.797000   \n",
              "25%     -0.713750   -0.473750   -0.680750   -0.596500   -0.510250   -0.889250   \n",
              "50%     -0.034000    0.147000   -0.026000   -0.014000    0.080000   -0.082000   \n",
              "75%      0.819000    0.750250    0.581750    0.741750    0.671500    0.721250   \n",
              "max      2.800000    2.959000    2.479000    3.224000    2.767000    3.755000   \n",
              "\n",
              "               65          66          67          68          69          70  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.003796   -0.114680    0.034240    0.006828   -0.059696    0.033832   \n",
              "std      0.983879    1.070762    0.993845    1.021970    1.016206    0.979236   \n",
              "min     -2.799000   -2.697000   -3.020000   -2.628000   -2.869000   -2.903000   \n",
              "25%     -0.636000   -0.796250   -0.604500   -0.691750   -0.740250   -0.637500   \n",
              "50%      0.022000   -0.101500   -0.034500   -0.023000   -0.050000    0.033000   \n",
              "75%      0.687000    0.644750    0.640750    0.750000    0.629250    0.655500   \n",
              "max      3.163000    3.064000    2.785000    2.965000    2.898000    2.544000   \n",
              "\n",
              "               71          72          73          74          75          76  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.033364    0.055336    0.005072   -0.117244    0.085096    0.071112   \n",
              "std      1.062990    1.021214    1.021526    0.929744    1.015468    0.967391   \n",
              "min     -2.912000   -3.177000   -2.868000   -2.570000   -2.308000   -3.044000   \n",
              "25%     -0.719750   -0.711250   -0.717250   -0.789250   -0.701250   -0.608500   \n",
              "50%     -0.023000    0.010500    0.054500   -0.129000    0.189000    0.120000   \n",
              "75%      0.585500    0.859750    0.705250    0.519250    0.763000    0.728500   \n",
              "max      3.521000    2.586000    2.151000    2.587000    2.976000    2.645000   \n",
              "\n",
              "               77          78          79          80          81          82  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.140400   -0.072248    0.021096    0.023664   -0.018056   -0.071508   \n",
              "std      1.058324    1.035092    0.904238    1.032585    0.951960    0.897700   \n",
              "min     -3.098000   -3.130000   -1.870000   -2.301000   -2.325000   -2.102000   \n",
              "25%     -0.793750   -0.713000   -0.614500   -0.762750   -0.695000   -0.681750   \n",
              "50%     -0.011500   -0.151000   -0.078000    0.060500   -0.032500   -0.053500   \n",
              "75%      0.505000    0.521500    0.714750    0.681750    0.592500    0.528500   \n",
              "max      3.298000    2.768000    2.443000    2.862000    2.794000    2.547000   \n",
              "\n",
              "               83          84          85          86          87          88  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.074868   -0.050976    0.045988    0.023304   -0.013688    0.029780   \n",
              "std      1.004530    0.936543    1.023601    0.972569    0.962015    0.997142   \n",
              "min     -2.445000   -2.269000   -2.961000   -3.196000   -2.656000   -2.690000   \n",
              "25%     -0.750000   -0.694000   -0.600750   -0.617250   -0.747000   -0.693500   \n",
              "50%     -0.157500   -0.018000    0.080000   -0.028500   -0.030000    0.038500   \n",
              "75%      0.616750    0.556250    0.705750    0.728250    0.617000    0.645250   \n",
              "max      3.171000    2.575000    2.799000    2.710000    2.504000    2.818000   \n",
              "\n",
              "               89          90          91          92          93          94  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.088448    0.003468   -0.067724   -0.162788   -0.136320   -0.015532   \n",
              "std      1.021435    0.968743    1.001998    0.990242    1.018165    1.058821   \n",
              "min     -2.755000   -3.109000   -2.909000   -2.681000   -3.054000   -2.225000   \n",
              "25%     -0.688500   -0.650750   -0.660000   -0.898500   -0.723750   -0.782250   \n",
              "50%     -0.108500   -0.034000   -0.043500   -0.144500   -0.119500   -0.086000   \n",
              "75%      0.670750    0.666750    0.518500    0.599250    0.567500    0.668000   \n",
              "max      2.788000    3.456000    2.493000    2.313000    2.905000    3.306000   \n",
              "\n",
              "               95          96          97          98          99         100  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.039068   -0.005716    0.006416   -0.027120    0.037784   -0.081212   \n",
              "std      0.930971    1.025627    1.014966    1.000413    1.029598    0.962005   \n",
              "min     -2.107000   -2.986000   -2.521000   -2.567000   -2.694000   -3.263000   \n",
              "25%     -0.836500   -0.669000   -0.678250   -0.583000   -0.675000   -0.689500   \n",
              "50%      0.024000   -0.060500    0.039000   -0.035000    0.011500   -0.053500   \n",
              "75%      0.660250    0.577250    0.684750    0.614250    0.703500    0.540000   \n",
              "max      1.984000    3.427000    2.540000    3.875000    3.266000    2.726000   \n",
              "\n",
              "              101         102         103         104         105         106  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.048188   -0.129152    0.084436    0.040980    0.005156    0.082088   \n",
              "std      1.058964    1.008200    1.028253    0.986306    0.980061    1.007852   \n",
              "min     -2.865000   -3.455000   -3.465000   -2.260000   -2.680000   -2.982000   \n",
              "25%     -0.627500   -0.793500   -0.494500   -0.611000   -0.630750   -0.596750   \n",
              "50%      0.076500   -0.168500    0.144000   -0.010000    0.004000    0.038000   \n",
              "75%      0.796500    0.600500    0.699500    0.649500    0.687750    0.905750   \n",
              "max      3.885000    3.035000    3.581000    3.022000    2.188000    2.791000   \n",
              "\n",
              "              107         108         109         110         111         112  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.018372    0.001304   -0.106732   -0.044920    0.018000   -0.044172   \n",
              "std      1.057517    1.012250    0.972992    1.048435    1.094542    0.998305   \n",
              "min     -2.855000   -2.275000   -3.900000   -2.608000   -3.889000   -2.832000   \n",
              "25%     -0.641500   -0.746750   -0.751750   -0.684250   -0.613500   -0.787500   \n",
              "50%      0.027500   -0.020500   -0.061500    0.041000    0.051500   -0.092500   \n",
              "75%      0.728250    0.708000    0.628500    0.569250    0.757000    0.555500   \n",
              "max      2.245000    2.949000    2.411000    3.220000    3.399000    2.478000   \n",
              "\n",
              "              113         114         115         116         117         118  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.050436    0.058124    0.137328    0.001136    0.007536   -0.002052   \n",
              "std      1.018891    0.972842    1.062220    1.034974    1.074398    1.023247   \n",
              "min     -3.140000   -2.295000   -2.925000   -2.706000   -4.270000   -2.770000   \n",
              "25%     -0.642000   -0.717000   -0.522000   -0.701500   -0.733500   -0.717750   \n",
              "50%      0.016500   -0.029500    0.085500   -0.002000   -0.011500   -0.054500   \n",
              "75%      0.815500    0.758750    0.859500    0.736500    0.805750    0.778500   \n",
              "max      2.974000    2.774000    3.056000    3.479000    3.100000    2.883000   \n",
              "\n",
              "              119         120         121         122         123         124  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.116228    0.050988    0.011496    0.055308    0.061228    0.048288   \n",
              "std      1.053193    1.003299    0.990817    0.972459    0.936815    1.022176   \n",
              "min     -2.573000   -2.960000   -2.488000   -2.842000   -2.607000   -2.582000   \n",
              "25%     -0.898750   -0.583500   -0.687000   -0.649500   -0.515000   -0.594750   \n",
              "50%     -0.214000    0.065500    0.045500    0.069000    0.001500    0.010500   \n",
              "75%      0.626250    0.689000    0.619750    0.751500    0.671750    0.701000   \n",
              "max      3.053000    2.636000    3.190000    2.869000    2.724000    3.267000   \n",
              "\n",
              "              125         126         127         128         129         130  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.039312   -0.080952    0.002332   -0.014024   -0.146060   -0.066192   \n",
              "std      1.020982    1.013309    1.056450    0.972288    0.912586    1.000926   \n",
              "min     -2.922000   -4.016000   -2.752000   -2.879000   -3.411000   -2.664000   \n",
              "25%     -0.714500   -0.638500   -0.683250   -0.661250   -0.708750   -0.695250   \n",
              "50%     -0.017000   -0.016500   -0.070500    0.023500   -0.164000   -0.098000   \n",
              "75%      0.631500    0.570750    0.787000    0.634500    0.471000    0.556500   \n",
              "max      3.174000    3.408000    2.689000    2.980000    2.382000    2.874000   \n",
              "\n",
              "              131         132         133         134         135         136  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.025044   -0.037212   -0.072808    0.063464    0.008856   -0.036892   \n",
              "std      1.009362    1.071714    1.000910    0.961140    0.996527    0.963481   \n",
              "min     -2.806000   -2.466000   -3.279000   -2.470000   -2.856000   -2.687000   \n",
              "25%     -0.642250   -0.825500   -0.734000   -0.680000   -0.677500   -0.675000   \n",
              "50%      0.072500    0.048500   -0.106000    0.123000    0.036500   -0.091000   \n",
              "75%      0.763250    0.628750    0.631000    0.770500    0.785750    0.617250   \n",
              "max      2.395000    3.170000    2.945000    2.848000    2.447000    2.649000   \n",
              "\n",
              "              137         138         139         140         141        142  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.00000   \n",
              "mean    -0.034632   -0.017148    0.022340    0.042056    0.018392    0.04880   \n",
              "std      1.033276    1.013884    0.977589    1.044865    1.067007    0.96140   \n",
              "min     -2.813000   -3.033000   -2.332000   -2.428000   -3.181000   -2.80600   \n",
              "25%     -0.738500   -0.757500   -0.706500   -0.745000   -0.819500   -0.59350   \n",
              "50%     -0.003000   -0.008000    0.076000    0.037500    0.049000    0.10150   \n",
              "75%      0.678000    0.681750    0.743000    0.748750    0.821750    0.71375   \n",
              "max      2.748000    3.000000    2.775000    3.270000    2.816000    3.20100   \n",
              "\n",
              "              143         144         145         146         147         148  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.054828   -0.006960   -0.078104   -0.074484   -0.113868   -0.006252   \n",
              "std      0.986516    0.975603    1.007537    0.954159    0.989752    0.957323   \n",
              "min     -2.411000   -2.430000   -3.110000   -2.944000   -2.901000   -2.394000   \n",
              "25%     -0.581500   -0.752750   -0.801250   -0.689750   -0.753250   -0.611750   \n",
              "50%      0.070500   -0.064500   -0.051500   -0.026500   -0.208000    0.018500   \n",
              "75%      0.647500    0.754750    0.647250    0.609250    0.593750    0.688250   \n",
              "max      2.752000    3.048000    2.720000    2.137000    2.298000    2.506000   \n",
              "\n",
              "              149         150         151         152         153         154  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.089764   -0.057740   -0.060148    0.045800   -0.028772    0.015844   \n",
              "std      1.060648    1.053604    0.974275    1.038575    1.056589    1.009334   \n",
              "min     -3.230000   -3.024000   -2.658000   -3.382000   -2.755000   -2.712000   \n",
              "25%     -0.618000   -0.674000   -0.703250   -0.704500   -0.764750   -0.644500   \n",
              "50%      0.041500   -0.035000   -0.017000   -0.003000    0.003500    0.025500   \n",
              "75%      0.750250    0.571500    0.582250    0.732250    0.710250    0.702000   \n",
              "max      3.523000    2.974000    2.610000    2.996000    3.431000    2.854000   \n",
              "\n",
              "              155         156         157         158         159         160  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.014892    0.000808   -0.148604    0.034092   -0.060968   -0.046732   \n",
              "std      0.920187    0.974076    1.046082    1.043446    0.989547    0.928692   \n",
              "min     -3.008000   -2.815000   -3.118000   -2.138000   -2.964000   -2.370000   \n",
              "25%     -0.571750   -0.677250   -0.870000   -0.787750   -0.719000   -0.693000   \n",
              "50%      0.067500   -0.005500   -0.206500    0.083000   -0.081500   -0.108500   \n",
              "75%      0.675250    0.700000    0.582750    0.740000    0.646250    0.613000   \n",
              "max      1.959000    3.555000    2.930000    2.728000    3.016000    2.250000   \n",
              "\n",
              "              161         162         163         164         165         166  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.034764    0.042196   -0.011332   -0.042468   -0.022832   -0.008856   \n",
              "std      0.975756    1.068247    0.998016    0.945895    1.037263    1.005934   \n",
              "min     -2.711000   -2.746000   -2.509000   -2.238000   -3.516000   -3.205000   \n",
              "25%     -0.588250   -0.628000   -0.771750   -0.752250   -0.785750   -0.728750   \n",
              "50%     -0.080000    0.057500   -0.026500   -0.063500   -0.139000   -0.126000   \n",
              "75%      0.585250    0.807750    0.631750    0.622500    0.652500    0.687000   \n",
              "max      2.889000    2.522000    2.747000    3.154000    2.805000    2.520000   \n",
              "\n",
              "              167         168         169         170         171         172  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.143128    0.012248   -0.059912   -0.076204   -0.021248   -0.011724   \n",
              "std      0.985518    0.889416    0.968491    1.043838    0.986582    0.978644   \n",
              "min     -2.524000   -2.312000   -2.858000   -2.823000   -3.032000   -2.449000   \n",
              "25%     -0.554500   -0.617000   -0.692500   -0.831500   -0.683750   -0.713750   \n",
              "50%      0.189000   -0.073000   -0.135000   -0.116500    0.024000   -0.057000   \n",
              "75%      0.841500    0.639000    0.633500    0.587000    0.670000    0.595000   \n",
              "max      2.621000    2.364000    2.844000    2.483000    2.589000    2.394000   \n",
              "\n",
              "              173         174         175         176         177         178  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.003372    0.046936   -0.113800   -0.040764   -0.000448    0.017132   \n",
              "std      1.061376    1.020330    0.996019    0.987487    0.977769    1.028095   \n",
              "min     -3.251000   -2.922000   -2.983000   -2.945000   -2.292000   -2.352000   \n",
              "25%     -0.724500   -0.692250   -0.703500   -0.703000   -0.629250   -0.720250   \n",
              "50%     -0.054000   -0.002000   -0.124000    0.017500    0.030500    0.074000   \n",
              "75%      0.695750    0.787000    0.485500    0.699750    0.614000    0.756750   \n",
              "max      2.975000    2.375000    2.546000    2.214000    3.160000    2.879000   \n",
              "\n",
              "              179        180         181         182         183         184  \\\n",
              "count  250.000000  250.00000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.003512    0.11202    0.056900    0.019344    0.005128   -0.028700   \n",
              "std      0.899947    1.02915    0.930588    0.904465    0.969452    1.030067   \n",
              "min     -2.454000   -2.93000   -2.782000   -2.471000   -3.118000   -3.155000   \n",
              "25%     -0.623500   -0.53750   -0.534750   -0.618250   -0.710500   -0.641250   \n",
              "50%     -0.083000    0.10800    0.101000    0.033000   -0.013500    0.013000   \n",
              "75%      0.678750    0.83600    0.619250    0.674500    0.588750    0.538500   \n",
              "max      2.553000    2.65600    2.562000    2.312000    2.943000    2.839000   \n",
              "\n",
              "              185         186         187         188         189         190  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.016536    0.095228    0.040540   -0.086368    0.018672   -0.005040   \n",
              "std      0.910916    1.006043    1.069322    0.945655    0.991395    1.021503   \n",
              "min     -2.045000   -3.315000   -3.059000   -2.360000   -2.813000   -3.701000   \n",
              "25%     -0.597750   -0.500250   -0.649250   -0.733250   -0.659750   -0.560250   \n",
              "50%     -0.012500    0.106500    0.039000   -0.110500   -0.075500    0.048000   \n",
              "75%      0.631000    0.811250    0.812000    0.565000    0.694500    0.608500   \n",
              "max      2.977000    2.366000    2.892000    2.152000    2.534000    2.893000   \n",
              "\n",
              "              191         192         193         194         195         196  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.027916    0.018764    0.089004    0.069460   -0.002348   -0.046272   \n",
              "std      1.056613    1.042937    0.950353    0.967331    1.003769    1.060352   \n",
              "min     -3.692000   -2.761000   -2.583000   -2.485000   -2.683000   -3.218000   \n",
              "25%     -0.713500   -0.726000   -0.604000   -0.594250   -0.692250   -0.818750   \n",
              "50%     -0.064000    0.073000    0.132000    0.060000   -0.003000    0.009500   \n",
              "75%      0.692750    0.689000    0.724250    0.687750    0.696750    0.691000   \n",
              "max      2.936000    3.763000    2.430000    2.651000    2.777000    2.510000   \n",
              "\n",
              "              197         198         199         200         201         202  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.004520   -0.093012   -0.079264   -0.030500   -0.007548    0.038812   \n",
              "std      0.967092    0.989061    1.039407    0.956564    1.057546    0.987352   \n",
              "min     -2.776000   -2.699000   -3.521000   -2.706000   -2.667000   -3.085000   \n",
              "25%     -0.674500   -0.748000   -0.853750   -0.692000   -0.701500   -0.542500   \n",
              "50%     -0.019000   -0.119500   -0.017000   -0.062000    0.007000    0.052000   \n",
              "75%      0.715750    0.585250    0.674750    0.641250    0.654500    0.599500   \n",
              "max      2.387000    2.344000    2.185000    3.084000    3.022000    3.435000   \n",
              "\n",
              "              203         204         205         206         207         208  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.082516    0.046000    0.076528   -0.061012    0.011580   -0.041820   \n",
              "std      0.963755    0.932865    1.056818    0.962153    1.005787    0.937099   \n",
              "min     -2.508000   -2.717000   -2.693000   -3.230000   -2.677000   -2.699000   \n",
              "25%     -0.549750   -0.532750   -0.640750   -0.699250   -0.686500   -0.652750   \n",
              "50%      0.032500   -0.001500   -0.014500    0.010500   -0.024000   -0.026000   \n",
              "75%      0.630750    0.633000    0.886750    0.520750    0.710750    0.511000   \n",
              "max      2.597000    3.112000    2.424000    2.835000    2.654000    2.519000   \n",
              "\n",
              "              209         210         211         212         213         214  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.025816   -0.053976   -0.043388    0.087544    0.029264    0.032324   \n",
              "std      1.009785    0.945263    0.919409    1.032664    1.043265    1.007421   \n",
              "min     -2.541000   -2.733000   -2.771000   -2.466000   -2.886000   -2.818000   \n",
              "25%     -0.650500   -0.745750   -0.610750   -0.663500   -0.697750   -0.703750   \n",
              "50%     -0.004000    0.042000   -0.020000    0.060000    0.049500    0.095500   \n",
              "75%      0.701250    0.602500    0.536750    0.771000    0.740250    0.648250   \n",
              "max      2.411000    3.132000    2.541000    3.302000    2.646000    2.443000   \n",
              "\n",
              "              215         216         217         218         219         220  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.014004    0.011668    0.041676   -0.070524    0.079772   -0.030328   \n",
              "std      0.992384    0.962415    1.016528    0.954239    1.002974    0.989978   \n",
              "min     -2.516000   -2.811000   -2.678000   -2.730000   -2.510000   -2.829000   \n",
              "25%     -0.674750   -0.643000   -0.661500   -0.656500   -0.627250   -0.743500   \n",
              "50%     -0.016500    0.022500    0.078000   -0.135500    0.050500    0.032000   \n",
              "75%      0.687500    0.636500    0.693000    0.522250    0.700500    0.722250   \n",
              "max      3.402000    2.933000    2.743000    2.493000    3.602000    2.149000   \n",
              "\n",
              "              221         222         223         224         225         226  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.035720   -0.057568    0.018484   -0.086640    0.038140    0.086800   \n",
              "std      0.925272    0.989053    1.028179    1.019368    0.996159    0.931904   \n",
              "min     -3.827000   -2.735000   -2.769000   -2.500000   -3.464000   -2.783000   \n",
              "25%     -0.577250   -0.708250   -0.718250   -0.791500   -0.644750   -0.496000   \n",
              "50%     -0.057500   -0.111000    0.036000   -0.120500    0.079500    0.157000   \n",
              "75%      0.537000    0.658000    0.702250    0.645250    0.728500    0.689000   \n",
              "max      2.045000    2.628000    2.696000    2.347000    2.856000    3.364000   \n",
              "\n",
              "              227         228         229         230         231         232  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.016068   -0.200064   -0.008108    0.019748   -0.058592    0.030492   \n",
              "std      1.103765    0.987543    1.065948    1.010590    0.948761    0.979871   \n",
              "min     -3.604000   -2.395000   -2.566000   -2.979000   -2.617000   -3.133000   \n",
              "25%     -0.655750   -0.871500   -0.779000   -0.767500   -0.671500   -0.570500   \n",
              "50%     -0.029000   -0.170000   -0.001000   -0.004500   -0.146500    0.025500   \n",
              "75%      0.812750    0.502000    0.732750    0.769250    0.568250    0.651000   \n",
              "max      2.557000    1.967000    2.614000    2.723000    2.451000    3.140000   \n",
              "\n",
              "              233        234        235         236         237         238  \\\n",
              "count  250.000000  250.00000  250.00000  250.000000  250.000000  250.000000   \n",
              "mean     0.092460    0.07190   -0.02140    0.027644   -0.096620    0.027476   \n",
              "std      1.011076    0.98353    0.99673    0.920217    0.994774    1.072805   \n",
              "min     -2.967000   -3.06200   -3.02700   -2.531000   -3.450000   -2.788000   \n",
              "25%     -0.613000   -0.58175   -0.62500   -0.598500   -0.756000   -0.746750   \n",
              "50%      0.103500    0.14900   -0.01850    0.097500   -0.078500   -0.058000   \n",
              "75%      0.815000    0.75500    0.67275    0.665000    0.587750    0.726000   \n",
              "max      2.410000    2.91600    2.33800    2.544000    2.194000    3.092000   \n",
              "\n",
              "              239        240         241         242         243         244  \\\n",
              "count  250.000000  250.00000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.044340   -0.05460   -0.089084    0.022528    0.189628    0.048204   \n",
              "std      0.969193    1.01953    0.962420    1.001536    1.077355    1.020979   \n",
              "min     -2.530000   -2.59500   -2.480000   -2.648000   -2.403000   -3.921000   \n",
              "25%     -0.656000   -0.75700   -0.675750   -0.669000   -0.553000   -0.628500   \n",
              "50%     -0.012000   -0.00250   -0.111500    0.019500    0.129500    0.072000   \n",
              "75%      0.574250    0.66525    0.460250    0.772250    0.869250    0.700000   \n",
              "max      2.979000    2.93700    3.069000    2.560000    3.334000    2.528000   \n",
              "\n",
              "              245         246         247         248         249         250  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.034680    0.114060    0.023388    0.082356    0.120912    0.037056   \n",
              "std      0.897634    1.039991    1.059544    1.052645    0.949140    1.066471   \n",
              "min     -2.851000   -2.526000   -3.202000   -2.421000   -2.143000   -3.186000   \n",
              "25%     -0.683500   -0.577500   -0.741750   -0.638500   -0.553000   -0.675750   \n",
              "50%     -0.031000    0.084500    0.068500    0.043500    0.115500    0.096000   \n",
              "75%      0.600500    0.765750    0.751500    0.783250    0.720000    0.769000   \n",
              "max      2.523000    3.083000    2.634000    2.377000    2.716000    2.589000   \n",
              "\n",
              "              251         252         253         254         255         256  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.092040    0.023136   -0.078856   -0.026748   -0.019820   -0.025568   \n",
              "std      1.002997    1.029485    1.044701    1.050109    0.973342    1.008486   \n",
              "min     -2.269000   -2.422000   -2.865000   -3.629000   -2.601000   -2.731000   \n",
              "25%     -0.629500   -0.644500   -0.778250   -0.727750   -0.713500   -0.734000   \n",
              "50%      0.062500   -0.057500   -0.101500   -0.001500   -0.056000    0.015500   \n",
              "75%      0.758000    0.678000    0.604000    0.667250    0.616250    0.608250   \n",
              "max      2.538000    3.617000    3.113000    2.842000    2.625000    2.019000   \n",
              "\n",
              "              257         258         259         260         261         262  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.033840   -0.037772    0.042652    0.005780   -0.102304   -0.013796   \n",
              "std      0.986381    0.990875    1.060948    0.994761    1.094494    1.026025   \n",
              "min     -3.005000   -2.773000   -2.510000   -2.512000   -2.873000   -2.549000   \n",
              "25%     -0.637750   -0.731500   -0.643250   -0.622750   -1.009250   -0.693250   \n",
              "50%     -0.041500   -0.016500    0.058000   -0.048500   -0.134000   -0.065500   \n",
              "75%      0.642750    0.616250    0.778500    0.675750    0.708750    0.726000   \n",
              "max      2.581000    2.774000    2.708000    2.612000    2.680000    2.964000   \n",
              "\n",
              "              263         264         265         266         267         268  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.089384    0.036368    0.016276   -0.069448   -0.113236    0.035696   \n",
              "std      0.963489    1.026373    1.008207    0.989451    1.002857    0.944743   \n",
              "min     -2.721000   -2.578000   -2.239000   -3.046000   -2.755000   -2.507000   \n",
              "25%     -0.567750   -0.696500   -0.684000   -0.703750   -0.771250   -0.624500   \n",
              "50%      0.097000   -0.088000    0.019500    0.001500   -0.107500    0.045500   \n",
              "75%      0.739250    0.856750    0.709250    0.616000    0.569500    0.655750   \n",
              "max      2.663000    2.406000    3.457000    2.496000    2.501000    2.832000   \n",
              "\n",
              "              269         270         271         272         273         274  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.034484   -0.066236   -0.057988    0.091556   -0.029896    0.115648   \n",
              "std      1.023709    0.985451    0.951879    1.027877    0.966882    1.037173   \n",
              "min     -3.369000   -2.448000   -2.771000   -2.903000   -2.522000   -2.759000   \n",
              "25%     -0.653000   -0.786750   -0.701000   -0.543250   -0.672750   -0.626750   \n",
              "50%     -0.023500   -0.101000   -0.109000    0.050500   -0.081500    0.087500   \n",
              "75%      0.634750    0.525750    0.612000    0.864000    0.630250    0.836750   \n",
              "max      2.897000    3.753000    2.498000    2.725000    2.680000    3.445000   \n",
              "\n",
              "              275         276         277         278         279         280  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.007372    0.033552    0.090524    0.001576   -0.007784    0.043184   \n",
              "std      1.004543    1.006219    1.037119    1.024067    1.056086    1.012516   \n",
              "min     -2.915000   -2.618000   -3.623000   -2.673000   -3.229000   -2.537000   \n",
              "25%     -0.730250   -0.649750   -0.589500   -0.725750   -0.667750   -0.605000   \n",
              "50%      0.035000    0.086500    0.126500    0.053500    0.063500    0.052500   \n",
              "75%      0.718000    0.791500    0.726000    0.688000    0.665750    0.603750   \n",
              "max      2.846000    2.315000    2.780000    2.364000    2.908000    2.926000   \n",
              "\n",
              "              281         282         283         284         285         286  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.082696    0.098476    0.055356    0.111708   -0.015688    0.035992   \n",
              "std      1.068741    0.934163    0.988100    1.043230    1.010720    1.058982   \n",
              "min     -2.748000   -2.850000   -2.577000   -2.973000   -2.709000   -3.605000   \n",
              "25%     -0.637750   -0.458250   -0.553500   -0.566750   -0.778250   -0.693250   \n",
              "50%      0.103000    0.135500    0.039000    0.093500    0.014500   -0.007500   \n",
              "75%      0.705500    0.688250    0.759000    0.704750    0.673750    0.748750   \n",
              "max      3.441000    2.319000    2.842000    3.343000    3.266000    3.061000   \n",
              "\n",
              "              287         288         289         290         291         292  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean     0.026452   -0.059152    0.077272    0.044652    0.126344    0.018436   \n",
              "std      0.896318    1.113760    0.972530    1.011416    0.972567    0.954229   \n",
              "min     -2.357000   -2.904000   -2.734000   -2.804000   -2.443000   -2.757000   \n",
              "25%     -0.596750   -0.789000   -0.671250   -0.617000   -0.510500   -0.535750   \n",
              "50%      0.000500   -0.122500    0.057500    0.067500    0.091000    0.057500   \n",
              "75%      0.604250    0.650500    0.772500    0.797250    0.804250    0.631500   \n",
              "max      2.146000    2.853000    3.026000    2.865000    2.801000    2.736000   \n",
              "\n",
              "              293         294         295         296         297         298  \\\n",
              "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
              "mean    -0.012092   -0.065720   -0.106112    0.046472    0.006452    0.009372   \n",
              "std      0.960630    1.057414    1.038389    0.967661    0.998984    1.008099   \n",
              "min     -2.466000   -3.287000   -3.072000   -2.634000   -2.776000   -3.211000   \n",
              "25%     -0.657000   -0.818500   -0.821000   -0.605500   -0.751250   -0.550000   \n",
              "50%     -0.021000   -0.009000   -0.079500    0.009500    0.005500   -0.009000   \n",
              "75%      0.650250    0.739500    0.493000    0.683000    0.794250    0.654250   \n",
              "max      2.596000    2.226000    3.131000    3.236000    2.626000    3.530000   \n",
              "\n",
              "              299  \n",
              "count  250.000000  \n",
              "mean    -0.128952  \n",
              "std      0.971219  \n",
              "min     -3.500000  \n",
              "25%     -0.754250  \n",
              "50%     -0.132500  \n",
              "75%      0.503250  \n",
              "max      2.771000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P0arf8p40xr"
      },
      "source": [
        "Another thing that has become clear at this point is that our target variable is binary, meaning it only takes values $y_i \\in [0, 1]$. We will visualize the distribution of target variable in [$\\S$Target](#target)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "MBMAk5KImQcp",
        "outputId": "85e852b4-6187-490c-e079-55c144139dc7"
      },
      "source": [
        "test_subm.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "      <th>201</th>\n",
              "      <th>202</th>\n",
              "      <th>203</th>\n",
              "      <th>204</th>\n",
              "      <th>205</th>\n",
              "      <th>206</th>\n",
              "      <th>207</th>\n",
              "      <th>208</th>\n",
              "      <th>209</th>\n",
              "      <th>210</th>\n",
              "      <th>211</th>\n",
              "      <th>212</th>\n",
              "      <th>213</th>\n",
              "      <th>214</th>\n",
              "      <th>215</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "      <td>19750.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.014043</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.005145</td>\n",
              "      <td>-0.003525</td>\n",
              "      <td>0.003394</td>\n",
              "      <td>0.002738</td>\n",
              "      <td>0.004213</td>\n",
              "      <td>-0.010618</td>\n",
              "      <td>-0.003211</td>\n",
              "      <td>-0.002738</td>\n",
              "      <td>-0.003261</td>\n",
              "      <td>0.007411</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.011621</td>\n",
              "      <td>-0.003081</td>\n",
              "      <td>0.006932</td>\n",
              "      <td>-0.000170</td>\n",
              "      <td>-0.000793</td>\n",
              "      <td>0.004675</td>\n",
              "      <td>-0.019747</td>\n",
              "      <td>0.007058</td>\n",
              "      <td>-0.006713</td>\n",
              "      <td>0.003181</td>\n",
              "      <td>-0.007348</td>\n",
              "      <td>0.004626</td>\n",
              "      <td>-0.002919</td>\n",
              "      <td>0.014807</td>\n",
              "      <td>0.002969</td>\n",
              "      <td>0.005930</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>-0.003012</td>\n",
              "      <td>-0.002511</td>\n",
              "      <td>0.006480</td>\n",
              "      <td>-0.012845</td>\n",
              "      <td>0.009695</td>\n",
              "      <td>0.010524</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>0.010531</td>\n",
              "      <td>0.009442</td>\n",
              "      <td>0.012217</td>\n",
              "      <td>-0.007291</td>\n",
              "      <td>-0.001280</td>\n",
              "      <td>0.007256</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>0.004484</td>\n",
              "      <td>-0.003157</td>\n",
              "      <td>0.004098</td>\n",
              "      <td>-0.006166</td>\n",
              "      <td>0.003410</td>\n",
              "      <td>0.005602</td>\n",
              "      <td>-0.016488</td>\n",
              "      <td>-0.005769</td>\n",
              "      <td>-0.007329</td>\n",
              "      <td>0.002661</td>\n",
              "      <td>0.010716</td>\n",
              "      <td>-0.003806</td>\n",
              "      <td>-0.005069</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>-0.007120</td>\n",
              "      <td>-0.013874</td>\n",
              "      <td>-0.002446</td>\n",
              "      <td>0.002950</td>\n",
              "      <td>0.006920</td>\n",
              "      <td>-0.007923</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>-0.011179</td>\n",
              "      <td>0.009296</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>-0.005079</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>-0.000171</td>\n",
              "      <td>-0.004877</td>\n",
              "      <td>-0.001313</td>\n",
              "      <td>0.003192</td>\n",
              "      <td>-0.001605</td>\n",
              "      <td>0.010409</td>\n",
              "      <td>-0.001248</td>\n",
              "      <td>0.007097</td>\n",
              "      <td>-0.002587</td>\n",
              "      <td>-0.005448</td>\n",
              "      <td>-0.002641</td>\n",
              "      <td>0.004750</td>\n",
              "      <td>0.002633</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>-0.001590</td>\n",
              "      <td>-0.005564</td>\n",
              "      <td>0.001942</td>\n",
              "      <td>-0.011500</td>\n",
              "      <td>0.001583</td>\n",
              "      <td>-0.011064</td>\n",
              "      <td>-0.007548</td>\n",
              "      <td>-0.010606</td>\n",
              "      <td>0.008324</td>\n",
              "      <td>-0.006834</td>\n",
              "      <td>-0.005020</td>\n",
              "      <td>0.006657</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.005931</td>\n",
              "      <td>-0.003778</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>-0.000845</td>\n",
              "      <td>-0.011630</td>\n",
              "      <td>-0.001109</td>\n",
              "      <td>0.007661</td>\n",
              "      <td>-0.015267</td>\n",
              "      <td>0.006328</td>\n",
              "      <td>0.004839</td>\n",
              "      <td>-0.005247</td>\n",
              "      <td>-0.000605</td>\n",
              "      <td>-0.004141</td>\n",
              "      <td>0.008242</td>\n",
              "      <td>0.010468</td>\n",
              "      <td>-0.007527</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.005567</td>\n",
              "      <td>0.009830</td>\n",
              "      <td>-0.009007</td>\n",
              "      <td>0.005505</td>\n",
              "      <td>-0.006416</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>-0.007685</td>\n",
              "      <td>-0.001432</td>\n",
              "      <td>-0.001247</td>\n",
              "      <td>-0.000216</td>\n",
              "      <td>0.002911</td>\n",
              "      <td>-0.005041</td>\n",
              "      <td>-0.000581</td>\n",
              "      <td>-0.006911</td>\n",
              "      <td>-0.008748</td>\n",
              "      <td>-0.016622</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>-0.009712</td>\n",
              "      <td>-0.002784</td>\n",
              "      <td>-0.009361</td>\n",
              "      <td>0.004658</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.002604</td>\n",
              "      <td>-0.009450</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.007427</td>\n",
              "      <td>-0.010823</td>\n",
              "      <td>-0.007322</td>\n",
              "      <td>0.002940</td>\n",
              "      <td>0.005598</td>\n",
              "      <td>-0.007599</td>\n",
              "      <td>0.004634</td>\n",
              "      <td>0.002762</td>\n",
              "      <td>0.003087</td>\n",
              "      <td>0.001680</td>\n",
              "      <td>-0.002814</td>\n",
              "      <td>0.002964</td>\n",
              "      <td>0.010122</td>\n",
              "      <td>-0.001446</td>\n",
              "      <td>-0.007203</td>\n",
              "      <td>0.011646</td>\n",
              "      <td>0.010156</td>\n",
              "      <td>0.004490</td>\n",
              "      <td>-0.009510</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>-0.004967</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>0.004311</td>\n",
              "      <td>-0.007783</td>\n",
              "      <td>-0.003442</td>\n",
              "      <td>-0.009652</td>\n",
              "      <td>-0.002678</td>\n",
              "      <td>-0.005350</td>\n",
              "      <td>0.001603</td>\n",
              "      <td>-0.002479</td>\n",
              "      <td>-0.003839</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>-0.001848</td>\n",
              "      <td>-0.001913</td>\n",
              "      <td>0.009593</td>\n",
              "      <td>-0.003361</td>\n",
              "      <td>0.009558</td>\n",
              "      <td>0.000480</td>\n",
              "      <td>0.008838</td>\n",
              "      <td>0.008749</td>\n",
              "      <td>-0.003281</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>-0.001422</td>\n",
              "      <td>-0.002335</td>\n",
              "      <td>0.002579</td>\n",
              "      <td>0.001421</td>\n",
              "      <td>-0.006455</td>\n",
              "      <td>-0.014659</td>\n",
              "      <td>-0.005065</td>\n",
              "      <td>0.005232</td>\n",
              "      <td>0.008112</td>\n",
              "      <td>-0.004780</td>\n",
              "      <td>-0.006264</td>\n",
              "      <td>-0.002444</td>\n",
              "      <td>-0.007437</td>\n",
              "      <td>-0.002259</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>-0.000357</td>\n",
              "      <td>0.004202</td>\n",
              "      <td>0.013911</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>-0.008164</td>\n",
              "      <td>-0.008366</td>\n",
              "      <td>0.003105</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>-0.004260</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>0.012737</td>\n",
              "      <td>-0.002243</td>\n",
              "      <td>0.004246</td>\n",
              "      <td>-0.004715</td>\n",
              "      <td>0.004819</td>\n",
              "      <td>-0.000325</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>-0.001406</td>\n",
              "      <td>0.009292</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>-0.002219</td>\n",
              "      <td>-0.004088</td>\n",
              "      <td>-0.007355</td>\n",
              "      <td>0.003813</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>-0.018762</td>\n",
              "      <td>0.012009</td>\n",
              "      <td>0.009399</td>\n",
              "      <td>-0.006499</td>\n",
              "      <td>-0.004309</td>\n",
              "      <td>-0.010915</td>\n",
              "      <td>0.005617</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>0.001938</td>\n",
              "      <td>-0.005009</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>-0.010274</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.003372</td>\n",
              "      <td>-0.002776</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>0.006598</td>\n",
              "      <td>-0.008309</td>\n",
              "      <td>-0.009133</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>-0.002006</td>\n",
              "      <td>-0.001088</td>\n",
              "      <td>-0.007761</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>-0.013075</td>\n",
              "      <td>-0.008433</td>\n",
              "      <td>0.009949</td>\n",
              "      <td>-0.002582</td>\n",
              "      <td>-0.006509</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>-0.004386</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>-0.007665</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>-0.011357</td>\n",
              "      <td>0.004223</td>\n",
              "      <td>0.008783</td>\n",
              "      <td>0.008566</td>\n",
              "      <td>-0.003155</td>\n",
              "      <td>-0.009304</td>\n",
              "      <td>0.005396</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.010622</td>\n",
              "      <td>-0.001993</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>-0.007535</td>\n",
              "      <td>0.004659</td>\n",
              "      <td>-0.002114</td>\n",
              "      <td>-0.008629</td>\n",
              "      <td>0.014307</td>\n",
              "      <td>-0.001669</td>\n",
              "      <td>-0.001801</td>\n",
              "      <td>0.007137</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>-0.006014</td>\n",
              "      <td>-0.004159</td>\n",
              "      <td>0.003853</td>\n",
              "      <td>-0.004600</td>\n",
              "      <td>0.002577</td>\n",
              "      <td>-0.010130</td>\n",
              "      <td>-0.003961</td>\n",
              "      <td>0.012793</td>\n",
              "      <td>0.009063</td>\n",
              "      <td>0.007512</td>\n",
              "      <td>-0.004283</td>\n",
              "      <td>-0.001203</td>\n",
              "      <td>0.013076</td>\n",
              "      <td>0.000070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.003779</td>\n",
              "      <td>0.993955</td>\n",
              "      <td>1.000809</td>\n",
              "      <td>1.008545</td>\n",
              "      <td>1.002826</td>\n",
              "      <td>1.002917</td>\n",
              "      <td>0.994315</td>\n",
              "      <td>0.997972</td>\n",
              "      <td>0.996938</td>\n",
              "      <td>1.000688</td>\n",
              "      <td>1.006049</td>\n",
              "      <td>0.998012</td>\n",
              "      <td>1.002858</td>\n",
              "      <td>0.998698</td>\n",
              "      <td>1.004494</td>\n",
              "      <td>0.999509</td>\n",
              "      <td>1.005204</td>\n",
              "      <td>1.000772</td>\n",
              "      <td>1.003637</td>\n",
              "      <td>1.001289</td>\n",
              "      <td>0.996804</td>\n",
              "      <td>0.992845</td>\n",
              "      <td>0.997785</td>\n",
              "      <td>1.003841</td>\n",
              "      <td>1.005888</td>\n",
              "      <td>0.996975</td>\n",
              "      <td>0.999497</td>\n",
              "      <td>0.997001</td>\n",
              "      <td>1.008973</td>\n",
              "      <td>1.004791</td>\n",
              "      <td>0.995823</td>\n",
              "      <td>0.998580</td>\n",
              "      <td>1.007158</td>\n",
              "      <td>0.995098</td>\n",
              "      <td>0.999363</td>\n",
              "      <td>0.996513</td>\n",
              "      <td>0.996013</td>\n",
              "      <td>1.004946</td>\n",
              "      <td>1.002930</td>\n",
              "      <td>1.004507</td>\n",
              "      <td>0.993414</td>\n",
              "      <td>1.004133</td>\n",
              "      <td>0.995491</td>\n",
              "      <td>0.997396</td>\n",
              "      <td>1.001088</td>\n",
              "      <td>0.999344</td>\n",
              "      <td>1.007084</td>\n",
              "      <td>1.000939</td>\n",
              "      <td>0.996475</td>\n",
              "      <td>0.991990</td>\n",
              "      <td>0.997686</td>\n",
              "      <td>1.001566</td>\n",
              "      <td>1.001679</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>0.996476</td>\n",
              "      <td>0.995495</td>\n",
              "      <td>1.000273</td>\n",
              "      <td>0.999553</td>\n",
              "      <td>1.001529</td>\n",
              "      <td>0.987338</td>\n",
              "      <td>1.002628</td>\n",
              "      <td>0.995847</td>\n",
              "      <td>1.004985</td>\n",
              "      <td>1.002373</td>\n",
              "      <td>0.992078</td>\n",
              "      <td>1.006548</td>\n",
              "      <td>1.002983</td>\n",
              "      <td>1.000911</td>\n",
              "      <td>1.004782</td>\n",
              "      <td>1.002772</td>\n",
              "      <td>0.994713</td>\n",
              "      <td>0.996518</td>\n",
              "      <td>1.003167</td>\n",
              "      <td>1.002726</td>\n",
              "      <td>1.001047</td>\n",
              "      <td>1.003915</td>\n",
              "      <td>0.988774</td>\n",
              "      <td>1.001685</td>\n",
              "      <td>0.988354</td>\n",
              "      <td>0.997489</td>\n",
              "      <td>1.002499</td>\n",
              "      <td>0.998147</td>\n",
              "      <td>0.992640</td>\n",
              "      <td>0.994977</td>\n",
              "      <td>1.006194</td>\n",
              "      <td>0.997751</td>\n",
              "      <td>1.000488</td>\n",
              "      <td>0.995039</td>\n",
              "      <td>1.008350</td>\n",
              "      <td>0.988202</td>\n",
              "      <td>0.991236</td>\n",
              "      <td>0.993122</td>\n",
              "      <td>1.003816</td>\n",
              "      <td>1.002676</td>\n",
              "      <td>1.001523</td>\n",
              "      <td>0.993994</td>\n",
              "      <td>0.987653</td>\n",
              "      <td>1.002141</td>\n",
              "      <td>0.995461</td>\n",
              "      <td>0.995942</td>\n",
              "      <td>1.006144</td>\n",
              "      <td>1.010039</td>\n",
              "      <td>1.000669</td>\n",
              "      <td>1.003929</td>\n",
              "      <td>1.004093</td>\n",
              "      <td>1.000796</td>\n",
              "      <td>1.009028</td>\n",
              "      <td>1.005079</td>\n",
              "      <td>1.001791</td>\n",
              "      <td>1.003586</td>\n",
              "      <td>0.999044</td>\n",
              "      <td>1.007043</td>\n",
              "      <td>0.999238</td>\n",
              "      <td>1.011722</td>\n",
              "      <td>1.001596</td>\n",
              "      <td>1.010089</td>\n",
              "      <td>0.998132</td>\n",
              "      <td>0.992286</td>\n",
              "      <td>0.989048</td>\n",
              "      <td>1.002759</td>\n",
              "      <td>1.002140</td>\n",
              "      <td>1.010807</td>\n",
              "      <td>1.006769</td>\n",
              "      <td>1.003961</td>\n",
              "      <td>0.999974</td>\n",
              "      <td>1.010938</td>\n",
              "      <td>0.998090</td>\n",
              "      <td>1.001861</td>\n",
              "      <td>0.996236</td>\n",
              "      <td>1.002005</td>\n",
              "      <td>1.002279</td>\n",
              "      <td>0.998812</td>\n",
              "      <td>1.001232</td>\n",
              "      <td>0.996635</td>\n",
              "      <td>1.003509</td>\n",
              "      <td>1.001250</td>\n",
              "      <td>0.994444</td>\n",
              "      <td>1.001023</td>\n",
              "      <td>1.001700</td>\n",
              "      <td>1.007590</td>\n",
              "      <td>1.006907</td>\n",
              "      <td>0.992410</td>\n",
              "      <td>0.998474</td>\n",
              "      <td>1.003158</td>\n",
              "      <td>0.994584</td>\n",
              "      <td>1.000374</td>\n",
              "      <td>1.000797</td>\n",
              "      <td>1.000051</td>\n",
              "      <td>1.000688</td>\n",
              "      <td>0.992447</td>\n",
              "      <td>1.006739</td>\n",
              "      <td>1.002076</td>\n",
              "      <td>1.001117</td>\n",
              "      <td>1.008009</td>\n",
              "      <td>0.993495</td>\n",
              "      <td>0.997993</td>\n",
              "      <td>0.998877</td>\n",
              "      <td>0.990670</td>\n",
              "      <td>1.006781</td>\n",
              "      <td>1.006846</td>\n",
              "      <td>1.008801</td>\n",
              "      <td>1.003776</td>\n",
              "      <td>0.991647</td>\n",
              "      <td>1.005664</td>\n",
              "      <td>1.002534</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.999894</td>\n",
              "      <td>1.004215</td>\n",
              "      <td>1.009588</td>\n",
              "      <td>0.995884</td>\n",
              "      <td>0.999296</td>\n",
              "      <td>0.994499</td>\n",
              "      <td>1.003773</td>\n",
              "      <td>0.999609</td>\n",
              "      <td>0.992976</td>\n",
              "      <td>1.006689</td>\n",
              "      <td>0.999546</td>\n",
              "      <td>0.994778</td>\n",
              "      <td>1.001713</td>\n",
              "      <td>0.996545</td>\n",
              "      <td>0.998949</td>\n",
              "      <td>0.999482</td>\n",
              "      <td>1.002993</td>\n",
              "      <td>1.005222</td>\n",
              "      <td>1.009192</td>\n",
              "      <td>1.008064</td>\n",
              "      <td>0.997152</td>\n",
              "      <td>1.001473</td>\n",
              "      <td>1.002011</td>\n",
              "      <td>1.005683</td>\n",
              "      <td>1.001773</td>\n",
              "      <td>0.998715</td>\n",
              "      <td>0.999914</td>\n",
              "      <td>1.000233</td>\n",
              "      <td>1.003445</td>\n",
              "      <td>1.003014</td>\n",
              "      <td>1.002330</td>\n",
              "      <td>0.995812</td>\n",
              "      <td>0.994961</td>\n",
              "      <td>1.003238</td>\n",
              "      <td>1.012109</td>\n",
              "      <td>0.998424</td>\n",
              "      <td>1.007469</td>\n",
              "      <td>1.011006</td>\n",
              "      <td>0.994142</td>\n",
              "      <td>1.003069</td>\n",
              "      <td>0.995562</td>\n",
              "      <td>0.992089</td>\n",
              "      <td>0.992587</td>\n",
              "      <td>0.994560</td>\n",
              "      <td>0.998080</td>\n",
              "      <td>0.995546</td>\n",
              "      <td>1.001011</td>\n",
              "      <td>1.004278</td>\n",
              "      <td>1.002482</td>\n",
              "      <td>0.981121</td>\n",
              "      <td>1.005715</td>\n",
              "      <td>0.995507</td>\n",
              "      <td>0.991942</td>\n",
              "      <td>0.999439</td>\n",
              "      <td>1.011381</td>\n",
              "      <td>1.012909</td>\n",
              "      <td>1.008380</td>\n",
              "      <td>1.009298</td>\n",
              "      <td>1.001120</td>\n",
              "      <td>0.997671</td>\n",
              "      <td>1.001453</td>\n",
              "      <td>0.993863</td>\n",
              "      <td>1.002485</td>\n",
              "      <td>1.001565</td>\n",
              "      <td>0.999866</td>\n",
              "      <td>1.001145</td>\n",
              "      <td>0.996025</td>\n",
              "      <td>1.006610</td>\n",
              "      <td>0.990046</td>\n",
              "      <td>1.003458</td>\n",
              "      <td>0.995538</td>\n",
              "      <td>1.003161</td>\n",
              "      <td>1.003950</td>\n",
              "      <td>1.001663</td>\n",
              "      <td>1.000705</td>\n",
              "      <td>0.995894</td>\n",
              "      <td>0.998148</td>\n",
              "      <td>0.990183</td>\n",
              "      <td>1.006882</td>\n",
              "      <td>0.993086</td>\n",
              "      <td>0.997615</td>\n",
              "      <td>0.993404</td>\n",
              "      <td>0.997070</td>\n",
              "      <td>1.011450</td>\n",
              "      <td>0.997306</td>\n",
              "      <td>1.005657</td>\n",
              "      <td>0.995253</td>\n",
              "      <td>0.993757</td>\n",
              "      <td>0.996798</td>\n",
              "      <td>0.998548</td>\n",
              "      <td>1.002636</td>\n",
              "      <td>1.002644</td>\n",
              "      <td>1.002458</td>\n",
              "      <td>1.002878</td>\n",
              "      <td>1.009213</td>\n",
              "      <td>1.000429</td>\n",
              "      <td>1.002594</td>\n",
              "      <td>0.999823</td>\n",
              "      <td>1.001426</td>\n",
              "      <td>0.999123</td>\n",
              "      <td>1.002517</td>\n",
              "      <td>0.997926</td>\n",
              "      <td>1.012118</td>\n",
              "      <td>1.000383</td>\n",
              "      <td>1.002407</td>\n",
              "      <td>0.996813</td>\n",
              "      <td>1.011419</td>\n",
              "      <td>0.994841</td>\n",
              "      <td>1.014016</td>\n",
              "      <td>1.001478</td>\n",
              "      <td>1.002022</td>\n",
              "      <td>1.003364</td>\n",
              "      <td>0.999189</td>\n",
              "      <td>1.003253</td>\n",
              "      <td>1.004273</td>\n",
              "      <td>0.997789</td>\n",
              "      <td>1.000953</td>\n",
              "      <td>1.007509</td>\n",
              "      <td>0.994646</td>\n",
              "      <td>0.991927</td>\n",
              "      <td>0.995396</td>\n",
              "      <td>1.000480</td>\n",
              "      <td>1.004560</td>\n",
              "      <td>0.990087</td>\n",
              "      <td>0.996314</td>\n",
              "      <td>0.996511</td>\n",
              "      <td>0.999788</td>\n",
              "      <td>1.014520</td>\n",
              "      <td>0.994000</td>\n",
              "      <td>0.999559</td>\n",
              "      <td>0.996270</td>\n",
              "      <td>1.003705</td>\n",
              "      <td>0.996285</td>\n",
              "      <td>1.000596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.070000</td>\n",
              "      <td>-3.664000</td>\n",
              "      <td>-4.258000</td>\n",
              "      <td>-4.140000</td>\n",
              "      <td>-4.411000</td>\n",
              "      <td>-3.586000</td>\n",
              "      <td>-3.953000</td>\n",
              "      <td>-3.906000</td>\n",
              "      <td>-4.203000</td>\n",
              "      <td>-4.024000</td>\n",
              "      <td>-3.794000</td>\n",
              "      <td>-4.284000</td>\n",
              "      <td>-3.830000</td>\n",
              "      <td>-4.551000</td>\n",
              "      <td>-3.649000</td>\n",
              "      <td>-3.669000</td>\n",
              "      <td>-4.161000</td>\n",
              "      <td>-3.984000</td>\n",
              "      <td>-4.599000</td>\n",
              "      <td>-3.896000</td>\n",
              "      <td>-3.655000</td>\n",
              "      <td>-4.146000</td>\n",
              "      <td>-4.257000</td>\n",
              "      <td>-3.914000</td>\n",
              "      <td>-4.032000</td>\n",
              "      <td>-3.990000</td>\n",
              "      <td>-4.532000</td>\n",
              "      <td>-3.768000</td>\n",
              "      <td>-3.625000</td>\n",
              "      <td>-4.212000</td>\n",
              "      <td>-3.810000</td>\n",
              "      <td>-4.358000</td>\n",
              "      <td>-3.674000</td>\n",
              "      <td>-4.311000</td>\n",
              "      <td>-3.834000</td>\n",
              "      <td>-3.691000</td>\n",
              "      <td>-3.696000</td>\n",
              "      <td>-3.645000</td>\n",
              "      <td>-3.867000</td>\n",
              "      <td>-3.802000</td>\n",
              "      <td>-3.930000</td>\n",
              "      <td>-4.402000</td>\n",
              "      <td>-4.074000</td>\n",
              "      <td>-3.654000</td>\n",
              "      <td>-4.291000</td>\n",
              "      <td>-4.057000</td>\n",
              "      <td>-3.823000</td>\n",
              "      <td>-4.119000</td>\n",
              "      <td>-4.008000</td>\n",
              "      <td>-3.594000</td>\n",
              "      <td>-3.929000</td>\n",
              "      <td>-4.195000</td>\n",
              "      <td>-4.115000</td>\n",
              "      <td>-3.697000</td>\n",
              "      <td>-3.772000</td>\n",
              "      <td>-4.380000</td>\n",
              "      <td>-4.267000</td>\n",
              "      <td>-4.189000</td>\n",
              "      <td>-3.739000</td>\n",
              "      <td>-3.477000</td>\n",
              "      <td>-4.077000</td>\n",
              "      <td>-3.699000</td>\n",
              "      <td>-3.477000</td>\n",
              "      <td>-3.662000</td>\n",
              "      <td>-3.709000</td>\n",
              "      <td>-3.732000</td>\n",
              "      <td>-4.095000</td>\n",
              "      <td>-3.714000</td>\n",
              "      <td>-3.897000</td>\n",
              "      <td>-4.162000</td>\n",
              "      <td>-3.631000</td>\n",
              "      <td>-3.749000</td>\n",
              "      <td>-4.131000</td>\n",
              "      <td>-3.847000</td>\n",
              "      <td>-4.059000</td>\n",
              "      <td>-3.803000</td>\n",
              "      <td>-4.293000</td>\n",
              "      <td>-3.852000</td>\n",
              "      <td>-3.704000</td>\n",
              "      <td>-4.301000</td>\n",
              "      <td>-3.844000</td>\n",
              "      <td>-3.749000</td>\n",
              "      <td>-3.508000</td>\n",
              "      <td>-4.025000</td>\n",
              "      <td>-3.753000</td>\n",
              "      <td>-4.365000</td>\n",
              "      <td>-3.850000</td>\n",
              "      <td>-3.574000</td>\n",
              "      <td>-4.117000</td>\n",
              "      <td>-3.972000</td>\n",
              "      <td>-4.072000</td>\n",
              "      <td>-3.797000</td>\n",
              "      <td>-3.900000</td>\n",
              "      <td>-4.172000</td>\n",
              "      <td>-4.822000</td>\n",
              "      <td>-3.452000</td>\n",
              "      <td>-3.858000</td>\n",
              "      <td>-4.419000</td>\n",
              "      <td>-3.684000</td>\n",
              "      <td>-4.029000</td>\n",
              "      <td>-3.970000</td>\n",
              "      <td>-4.331000</td>\n",
              "      <td>-3.688000</td>\n",
              "      <td>-4.201000</td>\n",
              "      <td>-4.084000</td>\n",
              "      <td>-3.616000</td>\n",
              "      <td>-4.289000</td>\n",
              "      <td>-4.106000</td>\n",
              "      <td>-4.208000</td>\n",
              "      <td>-3.799000</td>\n",
              "      <td>-3.945000</td>\n",
              "      <td>-3.810000</td>\n",
              "      <td>-3.957000</td>\n",
              "      <td>-4.471000</td>\n",
              "      <td>-4.164000</td>\n",
              "      <td>-3.562000</td>\n",
              "      <td>-3.571000</td>\n",
              "      <td>-3.851000</td>\n",
              "      <td>-3.909000</td>\n",
              "      <td>-3.749000</td>\n",
              "      <td>-3.771000</td>\n",
              "      <td>-4.276000</td>\n",
              "      <td>-3.811000</td>\n",
              "      <td>-3.910000</td>\n",
              "      <td>-4.041000</td>\n",
              "      <td>-3.837000</td>\n",
              "      <td>-4.051000</td>\n",
              "      <td>-4.053000</td>\n",
              "      <td>-4.290000</td>\n",
              "      <td>-4.149000</td>\n",
              "      <td>-3.817000</td>\n",
              "      <td>-4.013000</td>\n",
              "      <td>-3.705000</td>\n",
              "      <td>-3.742000</td>\n",
              "      <td>-3.942000</td>\n",
              "      <td>-4.369000</td>\n",
              "      <td>-3.787000</td>\n",
              "      <td>-4.279000</td>\n",
              "      <td>-4.753000</td>\n",
              "      <td>-4.226000</td>\n",
              "      <td>-4.094000</td>\n",
              "      <td>-3.808000</td>\n",
              "      <td>-4.010000</td>\n",
              "      <td>-3.625000</td>\n",
              "      <td>-3.564000</td>\n",
              "      <td>-3.738000</td>\n",
              "      <td>-3.970000</td>\n",
              "      <td>-4.573000</td>\n",
              "      <td>-4.609000</td>\n",
              "      <td>-3.994000</td>\n",
              "      <td>-3.745000</td>\n",
              "      <td>-3.883000</td>\n",
              "      <td>-4.258000</td>\n",
              "      <td>-4.364000</td>\n",
              "      <td>-4.011000</td>\n",
              "      <td>-4.526000</td>\n",
              "      <td>-3.649000</td>\n",
              "      <td>-3.755000</td>\n",
              "      <td>-4.281000</td>\n",
              "      <td>-3.908000</td>\n",
              "      <td>-3.747000</td>\n",
              "      <td>-4.193000</td>\n",
              "      <td>-4.033000</td>\n",
              "      <td>-4.028000</td>\n",
              "      <td>-3.941000</td>\n",
              "      <td>-4.237000</td>\n",
              "      <td>-3.784000</td>\n",
              "      <td>-4.044000</td>\n",
              "      <td>-4.094000</td>\n",
              "      <td>-4.124000</td>\n",
              "      <td>-3.922000</td>\n",
              "      <td>-4.052000</td>\n",
              "      <td>-4.624000</td>\n",
              "      <td>-3.790000</td>\n",
              "      <td>-4.099000</td>\n",
              "      <td>-4.379000</td>\n",
              "      <td>-3.572000</td>\n",
              "      <td>-3.922000</td>\n",
              "      <td>-4.016000</td>\n",
              "      <td>-4.623000</td>\n",
              "      <td>-4.215000</td>\n",
              "      <td>-4.045000</td>\n",
              "      <td>-4.205000</td>\n",
              "      <td>-3.954000</td>\n",
              "      <td>-4.017000</td>\n",
              "      <td>-3.950000</td>\n",
              "      <td>-3.724000</td>\n",
              "      <td>-4.154000</td>\n",
              "      <td>-3.998000</td>\n",
              "      <td>-3.677000</td>\n",
              "      <td>-3.717000</td>\n",
              "      <td>-4.062000</td>\n",
              "      <td>-4.150000</td>\n",
              "      <td>-4.277000</td>\n",
              "      <td>-3.791000</td>\n",
              "      <td>-4.050000</td>\n",
              "      <td>-4.068000</td>\n",
              "      <td>-4.008000</td>\n",
              "      <td>-4.098000</td>\n",
              "      <td>-4.004000</td>\n",
              "      <td>-3.775000</td>\n",
              "      <td>-3.716000</td>\n",
              "      <td>-4.024000</td>\n",
              "      <td>-3.979000</td>\n",
              "      <td>-4.122000</td>\n",
              "      <td>-3.788000</td>\n",
              "      <td>-3.928000</td>\n",
              "      <td>-4.369000</td>\n",
              "      <td>-3.510000</td>\n",
              "      <td>-3.750000</td>\n",
              "      <td>-3.874000</td>\n",
              "      <td>-5.068000</td>\n",
              "      <td>-4.207000</td>\n",
              "      <td>-4.335000</td>\n",
              "      <td>-4.564000</td>\n",
              "      <td>-3.688000</td>\n",
              "      <td>-3.956000</td>\n",
              "      <td>-4.574000</td>\n",
              "      <td>-4.070000</td>\n",
              "      <td>-4.149000</td>\n",
              "      <td>-3.844000</td>\n",
              "      <td>-3.733000</td>\n",
              "      <td>-4.014000</td>\n",
              "      <td>-4.163000</td>\n",
              "      <td>-3.870000</td>\n",
              "      <td>-5.222000</td>\n",
              "      <td>-3.980000</td>\n",
              "      <td>-3.910000</td>\n",
              "      <td>-3.744000</td>\n",
              "      <td>-4.662000</td>\n",
              "      <td>-4.243000</td>\n",
              "      <td>-4.323000</td>\n",
              "      <td>-4.396000</td>\n",
              "      <td>-3.932000</td>\n",
              "      <td>-4.783000</td>\n",
              "      <td>-4.050000</td>\n",
              "      <td>-4.265000</td>\n",
              "      <td>-3.875000</td>\n",
              "      <td>-4.115000</td>\n",
              "      <td>-4.225000</td>\n",
              "      <td>-4.303000</td>\n",
              "      <td>-4.277000</td>\n",
              "      <td>-3.669000</td>\n",
              "      <td>-3.802000</td>\n",
              "      <td>-4.360000</td>\n",
              "      <td>-4.064000</td>\n",
              "      <td>-3.856000</td>\n",
              "      <td>-4.282000</td>\n",
              "      <td>-4.248000</td>\n",
              "      <td>-3.813000</td>\n",
              "      <td>-4.139000</td>\n",
              "      <td>-4.140000</td>\n",
              "      <td>-3.966000</td>\n",
              "      <td>-3.785000</td>\n",
              "      <td>-3.802000</td>\n",
              "      <td>-4.376000</td>\n",
              "      <td>-4.612000</td>\n",
              "      <td>-3.738000</td>\n",
              "      <td>-4.318000</td>\n",
              "      <td>-4.090000</td>\n",
              "      <td>-4.021000</td>\n",
              "      <td>-4.878000</td>\n",
              "      <td>-3.805000</td>\n",
              "      <td>-4.194000</td>\n",
              "      <td>-3.757000</td>\n",
              "      <td>-3.738000</td>\n",
              "      <td>-3.828000</td>\n",
              "      <td>-3.747000</td>\n",
              "      <td>-4.460000</td>\n",
              "      <td>-4.303000</td>\n",
              "      <td>-3.846000</td>\n",
              "      <td>-3.857000</td>\n",
              "      <td>-3.968000</td>\n",
              "      <td>-3.592000</td>\n",
              "      <td>-4.128000</td>\n",
              "      <td>-3.562000</td>\n",
              "      <td>-3.952000</td>\n",
              "      <td>-4.585000</td>\n",
              "      <td>-3.849000</td>\n",
              "      <td>-4.036000</td>\n",
              "      <td>-3.785000</td>\n",
              "      <td>-4.031000</td>\n",
              "      <td>-4.016000</td>\n",
              "      <td>-4.062000</td>\n",
              "      <td>-4.320000</td>\n",
              "      <td>-3.946000</td>\n",
              "      <td>-3.931000</td>\n",
              "      <td>-3.846000</td>\n",
              "      <td>-4.608000</td>\n",
              "      <td>-3.832000</td>\n",
              "      <td>-3.688000</td>\n",
              "      <td>-3.877000</td>\n",
              "      <td>-3.599000</td>\n",
              "      <td>-3.650000</td>\n",
              "      <td>-3.865000</td>\n",
              "      <td>-3.814000</td>\n",
              "      <td>-3.835000</td>\n",
              "      <td>-3.908000</td>\n",
              "      <td>-3.581000</td>\n",
              "      <td>-4.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.688750</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.668000</td>\n",
              "      <td>-0.686000</td>\n",
              "      <td>-0.671000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.690750</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.681750</td>\n",
              "      <td>-0.662000</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.663000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.670750</td>\n",
              "      <td>-0.693000</td>\n",
              "      <td>-0.665000</td>\n",
              "      <td>-0.690000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.687000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.671000</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.692000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.662000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.663000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.691000</td>\n",
              "      <td>-0.678750</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.690000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.687000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.686750</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.674750</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.687000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.665000</td>\n",
              "      <td>-0.675750</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.682750</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.669750</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.675750</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.668000</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.688000</td>\n",
              "      <td>-0.667750</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.684000</td>\n",
              "      <td>-0.678750</td>\n",
              "      <td>-0.682750</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.690000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.689000</td>\n",
              "      <td>-0.665000</td>\n",
              "      <td>-0.677750</td>\n",
              "      <td>-0.688000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.665750</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.686000</td>\n",
              "      <td>-0.673750</td>\n",
              "      <td>-0.684750</td>\n",
              "      <td>-0.691750</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.671750</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.668000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.671750</td>\n",
              "      <td>-0.680750</td>\n",
              "      <td>-0.695750</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.697000</td>\n",
              "      <td>-0.689000</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.681750</td>\n",
              "      <td>-0.679750</td>\n",
              "      <td>-0.690000</td>\n",
              "      <td>-0.669750</td>\n",
              "      <td>-0.692000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.663000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.681750</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.668750</td>\n",
              "      <td>-0.661000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.665750</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.696000</td>\n",
              "      <td>-0.655750</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.663000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.661000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.684000</td>\n",
              "      <td>-0.668000</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.651750</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.671000</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.662000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.668000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.689000</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.656000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.684000</td>\n",
              "      <td>-0.686750</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.687000</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.653750</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.686000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.665750</td>\n",
              "      <td>-0.664750</td>\n",
              "      <td>-0.671750</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.688000</td>\n",
              "      <td>-0.668000</td>\n",
              "      <td>-0.678750</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.689000</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.657750</td>\n",
              "      <td>-0.697750</td>\n",
              "      <td>-0.663750</td>\n",
              "      <td>-0.671000</td>\n",
              "      <td>-0.675750</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.665000</td>\n",
              "      <td>-0.682000</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.669000</td>\n",
              "      <td>-0.660000</td>\n",
              "      <td>-0.686000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.663000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.685750</td>\n",
              "      <td>-0.688750</td>\n",
              "      <td>-0.679750</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.676000</td>\n",
              "      <td>-0.666000</td>\n",
              "      <td>-0.673000</td>\n",
              "      <td>-0.686000</td>\n",
              "      <td>-0.686000</td>\n",
              "      <td>-0.659000</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.677750</td>\n",
              "      <td>-0.695000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.681000</td>\n",
              "      <td>-0.679000</td>\n",
              "      <td>-0.665750</td>\n",
              "      <td>-0.661000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.684000</td>\n",
              "      <td>-0.665000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.674000</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.678000</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.677000</td>\n",
              "      <td>-0.683000</td>\n",
              "      <td>-0.651750</td>\n",
              "      <td>-0.660000</td>\n",
              "      <td>-0.682750</td>\n",
              "      <td>-0.670000</td>\n",
              "      <td>-0.674750</td>\n",
              "      <td>-0.674750</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.660000</td>\n",
              "      <td>-0.667000</td>\n",
              "      <td>-0.660000</td>\n",
              "      <td>-0.675000</td>\n",
              "      <td>-0.684750</td>\n",
              "      <td>-0.672000</td>\n",
              "      <td>-0.656750</td>\n",
              "      <td>-0.664000</td>\n",
              "      <td>-0.665000</td>\n",
              "      <td>-0.680000</td>\n",
              "      <td>-0.663000</td>\n",
              "      <td>-0.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>-0.014000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.007000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>-0.007000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.019000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>-0.016500</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>-0.009500</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-0.022000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>-0.007000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>-0.024000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.001500</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.011000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.000500</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>-0.014000</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>-0.018500</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>-0.015500</td>\n",
              "      <td>-0.013000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>-0.022000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>-0.007000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>-0.018500</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>-0.008500</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.003500</td>\n",
              "      <td>-0.003500</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>-0.014000</td>\n",
              "      <td>-0.009000</td>\n",
              "      <td>-0.015000</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>-0.014000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>-0.014500</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>-0.010000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.004500</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.015000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>-0.002500</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.001500</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>-0.002500</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>-0.003000</td>\n",
              "      <td>-0.004500</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>-0.018000</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.011000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.007000</td>\n",
              "      <td>-0.007000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>0.004000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>-0.023000</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>0.009000</td>\n",
              "      <td>-0.000500</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.008000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.002000</td>\n",
              "      <td>-0.017000</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.003500</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>-0.012000</td>\n",
              "      <td>-0.006000</td>\n",
              "      <td>-0.015000</td>\n",
              "      <td>-0.004000</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.010000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.684750</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.660750</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.643000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.676750</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.695000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.692000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.671750</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.655750</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.693000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.692000</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.655750</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.683000</td>\n",
              "      <td>0.665750</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.666750</td>\n",
              "      <td>0.663750</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.683000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.647000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.661000</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.681750</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.670750</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.670750</td>\n",
              "      <td>0.678750</td>\n",
              "      <td>0.663000</td>\n",
              "      <td>0.654000</td>\n",
              "      <td>0.674750</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.683000</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.683750</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.696000</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.670750</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.666750</td>\n",
              "      <td>0.699000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.663000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.682750</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.691000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.691000</td>\n",
              "      <td>0.666750</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.691000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.678750</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.699000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.673750</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.673750</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.674750</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.680750</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.681750</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.668750</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.664750</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.684750</td>\n",
              "      <td>0.658000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.664000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.668750</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.685750</td>\n",
              "      <td>0.659000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.661000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.680750</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>0.657000</td>\n",
              "      <td>0.687000</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.656000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.651000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.655000</td>\n",
              "      <td>0.677750</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.652000</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.679750</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.671000</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.656000</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.670750</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>0.693000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.681750</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.689000</td>\n",
              "      <td>0.670000</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.684750</td>\n",
              "      <td>0.662000</td>\n",
              "      <td>0.676750</td>\n",
              "      <td>0.679000</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.671750</td>\n",
              "      <td>0.684000</td>\n",
              "      <td>0.653000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.654000</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.673000</td>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.767000</td>\n",
              "      <td>3.864000</td>\n",
              "      <td>3.866000</td>\n",
              "      <td>3.871000</td>\n",
              "      <td>3.955000</td>\n",
              "      <td>3.819000</td>\n",
              "      <td>3.954000</td>\n",
              "      <td>3.669000</td>\n",
              "      <td>3.948000</td>\n",
              "      <td>3.812000</td>\n",
              "      <td>4.469000</td>\n",
              "      <td>3.692000</td>\n",
              "      <td>3.840000</td>\n",
              "      <td>3.971000</td>\n",
              "      <td>3.968000</td>\n",
              "      <td>4.035000</td>\n",
              "      <td>3.748000</td>\n",
              "      <td>4.501000</td>\n",
              "      <td>3.886000</td>\n",
              "      <td>4.064000</td>\n",
              "      <td>4.343000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>4.140000</td>\n",
              "      <td>4.032000</td>\n",
              "      <td>4.094000</td>\n",
              "      <td>3.532000</td>\n",
              "      <td>3.740000</td>\n",
              "      <td>4.254000</td>\n",
              "      <td>3.996000</td>\n",
              "      <td>4.734000</td>\n",
              "      <td>3.972000</td>\n",
              "      <td>3.652000</td>\n",
              "      <td>4.766000</td>\n",
              "      <td>3.966000</td>\n",
              "      <td>3.830000</td>\n",
              "      <td>3.785000</td>\n",
              "      <td>3.995000</td>\n",
              "      <td>3.969000</td>\n",
              "      <td>4.241000</td>\n",
              "      <td>3.864000</td>\n",
              "      <td>3.663000</td>\n",
              "      <td>4.065000</td>\n",
              "      <td>4.209000</td>\n",
              "      <td>4.029000</td>\n",
              "      <td>4.524000</td>\n",
              "      <td>4.091000</td>\n",
              "      <td>4.234000</td>\n",
              "      <td>4.297000</td>\n",
              "      <td>4.166000</td>\n",
              "      <td>3.777000</td>\n",
              "      <td>4.082000</td>\n",
              "      <td>3.978000</td>\n",
              "      <td>3.465000</td>\n",
              "      <td>4.087000</td>\n",
              "      <td>3.693000</td>\n",
              "      <td>3.845000</td>\n",
              "      <td>4.229000</td>\n",
              "      <td>3.621000</td>\n",
              "      <td>3.915000</td>\n",
              "      <td>4.059000</td>\n",
              "      <td>3.726000</td>\n",
              "      <td>3.966000</td>\n",
              "      <td>3.678000</td>\n",
              "      <td>3.960000</td>\n",
              "      <td>4.632000</td>\n",
              "      <td>4.012000</td>\n",
              "      <td>4.430000</td>\n",
              "      <td>3.986000</td>\n",
              "      <td>4.261000</td>\n",
              "      <td>3.830000</td>\n",
              "      <td>3.566000</td>\n",
              "      <td>3.821000</td>\n",
              "      <td>4.448000</td>\n",
              "      <td>3.927000</td>\n",
              "      <td>4.772000</td>\n",
              "      <td>3.964000</td>\n",
              "      <td>3.898000</td>\n",
              "      <td>3.819000</td>\n",
              "      <td>4.880000</td>\n",
              "      <td>3.888000</td>\n",
              "      <td>4.467000</td>\n",
              "      <td>3.767000</td>\n",
              "      <td>3.651000</td>\n",
              "      <td>4.162000</td>\n",
              "      <td>3.716000</td>\n",
              "      <td>3.979000</td>\n",
              "      <td>3.901000</td>\n",
              "      <td>4.092000</td>\n",
              "      <td>3.772000</td>\n",
              "      <td>3.824000</td>\n",
              "      <td>3.676000</td>\n",
              "      <td>3.881000</td>\n",
              "      <td>4.901000</td>\n",
              "      <td>3.883000</td>\n",
              "      <td>3.861000</td>\n",
              "      <td>4.080000</td>\n",
              "      <td>4.076000</td>\n",
              "      <td>4.066000</td>\n",
              "      <td>4.084000</td>\n",
              "      <td>3.953000</td>\n",
              "      <td>4.338000</td>\n",
              "      <td>3.617000</td>\n",
              "      <td>3.961000</td>\n",
              "      <td>3.654000</td>\n",
              "      <td>4.218000</td>\n",
              "      <td>3.986000</td>\n",
              "      <td>4.257000</td>\n",
              "      <td>3.784000</td>\n",
              "      <td>4.048000</td>\n",
              "      <td>3.915000</td>\n",
              "      <td>4.084000</td>\n",
              "      <td>3.657000</td>\n",
              "      <td>4.002000</td>\n",
              "      <td>3.768000</td>\n",
              "      <td>4.204000</td>\n",
              "      <td>4.037000</td>\n",
              "      <td>4.205000</td>\n",
              "      <td>3.761000</td>\n",
              "      <td>3.876000</td>\n",
              "      <td>3.950000</td>\n",
              "      <td>3.674000</td>\n",
              "      <td>4.078000</td>\n",
              "      <td>4.183000</td>\n",
              "      <td>3.955000</td>\n",
              "      <td>3.676000</td>\n",
              "      <td>4.003000</td>\n",
              "      <td>3.748000</td>\n",
              "      <td>3.807000</td>\n",
              "      <td>3.958000</td>\n",
              "      <td>4.466000</td>\n",
              "      <td>4.090000</td>\n",
              "      <td>4.077000</td>\n",
              "      <td>3.757000</td>\n",
              "      <td>4.536000</td>\n",
              "      <td>4.443000</td>\n",
              "      <td>4.646000</td>\n",
              "      <td>4.171000</td>\n",
              "      <td>3.954000</td>\n",
              "      <td>4.109000</td>\n",
              "      <td>4.068000</td>\n",
              "      <td>3.713000</td>\n",
              "      <td>4.465000</td>\n",
              "      <td>3.963000</td>\n",
              "      <td>3.972000</td>\n",
              "      <td>3.802000</td>\n",
              "      <td>4.110000</td>\n",
              "      <td>4.135000</td>\n",
              "      <td>4.197000</td>\n",
              "      <td>4.316000</td>\n",
              "      <td>4.246000</td>\n",
              "      <td>3.757000</td>\n",
              "      <td>3.530000</td>\n",
              "      <td>4.407000</td>\n",
              "      <td>3.829000</td>\n",
              "      <td>3.837000</td>\n",
              "      <td>3.630000</td>\n",
              "      <td>4.413000</td>\n",
              "      <td>4.467000</td>\n",
              "      <td>4.154000</td>\n",
              "      <td>4.025000</td>\n",
              "      <td>4.073000</td>\n",
              "      <td>3.697000</td>\n",
              "      <td>3.652000</td>\n",
              "      <td>3.988000</td>\n",
              "      <td>3.921000</td>\n",
              "      <td>4.067000</td>\n",
              "      <td>4.243000</td>\n",
              "      <td>3.976000</td>\n",
              "      <td>4.136000</td>\n",
              "      <td>3.617000</td>\n",
              "      <td>4.119000</td>\n",
              "      <td>4.174000</td>\n",
              "      <td>4.012000</td>\n",
              "      <td>4.675000</td>\n",
              "      <td>4.262000</td>\n",
              "      <td>4.654000</td>\n",
              "      <td>4.367000</td>\n",
              "      <td>4.528000</td>\n",
              "      <td>4.091000</td>\n",
              "      <td>4.147000</td>\n",
              "      <td>4.331000</td>\n",
              "      <td>3.862000</td>\n",
              "      <td>4.419000</td>\n",
              "      <td>3.913000</td>\n",
              "      <td>4.798000</td>\n",
              "      <td>3.648000</td>\n",
              "      <td>4.164000</td>\n",
              "      <td>4.277000</td>\n",
              "      <td>3.989000</td>\n",
              "      <td>4.362000</td>\n",
              "      <td>4.257000</td>\n",
              "      <td>4.071000</td>\n",
              "      <td>3.679000</td>\n",
              "      <td>4.025000</td>\n",
              "      <td>3.826000</td>\n",
              "      <td>4.141000</td>\n",
              "      <td>4.226000</td>\n",
              "      <td>4.244000</td>\n",
              "      <td>3.809000</td>\n",
              "      <td>4.242000</td>\n",
              "      <td>3.626000</td>\n",
              "      <td>4.052000</td>\n",
              "      <td>4.193000</td>\n",
              "      <td>4.031000</td>\n",
              "      <td>4.084000</td>\n",
              "      <td>3.670000</td>\n",
              "      <td>3.785000</td>\n",
              "      <td>4.286000</td>\n",
              "      <td>4.039000</td>\n",
              "      <td>4.364000</td>\n",
              "      <td>3.706000</td>\n",
              "      <td>3.806000</td>\n",
              "      <td>3.891000</td>\n",
              "      <td>3.790000</td>\n",
              "      <td>3.910000</td>\n",
              "      <td>4.373000</td>\n",
              "      <td>3.919000</td>\n",
              "      <td>3.974000</td>\n",
              "      <td>3.852000</td>\n",
              "      <td>4.621000</td>\n",
              "      <td>3.683000</td>\n",
              "      <td>4.226000</td>\n",
              "      <td>3.735000</td>\n",
              "      <td>3.838000</td>\n",
              "      <td>4.731000</td>\n",
              "      <td>4.084000</td>\n",
              "      <td>3.870000</td>\n",
              "      <td>3.720000</td>\n",
              "      <td>4.107000</td>\n",
              "      <td>3.714000</td>\n",
              "      <td>3.884000</td>\n",
              "      <td>3.654000</td>\n",
              "      <td>3.788000</td>\n",
              "      <td>4.364000</td>\n",
              "      <td>4.154000</td>\n",
              "      <td>3.971000</td>\n",
              "      <td>3.962000</td>\n",
              "      <td>4.030000</td>\n",
              "      <td>3.675000</td>\n",
              "      <td>4.063000</td>\n",
              "      <td>4.118000</td>\n",
              "      <td>3.929000</td>\n",
              "      <td>3.540000</td>\n",
              "      <td>3.639000</td>\n",
              "      <td>3.651000</td>\n",
              "      <td>3.731000</td>\n",
              "      <td>3.730000</td>\n",
              "      <td>3.873000</td>\n",
              "      <td>4.628000</td>\n",
              "      <td>3.888000</td>\n",
              "      <td>3.843000</td>\n",
              "      <td>3.801000</td>\n",
              "      <td>3.943000</td>\n",
              "      <td>4.348000</td>\n",
              "      <td>4.286000</td>\n",
              "      <td>3.556000</td>\n",
              "      <td>4.101000</td>\n",
              "      <td>4.149000</td>\n",
              "      <td>4.446000</td>\n",
              "      <td>3.770000</td>\n",
              "      <td>3.754000</td>\n",
              "      <td>3.420000</td>\n",
              "      <td>3.871000</td>\n",
              "      <td>4.061000</td>\n",
              "      <td>4.086000</td>\n",
              "      <td>4.109000</td>\n",
              "      <td>3.597000</td>\n",
              "      <td>4.589000</td>\n",
              "      <td>3.928000</td>\n",
              "      <td>3.678000</td>\n",
              "      <td>3.839000</td>\n",
              "      <td>3.798000</td>\n",
              "      <td>3.647000</td>\n",
              "      <td>4.215000</td>\n",
              "      <td>4.156000</td>\n",
              "      <td>3.899000</td>\n",
              "      <td>3.707000</td>\n",
              "      <td>3.802000</td>\n",
              "      <td>4.087000</td>\n",
              "      <td>4.103000</td>\n",
              "      <td>4.275000</td>\n",
              "      <td>3.681000</td>\n",
              "      <td>3.648000</td>\n",
              "      <td>3.880000</td>\n",
              "      <td>3.902000</td>\n",
              "      <td>3.963000</td>\n",
              "      <td>4.099000</td>\n",
              "      <td>4.691000</td>\n",
              "      <td>4.142000</td>\n",
              "      <td>3.777000</td>\n",
              "      <td>3.619000</td>\n",
              "      <td>3.829000</td>\n",
              "      <td>3.717000</td>\n",
              "      <td>5.092000</td>\n",
              "      <td>5.125000</td>\n",
              "      <td>3.681000</td>\n",
              "      <td>3.716000</td>\n",
              "      <td>3.932000</td>\n",
              "      <td>3.764000</td>\n",
              "      <td>4.070000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1             2             3             4  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.014043      0.000972      0.005145     -0.003525      0.003394   \n",
              "std        1.003779      0.993955      1.000809      1.008545      1.002826   \n",
              "min       -4.070000     -3.664000     -4.258000     -4.140000     -4.411000   \n",
              "25%       -0.688750     -0.667000     -0.668000     -0.686000     -0.671000   \n",
              "50%       -0.006000      0.001000      0.017000     -0.006000      0.007000   \n",
              "75%        0.664000      0.676000      0.681000      0.682000      0.676000   \n",
              "max        3.767000      3.864000      3.866000      3.871000      3.955000   \n",
              "\n",
              "                  5             6             7             8             9  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.002738      0.004213     -0.010618     -0.003211     -0.002738   \n",
              "std        1.002917      0.994315      0.997972      0.996938      1.000688   \n",
              "min       -3.586000     -3.953000     -3.906000     -4.203000     -4.024000   \n",
              "25%       -0.679000     -0.673000     -0.680000     -0.667000     -0.677000   \n",
              "50%        0.005000      0.014000     -0.014000     -0.003000     -0.007000   \n",
              "75%        0.684750      0.670000      0.660750      0.671000      0.673000   \n",
              "max        3.819000      3.954000      3.669000      3.948000      3.812000   \n",
              "\n",
              "                 10            11            12            13            14  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.003261      0.007411      0.000794     -0.000246      0.000441   \n",
              "std        1.006049      0.998012      1.002858      0.998698      1.004494   \n",
              "min       -3.794000     -4.284000     -3.830000     -4.551000     -3.649000   \n",
              "25%       -0.690750     -0.669000     -0.681000     -0.667000     -0.681750   \n",
              "50%       -0.004000      0.006000      0.003000      0.009000     -0.001000   \n",
              "75%        0.679000      0.686000      0.677000      0.678000      0.680000   \n",
              "max        4.469000      3.692000      3.840000      3.971000      3.968000   \n",
              "\n",
              "                 15            16            17            18            19  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.011621     -0.003081      0.006932     -0.000170     -0.000793   \n",
              "std        0.999509      1.005204      1.000772      1.003637      1.001289   \n",
              "min       -3.669000     -4.161000     -3.984000     -4.599000     -3.896000   \n",
              "25%       -0.662000     -0.682000     -0.663000     -0.679000     -0.672000   \n",
              "50%        0.009000     -0.003000      0.016000     -0.006000     -0.007000   \n",
              "75%        0.682000      0.674000      0.690000      0.679000      0.675000   \n",
              "max        4.035000      3.748000      4.501000      3.886000      4.064000   \n",
              "\n",
              "                 20            21            22            23            24  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.004675     -0.019747      0.007058     -0.006713      0.003181   \n",
              "std        0.996804      0.992845      0.997785      1.003841      1.005888   \n",
              "min       -3.655000     -4.146000     -4.257000     -3.914000     -4.032000   \n",
              "25%       -0.670750     -0.693000     -0.665000     -0.690000     -0.680000   \n",
              "50%        0.000000     -0.019000     -0.004000      0.001000      0.003000   \n",
              "75%        0.678000      0.643000      0.672000      0.667000      0.676750   \n",
              "max        4.343000      3.800000      4.140000      4.032000      4.094000   \n",
              "\n",
              "                 25            26            27            28            29  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.007348      0.004626     -0.002919      0.014807      0.002969   \n",
              "std        0.996975      0.999497      0.997001      1.008973      1.004791   \n",
              "min       -3.990000     -4.532000     -3.768000     -3.625000     -4.212000   \n",
              "25%       -0.687000     -0.664000     -0.671000     -0.669000     -0.683000   \n",
              "50%       -0.006000      0.004000      0.003000      0.006000     -0.001000   \n",
              "75%        0.672000      0.674000      0.671000      0.695000      0.682000   \n",
              "max        3.532000      3.740000      4.254000      3.996000      4.734000   \n",
              "\n",
              "                 30            31            32            33            34  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.005930      0.008526     -0.003012     -0.002511      0.006480   \n",
              "std        0.995823      0.998580      1.007158      0.995098      0.999363   \n",
              "min       -3.810000     -4.358000     -3.674000     -4.311000     -3.834000   \n",
              "25%       -0.676000     -0.664000     -0.692000     -0.675000     -0.672000   \n",
              "50%       -0.005000      0.011500      0.000000     -0.005000      0.011000   \n",
              "75%        0.682000      0.692000      0.690000      0.671750      0.684000   \n",
              "max        3.972000      3.652000      4.766000      3.966000      3.830000   \n",
              "\n",
              "                 35            36            37            38            39  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.012845      0.009695      0.010524     -0.000009      0.010531   \n",
              "std        0.996513      0.996013      1.004946      1.002930      1.004507   \n",
              "min       -3.691000     -3.696000     -3.645000     -3.867000     -3.802000   \n",
              "25%       -0.677000     -0.666000     -0.673000     -0.673000     -0.674000   \n",
              "50%       -0.016500      0.004000      0.008500     -0.006000      0.014000   \n",
              "75%        0.655750      0.675000      0.693000      0.672000      0.692000   \n",
              "max        3.785000      3.995000      3.969000      4.241000      3.864000   \n",
              "\n",
              "                 40            41            42            43            44  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.009442      0.012217     -0.007291     -0.001280      0.007256   \n",
              "std        0.993414      1.004133      0.995491      0.997396      1.001088   \n",
              "min       -3.930000     -4.402000     -4.074000     -3.654000     -4.291000   \n",
              "25%       -0.662000     -0.672000     -0.677000     -0.659750     -0.663000   \n",
              "50%        0.003000      0.019000     -0.004000     -0.009000      0.004000   \n",
              "75%        0.687000      0.690000      0.655750      0.668000      0.684000   \n",
              "max        3.663000      4.065000      4.209000      4.029000      4.524000   \n",
              "\n",
              "                 45            46            47            48            49  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.005917      0.004484     -0.003157      0.004098     -0.006166   \n",
              "std        0.999344      1.007084      1.000939      0.996475      0.991990   \n",
              "min       -4.057000     -3.823000     -4.119000     -4.008000     -3.594000   \n",
              "25%       -0.666000     -0.675000     -0.670000     -0.666000     -0.680000   \n",
              "50%        0.013500      0.001000      0.003000      0.010000     -0.009500   \n",
              "75%        0.686000      0.683000      0.665750      0.680000      0.664000   \n",
              "max        4.091000      4.234000      4.297000      4.166000      3.777000   \n",
              "\n",
              "                 50            51            52            53            54  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.003410      0.005602     -0.016488     -0.005769     -0.007329   \n",
              "std        0.997686      1.001566      1.001679      1.001468      0.996476   \n",
              "min       -3.929000     -4.195000     -4.115000     -3.697000     -3.772000   \n",
              "25%       -0.674000     -0.669000     -0.691000     -0.678750     -0.682000   \n",
              "50%        0.002000     -0.000000     -0.022000     -0.008000     -0.002000   \n",
              "75%        0.672000      0.682000      0.665000      0.666750      0.663750   \n",
              "max        4.082000      3.978000      3.465000      4.087000      3.693000   \n",
              "\n",
              "                 55            56            57            58            59  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.002661      0.010716     -0.003806     -0.005069      0.002721   \n",
              "std        0.995495      1.000273      0.999553      1.001529      0.987338   \n",
              "min       -4.380000     -4.267000     -4.189000     -3.739000     -3.477000   \n",
              "25%       -0.676000     -0.659000     -0.672000     -0.675000     -0.666000   \n",
              "50%        0.013000      0.004000     -0.008000     -0.007000      0.005000   \n",
              "75%        0.675000      0.683000      0.675000      0.668000      0.665000   \n",
              "max        3.845000      4.229000      3.621000      3.915000      4.059000   \n",
              "\n",
              "                 60            61            62            63            64  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.007120     -0.013874     -0.002446      0.002950      0.006920   \n",
              "std        1.002628      0.995847      1.004985      1.002373      0.992078   \n",
              "min       -4.077000     -3.699000     -3.477000     -3.662000     -3.709000   \n",
              "25%       -0.690000     -0.681000     -0.687000     -0.679000     -0.675000   \n",
              "50%       -0.002000     -0.009000     -0.002000      0.002000      0.005000   \n",
              "75%        0.676000      0.647000      0.679000      0.688000      0.678000   \n",
              "max        3.726000      3.966000      3.678000      3.960000      4.632000   \n",
              "\n",
              "                 65            66            67            68            69  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.007923      0.000606     -0.011179      0.009296     -0.000537   \n",
              "std        1.006548      1.002983      1.000911      1.004782      1.002772   \n",
              "min       -3.732000     -4.095000     -3.714000     -3.897000     -4.162000   \n",
              "25%       -0.686750     -0.682000     -0.678000     -0.674750     -0.683000   \n",
              "50%       -0.002000      0.004000     -0.024000      0.000000     -0.001500   \n",
              "75%        0.671000      0.664000      0.661000      0.694000      0.676000   \n",
              "max        4.012000      4.430000      3.986000      4.261000      3.830000   \n",
              "\n",
              "                 70            71            72            73            74  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.005079      0.000357      0.002333     -0.000171     -0.004877   \n",
              "std        0.994713      0.996518      1.003167      1.002726      1.001047   \n",
              "min       -3.631000     -3.749000     -4.131000     -3.847000     -4.059000   \n",
              "25%       -0.687000     -0.677000     -0.678000     -0.674000     -0.676000   \n",
              "50%        0.001000     -0.001000      0.008000      0.001000     -0.011000   \n",
              "75%        0.681750      0.671000      0.687000      0.679000      0.673000   \n",
              "max        3.566000      3.821000      4.448000      3.927000      4.772000   \n",
              "\n",
              "                 75            76            77            78            79  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.001313      0.003192     -0.001605      0.010409     -0.001248   \n",
              "std        1.003915      0.988774      1.001685      0.988354      0.997489   \n",
              "min       -3.803000     -4.293000     -3.852000     -3.704000     -4.301000   \n",
              "25%       -0.682000     -0.665000     -0.675750     -0.666000     -0.678000   \n",
              "50%       -0.006000      0.001000      0.003000      0.009000     -0.004000   \n",
              "75%        0.686000      0.670750      0.669000      0.675000      0.680000   \n",
              "max        3.964000      3.898000      3.819000      4.880000      3.888000   \n",
              "\n",
              "                 80            81            82            83            84  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.007097     -0.002587     -0.005448     -0.002641      0.004750   \n",
              "std        1.002499      0.998147      0.992640      0.994977      1.006194   \n",
              "min       -3.844000     -3.749000     -3.508000     -4.025000     -3.753000   \n",
              "25%       -0.672000     -0.673000     -0.683000     -0.677000     -0.682750   \n",
              "50%        0.014000     -0.004000     -0.004000     -0.002000      0.007000   \n",
              "75%        0.682000      0.678000      0.673000      0.678000      0.690000   \n",
              "max        4.467000      3.767000      3.651000      4.162000      3.716000   \n",
              "\n",
              "                 85            86            87            88            89  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.002633      0.001124     -0.001590     -0.005564      0.001942   \n",
              "std        0.997751      1.000488      0.995039      1.008350      0.988202   \n",
              "min       -4.365000     -3.850000     -3.574000     -4.117000     -3.972000   \n",
              "25%       -0.675000     -0.677000     -0.669750     -0.685000     -0.664000   \n",
              "50%        0.003000      0.003000     -0.012000     -0.000500     -0.004000   \n",
              "75%        0.680000      0.673000      0.670750      0.678750      0.663000   \n",
              "max        3.979000      3.901000      4.092000      3.772000      3.824000   \n",
              "\n",
              "                 90            91            92            93            94  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.011500      0.001583     -0.011064     -0.007548     -0.010606   \n",
              "std        0.991236      0.993122      1.003816      1.002676      1.001523   \n",
              "min       -4.072000     -3.797000     -3.900000     -4.172000     -4.822000   \n",
              "25%       -0.675750     -0.670000     -0.685000     -0.679000     -0.685000   \n",
              "50%       -0.009000      0.006000     -0.014000     -0.005500     -0.018500   \n",
              "75%        0.654000      0.674750      0.666000      0.667000      0.670000   \n",
              "max        3.676000      3.881000      4.901000      3.883000      3.861000   \n",
              "\n",
              "                 95            96            97            98            99  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.008324     -0.006834     -0.005020      0.006657      0.002721   \n",
              "std        0.993994      0.987653      1.002141      0.995461      0.995942   \n",
              "min       -3.452000     -3.858000     -4.419000     -3.684000     -4.029000   \n",
              "25%       -0.659000     -0.680000     -0.685000     -0.666000     -0.668000   \n",
              "50%        0.013000     -0.015500     -0.013000      0.004000      0.006000   \n",
              "75%        0.677000      0.670000      0.666000      0.678000      0.681000   \n",
              "max        4.080000      4.076000      4.066000      4.084000      3.953000   \n",
              "\n",
              "                100           101           102           103           104  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.005931     -0.003778      0.000630      0.000334     -0.000845   \n",
              "std        1.006144      1.010039      1.000669      1.003929      1.004093   \n",
              "min       -3.970000     -4.331000     -3.688000     -4.201000     -4.084000   \n",
              "25%       -0.669000     -0.688000     -0.667750     -0.680000     -0.684000   \n",
              "50%        0.007000      0.000000      0.001000     -0.005000     -0.005000   \n",
              "75%        0.683000      0.685000      0.670000      0.679000      0.683750   \n",
              "max        4.338000      3.617000      3.961000      3.654000      4.218000   \n",
              "\n",
              "                105           106           107           108           109  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.011630     -0.001109      0.007661     -0.015267      0.006328   \n",
              "std        1.000796      1.009028      1.005079      1.001791      1.003586   \n",
              "min       -3.616000     -4.289000     -4.106000     -4.208000     -3.799000   \n",
              "25%       -0.678750     -0.682750     -0.667000     -0.700000     -0.675000   \n",
              "50%        0.000500      0.005000      0.005000     -0.022000      0.000000   \n",
              "75%        0.669000      0.682000      0.686000      0.660000      0.696000   \n",
              "max        3.986000      4.257000      3.784000      4.048000      3.915000   \n",
              "\n",
              "                110           111           112           113           114  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.004839     -0.005247     -0.000605     -0.004141      0.008242   \n",
              "std        0.999044      1.007043      0.999238      1.011722      1.001596   \n",
              "min       -3.945000     -3.810000     -3.957000     -4.471000     -4.164000   \n",
              "25%       -0.673000     -0.690000     -0.675000     -0.689000     -0.665000   \n",
              "50%        0.011000     -0.007000     -0.004000     -0.006000      0.014500   \n",
              "75%        0.681000      0.668000      0.667000      0.682000      0.674000   \n",
              "max        4.084000      3.657000      4.002000      3.768000      4.204000   \n",
              "\n",
              "                115           116           117           118           119  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.010468     -0.007527      0.000731     -0.003000      0.005917   \n",
              "std        1.010089      0.998132      0.992286      0.989048      1.002759   \n",
              "min       -3.562000     -3.571000     -3.851000     -3.909000     -3.749000   \n",
              "25%       -0.677750     -0.688000     -0.676000     -0.665750     -0.680000   \n",
              "50%        0.012500     -0.005000      0.000000      0.008000      0.017000   \n",
              "75%        0.690000      0.660000      0.670750      0.671000      0.675750   \n",
              "max        4.037000      4.205000      3.761000      3.876000      3.950000   \n",
              "\n",
              "                120           121           122           123           124  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.005567      0.009830     -0.009007      0.005505     -0.006416   \n",
              "std        1.002140      1.010807      1.006769      1.003961      0.999974   \n",
              "min       -3.771000     -4.276000     -3.811000     -3.910000     -4.041000   \n",
              "25%       -0.682000     -0.679000     -0.686000     -0.673750     -0.684750   \n",
              "50%       -0.004000      0.004000     -0.018500      0.006000     -0.005000   \n",
              "75%        0.666750      0.699000      0.662000      0.677000      0.670000   \n",
              "max        3.674000      4.078000      4.183000      3.955000      3.676000   \n",
              "\n",
              "                125           126           127           128           129  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.001148     -0.007685     -0.001432     -0.001247     -0.000216   \n",
              "std        1.010938      0.998090      1.001861      0.996236      1.002005   \n",
              "min       -3.837000     -4.051000     -4.053000     -4.290000     -4.149000   \n",
              "25%       -0.691750     -0.678000     -0.679000     -0.671750     -0.681000   \n",
              "50%        0.004000      0.003000     -0.005000     -0.006000      0.002000   \n",
              "75%        0.687000      0.662000      0.670000      0.671000      0.681000   \n",
              "max        4.003000      3.748000      3.807000      3.958000      4.466000   \n",
              "\n",
              "                130           131           132           133           134  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.002911     -0.005041     -0.000581     -0.006911     -0.008748   \n",
              "std        1.002279      0.998812      1.001232      0.996635      1.003509   \n",
              "min       -3.817000     -4.013000     -3.705000     -3.742000     -3.942000   \n",
              "25%       -0.668000     -0.673000     -0.673000     -0.671750     -0.680750   \n",
              "50%        0.009000     -0.008500     -0.004000     -0.003000     -0.003500   \n",
              "75%        0.680000      0.671000      0.672000      0.666000      0.663000   \n",
              "max        4.090000      4.077000      3.757000      4.536000      4.443000   \n",
              "\n",
              "                135           136           137           138           139  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.016622      0.002420      0.000370     -0.009712     -0.002784   \n",
              "std        1.001250      0.994444      1.001023      1.001700      1.007590   \n",
              "min       -4.369000     -3.787000     -4.279000     -4.753000     -4.226000   \n",
              "25%       -0.695750     -0.666000     -0.670000     -0.697000     -0.689000   \n",
              "50%       -0.003500      0.002000     -0.009000     -0.014000     -0.009000   \n",
              "75%        0.662000      0.676000      0.662000      0.668000      0.682750   \n",
              "max        4.646000      4.171000      3.954000      4.109000      4.068000   \n",
              "\n",
              "                140           141           142           143           144  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.009361      0.004658      0.001051      0.002604     -0.009450   \n",
              "std        1.006907      0.992410      0.998474      1.003158      0.994584   \n",
              "min       -4.094000     -3.808000     -4.010000     -3.625000     -3.564000   \n",
              "25%       -0.685000     -0.672000     -0.664000     -0.681750     -0.679750   \n",
              "50%       -0.015000      0.001500     -0.005000      0.006000     -0.014000   \n",
              "75%        0.679000      0.673000      0.675750      0.691000      0.662000   \n",
              "max        3.713000      4.465000      3.963000      3.972000      3.802000   \n",
              "\n",
              "                145           146           147           148           149  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.000002      0.007427     -0.010823     -0.007322      0.002940   \n",
              "std        1.000374      1.000797      1.000051      1.000688      0.992447   \n",
              "min       -3.738000     -3.970000     -4.573000     -4.609000     -3.994000   \n",
              "25%       -0.690000     -0.669750     -0.692000     -0.680000     -0.663000   \n",
              "50%       -0.001000      0.007000     -0.014500     -0.008000      0.000000   \n",
              "75%        0.669000      0.691000      0.666750      0.673000      0.681000   \n",
              "max        4.110000      4.135000      4.197000      4.316000      4.246000   \n",
              "\n",
              "                150           151           152           153           154  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.005598     -0.007599      0.004634      0.002762      0.003087   \n",
              "std        1.006739      1.002076      1.001117      1.008009      0.993495   \n",
              "min       -3.745000     -3.883000     -4.258000     -4.364000     -4.011000   \n",
              "25%       -0.678000     -0.681750     -0.659750     -0.668750     -0.661000   \n",
              "50%        0.010000     -0.012000      0.005000      0.008500      0.007000   \n",
              "75%        0.691000      0.670000      0.670000      0.678750      0.681000   \n",
              "max        3.757000      3.530000      4.407000      3.829000      3.837000   \n",
              "\n",
              "                155           156           157           158           159  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.001680     -0.002814      0.002964      0.010122     -0.001446   \n",
              "std        0.997993      0.998877      0.990670      1.006781      1.006846   \n",
              "min       -4.526000     -3.649000     -3.755000     -4.281000     -3.908000   \n",
              "25%       -0.676000     -0.677000     -0.667000     -0.665750     -0.675000   \n",
              "50%        0.003000     -0.003000      0.010000      0.004000      0.009000   \n",
              "75%        0.674000      0.674000      0.678000      0.699000      0.678000   \n",
              "max        3.630000      4.413000      4.467000      4.154000      4.025000   \n",
              "\n",
              "                160           161           162           163           164  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.007203      0.011646      0.010156      0.004490     -0.009510   \n",
              "std        1.008801      1.003776      0.991647      1.005664      1.002534   \n",
              "min       -3.747000     -4.193000     -4.033000     -4.028000     -3.941000   \n",
              "25%       -0.696000     -0.655750     -0.659000     -0.681000     -0.681000   \n",
              "50%       -0.010000      0.008000      0.014000      0.005000     -0.005500   \n",
              "75%        0.662000      0.685000      0.678000      0.684000      0.675000   \n",
              "max        4.073000      3.697000      3.652000      3.988000      3.921000   \n",
              "\n",
              "                165           166           167           168           169  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.000130      0.001830     -0.004967      0.002333      0.004311   \n",
              "std        0.993729      0.999894      1.004215      1.009588      0.995884   \n",
              "min       -4.237000     -3.784000     -4.044000     -4.094000     -4.124000   \n",
              "25%       -0.663000     -0.672000     -0.677000     -0.680000     -0.661000   \n",
              "50%        0.004000      0.001000     -0.006000      0.003000      0.005000   \n",
              "75%        0.673750      0.679000      0.673750      0.687000      0.675750   \n",
              "max        4.067000      4.243000      3.976000      4.136000      3.617000   \n",
              "\n",
              "                170           171           172           173           174  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.007783     -0.003442     -0.009652     -0.002678     -0.005350   \n",
              "std        0.999296      0.994499      1.003773      0.999609      0.992976   \n",
              "min       -3.922000     -4.052000     -4.624000     -3.790000     -4.099000   \n",
              "25%       -0.681000     -0.677000     -0.684000     -0.668000     -0.682000   \n",
              "50%        0.003000     -0.003000     -0.008000     -0.004000     -0.008000   \n",
              "75%        0.664000      0.664000      0.674750      0.677000      0.671000   \n",
              "max        4.119000      4.174000      4.012000      4.675000      4.262000   \n",
              "\n",
              "                175           176           177           178           179  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.001603     -0.002479     -0.003839      0.006300     -0.001848   \n",
              "std        1.006689      0.999546      0.994778      1.001713      0.996545   \n",
              "min       -4.379000     -3.572000     -3.922000     -4.016000     -4.623000   \n",
              "25%       -0.674000     -0.677000     -0.679000     -0.664000     -0.675000   \n",
              "50%        0.003000      0.000000     -0.004500      0.012000     -0.002000   \n",
              "75%        0.680750      0.671000      0.670000      0.675000      0.671000   \n",
              "max        4.654000      4.367000      4.528000      4.091000      4.147000   \n",
              "\n",
              "                180           181           182           183           184  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.001913      0.009593     -0.003361      0.009558      0.000480   \n",
              "std        0.998949      0.999482      1.002993      1.005222      1.009192   \n",
              "min       -4.215000     -4.045000     -4.205000     -3.954000     -4.017000   \n",
              "25%       -0.677000     -0.651750     -0.679000     -0.671000     -0.670000   \n",
              "50%       -0.004000      0.010000     -0.001000      0.004500      0.007000   \n",
              "75%        0.679000      0.690000      0.672000      0.681750      0.672000   \n",
              "max        4.331000      3.862000      4.419000      3.913000      4.798000   \n",
              "\n",
              "                185           186           187           188           189  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.008838      0.008749     -0.003281      0.001291     -0.001422   \n",
              "std        1.008064      0.997152      1.001473      1.002011      1.005683   \n",
              "min       -3.950000     -3.724000     -4.154000     -3.998000     -3.677000   \n",
              "25%       -0.666000     -0.662000     -0.673000     -0.666000     -0.682000   \n",
              "50%        0.015000      0.002000     -0.008000      0.005000      0.002000   \n",
              "75%        0.688000      0.675750      0.672000      0.668750      0.675000   \n",
              "max        3.648000      4.164000      4.277000      3.989000      4.362000   \n",
              "\n",
              "                190           191           192           193           194  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.002335      0.002579      0.001421     -0.006455     -0.014659   \n",
              "std        1.001773      0.998715      0.999914      1.000233      1.003445   \n",
              "min       -3.717000     -4.062000     -4.150000     -4.277000     -3.791000   \n",
              "25%       -0.675000     -0.674000     -0.668000     -0.681000     -0.689000   \n",
              "50%        0.007000      0.004000     -0.003000     -0.006000     -0.017000   \n",
              "75%        0.673000      0.682000      0.670000      0.664750      0.660000   \n",
              "max        4.257000      4.071000      3.679000      4.025000      3.826000   \n",
              "\n",
              "                195           196           197           198           199  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.005065      0.005232      0.008112     -0.004780     -0.006264   \n",
              "std        1.003014      1.002330      0.995812      0.994961      1.003238   \n",
              "min       -4.050000     -4.068000     -4.008000     -4.098000     -4.004000   \n",
              "25%       -0.683000     -0.666000     -0.656000     -0.677000     -0.684000   \n",
              "50%       -0.002500     -0.001000      0.006000     -0.006000     -0.003000   \n",
              "75%        0.667000      0.682000      0.684750      0.658000      0.671000   \n",
              "max        4.141000      4.226000      4.244000      3.809000      4.242000   \n",
              "\n",
              "                200           201           202           203           204  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.002444     -0.007437     -0.002259      0.000867     -0.000357   \n",
              "std        1.012109      0.998424      1.007469      1.011006      0.994142   \n",
              "min       -3.775000     -3.716000     -4.024000     -3.979000     -4.122000   \n",
              "25%       -0.686750     -0.679000     -0.687000     -0.670000     -0.676000   \n",
              "50%        0.008000     -0.002000     -0.001500      0.007000      0.001500   \n",
              "75%        0.677000      0.664000      0.674000      0.684000      0.668750   \n",
              "max        3.626000      4.052000      4.193000      4.031000      4.084000   \n",
              "\n",
              "                205           206           207           208           209  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.004202      0.013911      0.004091     -0.008164     -0.008366   \n",
              "std        1.003069      0.995562      0.992089      0.992587      0.994560   \n",
              "min       -3.788000     -3.928000     -4.369000     -3.510000     -3.750000   \n",
              "25%       -0.667000     -0.653750     -0.666000     -0.686000     -0.676000   \n",
              "50%        0.011000      0.009000      0.010500     -0.012000     -0.012000   \n",
              "75%        0.677000      0.679000      0.675000      0.660000      0.670000   \n",
              "max        3.670000      3.785000      4.286000      4.039000      4.364000   \n",
              "\n",
              "                210           211           212           213           214  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.003105      0.002344     -0.004260      0.002193      0.012737   \n",
              "std        0.998080      0.995546      1.001011      1.004278      1.002482   \n",
              "min       -3.874000     -5.068000     -4.207000     -4.335000     -4.564000   \n",
              "25%       -0.667000     -0.672000     -0.681000     -0.673000     -0.665750   \n",
              "50%       -0.002000      0.001000     -0.003000     -0.005000      0.019000   \n",
              "75%        0.675750      0.679000      0.678000      0.681000      0.685750   \n",
              "max        3.706000      3.806000      3.891000      3.790000      3.910000   \n",
              "\n",
              "                215           216           217           218           219  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.002243      0.004246     -0.004715      0.004819     -0.000325   \n",
              "std        0.981121      1.005715      0.995507      0.991942      0.999439   \n",
              "min       -3.688000     -3.956000     -4.574000     -4.070000     -4.149000   \n",
              "25%       -0.664750     -0.671750     -0.680000     -0.669000     -0.664000   \n",
              "50%       -0.003000      0.012000      0.000000      0.001000      0.007000   \n",
              "75%        0.659000      0.682000      0.660000      0.674000      0.661000   \n",
              "max        4.373000      3.919000      3.974000      3.852000      4.621000   \n",
              "\n",
              "                220           221           222           223           224  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.000522     -0.001406      0.009292      0.002686     -0.002219   \n",
              "std        1.011381      1.012909      1.008380      1.009298      1.001120   \n",
              "min       -3.844000     -3.733000     -4.014000     -4.163000     -3.870000   \n",
              "25%       -0.678000     -0.688000     -0.668000     -0.678750     -0.674000   \n",
              "50%       -0.002500     -0.003000      0.019000      0.004000      0.010000   \n",
              "75%        0.679000      0.686000      0.686000      0.688000      0.680750   \n",
              "max        3.683000      4.226000      3.735000      3.838000      4.731000   \n",
              "\n",
              "                225           226           227           228           229  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.004088     -0.007355      0.003813      0.020106     -0.018762   \n",
              "std        0.997671      1.001453      0.993863      1.002485      1.001565   \n",
              "min       -5.222000     -3.980000     -3.910000     -3.744000     -4.662000   \n",
              "25%       -0.676000     -0.689000     -0.664000     -0.657750     -0.697750   \n",
              "50%       -0.003000     -0.004500      0.011000      0.019000     -0.018000   \n",
              "75%        0.668000      0.670000      0.671000      0.701000      0.657000   \n",
              "max        4.084000      3.870000      3.720000      4.107000      3.714000   \n",
              "\n",
              "                230           231           232           233           234  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.012009      0.009399     -0.006499     -0.004309     -0.010915   \n",
              "std        0.999866      1.001145      0.996025      1.006610      0.990046   \n",
              "min       -4.243000     -4.323000     -4.396000     -3.932000     -4.783000   \n",
              "25%       -0.663750     -0.671000     -0.675750     -0.680000     -0.675000   \n",
              "50%        0.014500      0.017000     -0.002000     -0.011000     -0.001000   \n",
              "75%        0.687000      0.684000      0.656000      0.670000      0.651000   \n",
              "max        3.884000      3.654000      3.788000      4.364000      4.154000   \n",
              "\n",
              "                235           236           237           238           239  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.005617     -0.000340      0.001938     -0.005009      0.000389   \n",
              "std        1.003458      0.995538      1.003161      1.003950      1.001663   \n",
              "min       -4.050000     -4.265000     -3.875000     -4.115000     -4.225000   \n",
              "25%       -0.659000     -0.665000     -0.682000     -0.685000     -0.669000   \n",
              "50%        0.008000     -0.001000      0.001000     -0.007000     -0.007000   \n",
              "75%        0.677000      0.667000      0.677000      0.670000      0.680000   \n",
              "max        3.971000      3.962000      4.030000      3.675000      4.063000   \n",
              "\n",
              "                240           241           242           243           244  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.008562     -0.010274      0.001926      0.003372     -0.002776   \n",
              "std        1.000705      0.995894      0.998148      0.990183      1.006882   \n",
              "min       -4.303000     -4.277000     -3.669000     -3.802000     -4.360000   \n",
              "25%       -0.660000     -0.686000     -0.673000     -0.663000     -0.672000   \n",
              "50%        0.007000     -0.017000      0.007000     -0.001000     -0.004000   \n",
              "75%        0.680000      0.655000      0.677750      0.669000      0.682000   \n",
              "max        4.118000      3.929000      3.540000      3.639000      3.651000   \n",
              "\n",
              "                245           246           247           248           249  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.006193      0.006598     -0.008309     -0.009133      0.003781   \n",
              "std        0.993086      0.997615      0.993404      0.997070      1.011450   \n",
              "min       -4.064000     -3.856000     -4.282000     -4.248000     -3.813000   \n",
              "25%       -0.659000     -0.674000     -0.685750     -0.688750     -0.679750   \n",
              "50%        0.004000      0.004000     -0.002000     -0.001000     -0.002000   \n",
              "75%        0.673000      0.680000      0.652000      0.666000      0.679750   \n",
              "max        3.731000      3.730000      3.873000      4.628000      3.888000   \n",
              "\n",
              "                250           251           252           253           254  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.003003     -0.002006     -0.001088     -0.007761      0.006484   \n",
              "std        0.997306      1.005657      0.995253      0.993757      0.996798   \n",
              "min       -4.139000     -4.140000     -3.966000     -3.785000     -3.802000   \n",
              "25%       -0.670000     -0.675000     -0.676000     -0.666000     -0.673000   \n",
              "50%        0.000000     -0.001000      0.007000     -0.006000      0.008000   \n",
              "75%        0.678000      0.671000      0.666000      0.656000      0.677000   \n",
              "max        3.843000      3.801000      3.943000      4.348000      4.286000   \n",
              "\n",
              "                255           256           257           258           259  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.013075     -0.008433      0.009949     -0.002582     -0.006509   \n",
              "std        0.998548      1.002636      1.002644      1.002458      1.002878   \n",
              "min       -4.376000     -4.612000     -3.738000     -4.318000     -4.090000   \n",
              "25%       -0.686000     -0.686000     -0.659000     -0.683000     -0.677750   \n",
              "50%       -0.023000     -0.017000      0.013000      0.003500     -0.002000   \n",
              "75%        0.668000      0.670750      0.679000      0.662000      0.672000   \n",
              "max        3.556000      4.101000      4.149000      4.446000      3.770000   \n",
              "\n",
              "                260           261           262           263           264  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.000567     -0.004386      0.000693     -0.007665      0.000038   \n",
              "std        1.009213      1.000429      1.002594      0.999823      1.001426   \n",
              "min       -4.021000     -4.878000     -3.805000     -4.194000     -3.757000   \n",
              "25%       -0.695000     -0.679000     -0.678000     -0.681000     -0.681000   \n",
              "50%       -0.002000     -0.001000      0.008500     -0.004000     -0.002000   \n",
              "75%        0.673000      0.667000      0.675000      0.668000      0.676000   \n",
              "max        3.754000      3.420000      3.871000      4.061000      4.086000   \n",
              "\n",
              "                265           266           267           268           269  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.011357      0.004223      0.008783      0.008566     -0.003155   \n",
              "std        0.999123      1.002517      0.997926      1.012118      1.000383   \n",
              "min       -3.738000     -3.828000     -3.747000     -4.460000     -4.303000   \n",
              "25%       -0.679000     -0.665750     -0.661000     -0.672000     -0.675000   \n",
              "50%       -0.006000      0.010000      0.010000      0.012000     -0.012000   \n",
              "75%        0.660000      0.679000      0.675000      0.693000      0.686000   \n",
              "max        4.109000      3.597000      4.589000      3.928000      3.678000   \n",
              "\n",
              "                270           271           272           273           274  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.009304      0.005396      0.003064      0.001045      0.010622   \n",
              "std        1.002407      0.996813      1.011419      0.994841      1.014016   \n",
              "min       -3.846000     -3.857000     -3.968000     -3.592000     -4.128000   \n",
              "25%       -0.684000     -0.665000     -0.678000     -0.672000     -0.674000   \n",
              "50%       -0.008000      0.009000     -0.000500      0.010000      0.008000   \n",
              "75%        0.662000      0.684000      0.681750      0.667000      0.689000   \n",
              "max        3.839000      3.798000      3.647000      4.215000      4.156000   \n",
              "\n",
              "                275           276           277           278           279  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.001993      0.000681     -0.007535      0.004659     -0.002114   \n",
              "std        1.001478      1.002022      1.003364      0.999189      1.003253   \n",
              "min       -3.562000     -3.952000     -4.585000     -3.849000     -4.036000   \n",
              "25%       -0.672000     -0.678000     -0.683000     -0.677000     -0.677000   \n",
              "50%       -0.004000      0.002000     -0.008000      0.002000     -0.002000   \n",
              "75%        0.670000      0.676000      0.674000      0.678000      0.672000   \n",
              "max        3.899000      3.707000      3.802000      4.087000      4.103000   \n",
              "\n",
              "                280           281           282           283           284  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean      -0.008629      0.014307     -0.001669     -0.001801      0.007137   \n",
              "std        1.004273      0.997789      1.000953      1.007509      0.994646   \n",
              "min       -3.785000     -4.031000     -4.016000     -4.062000     -4.320000   \n",
              "25%       -0.683000     -0.651750     -0.660000     -0.682750     -0.670000   \n",
              "50%       -0.017000      0.019000     -0.001000      0.006000      0.013000   \n",
              "75%        0.666000      0.684750      0.662000      0.676750      0.679000   \n",
              "max        4.275000      3.681000      3.648000      3.880000      3.902000   \n",
              "\n",
              "                285           286           287           288           289  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.000806     -0.006014     -0.004159      0.003853     -0.004600   \n",
              "std        0.991927      0.995396      1.000480      1.004560      0.990087   \n",
              "min       -3.946000     -3.931000     -3.846000     -4.608000     -3.832000   \n",
              "25%       -0.674750     -0.674750     -0.685000     -0.660000     -0.667000   \n",
              "50%        0.005000     -0.001000     -0.003500      0.002000     -0.012000   \n",
              "75%        0.668000      0.667000      0.671750      0.684000      0.653000   \n",
              "max        3.963000      4.099000      4.691000      4.142000      3.777000   \n",
              "\n",
              "                290           291           292           293           294  \\\n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000   \n",
              "mean       0.002577     -0.010130     -0.003961      0.012793      0.009063   \n",
              "std        0.996314      0.996511      0.999788      1.014520      0.994000   \n",
              "min       -3.688000     -3.877000     -3.599000     -3.650000     -3.865000   \n",
              "25%       -0.660000     -0.675000     -0.684750     -0.672000     -0.656750   \n",
              "50%       -0.006000     -0.015000     -0.004000      0.007000      0.001000   \n",
              "75%        0.667000      0.654000      0.680000      0.694000      0.682000   \n",
              "max        3.619000      3.829000      3.717000      5.092000      5.125000   \n",
              "\n",
              "                295           296           297           298           299  \n",
              "count  19750.000000  19750.000000  19750.000000  19750.000000  19750.000000  \n",
              "mean       0.007512     -0.004283     -0.001203      0.013076      0.000070  \n",
              "std        0.999559      0.996270      1.003705      0.996285      1.000596  \n",
              "min       -3.814000     -3.835000     -3.908000     -3.581000     -4.135000  \n",
              "25%       -0.664000     -0.665000     -0.680000     -0.663000     -0.675000  \n",
              "50%        0.001000     -0.001000     -0.010000      0.016000      0.007000  \n",
              "75%        0.685000      0.669000      0.673000      0.686000      0.676000  \n",
              "max        3.681000      3.716000      3.932000      3.764000      4.070000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYpgKLDq6iN1"
      },
      "source": [
        "Final thing that draws our attention just by looking at descriptive statistics table is that the distributions for samples in train set and test set might not be that similar (in contrast to the general assumption for most of the machine learning algorithms). Compare the mean of variable \"1\": mean value in train set is -0.026872 while in the test set its 0.000972). The difference in means is $\\approx$ 27 times larger than the mean value in the test set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWimTPkMZyAb"
      },
      "source": [
        "y = train_df[\"target\"]\n",
        "X = train_df.drop([\"target\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "nRpJEEAoESd0",
        "outputId": "928eb43f-a638-4204-d602-747c6e96dda9"
      },
      "source": [
        "get_diff_columns(X, test_subm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of features with diff distribution : 284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1080 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih0UnoLYrLUL"
      },
      "source": [
        "Have we chosen a different threshold value to consider a difference between distributions of two variables statistically significant, we would have ended up with a different set of columns. For this reason, we go with a safest option available and standardize both the train and test sets (separately, in order to avoid data leakage) before fitting the models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKndW4LeDNcp"
      },
      "source": [
        "Despite the fact that we identified the variables the distributions of which are significally different (relative to the threshold value), it can stil be the case that there are variables the distributions of which are not different, yet they are not normal whereas normality of the distributions is yet another thing that is usually assumed to have in place before fitting the models. For this we additionally check whether the skewness of the distributions of the variables does not exceed the commonly agreed threshold value of 0.75. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbRsUW8smTd8"
      },
      "source": [
        "## Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDyMHRsc2niT"
      },
      "source": [
        "There is another potential issue looming that could have slipped our attention when looking at the statistics' table, namely, that of missing values. For this task we use a function that should provide us with a number and a percentage of total values that a missing for each of the columns in both train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwMVgu1PmXc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "c827e616-85d9-4348-9ab3-67093ffdb3f7"
      },
      "source": [
        "missing_train = missing_values_pct(train_df)\n",
        "missing_train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>Percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Total, Percentage]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ges6QCLYmZF0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "3b13cbdd-0a7f-4cca-cc60-f6a1fe05fb48"
      },
      "source": [
        "missing_test = missing_values_pct(test_subm)\n",
        "missing_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>Percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Total, Percentage]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJ-OLPjfwJ4"
      },
      "source": [
        "## Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6-CTTmleyQk"
      },
      "source": [
        "While there no missing values in the form of `NaN`, it could still be the case that there are datapoints lie far from the general distribution of the variable, or, in other words, *an outlier*. For this, we use a series of techniques for automatic outlier detection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWa-hiTvxtjG"
      },
      "source": [
        "### Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APvA24hOf0pG"
      },
      "source": [
        "iso = IsolationForest(contamination=0.01)\n",
        "yhat = iso.fit_predict(X)\n",
        "\n",
        "mask = yhat != -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "divW39vUhex8"
      },
      "source": [
        "X_out, y_out = X[mask], y[mask]\n",
        "iso_missno = len(X[~X.isin(X_out)].dropna().index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "688rb9pbh-YF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67cdd32-d18e-497d-edbd-e2bd17100cb0"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_iso = get_score(logreg, X_out, y_out)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression with outliers identified and removed using Isolation Forest: \",\n",
        "    \"{:.4f}\".format(score_iso),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression with outliers identified and removed using Isolation Forest:  0.7678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rczx577Bx9cX"
      },
      "source": [
        "### Elliptic Envelope"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6wmoF0_jIQ2"
      },
      "source": [
        "ee = EllipticEnvelope(contamination=0.01)\n",
        "yhat = ee.fit_predict(X)\n",
        "mask = yhat != -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnpKMoCSjyO8"
      },
      "source": [
        "X_out, y_out = X[mask], y[mask]\n",
        "ee_missno = len(X[~X.isin(X_out)].dropna().index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrkykLGmj6ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08976d91-3f76-403f-b86d-07b82d4f175a"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_ee = get_score(logreg, X_out, y_out)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression with outliers identified and removed using Elliptic Envelope: \",\n",
        "    \"{:.4f}\".format(score_ee),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression with outliers identified and removed using Elliptic Envelope:  0.7636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjCftJQjyjS2"
      },
      "source": [
        "### OneClassSVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwNvRHuTknWt"
      },
      "source": [
        "oc = OneClassSVM(nu=0.01)\n",
        "yhat = oc.fit_predict(X)\n",
        "mask = yhat != -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgynTNZ1k0Bz"
      },
      "source": [
        "X_out, y_out = X[mask], y[mask]\n",
        "oc_missno = len(X[~X.isin(X_out)].dropna().index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_72rZIvvk6Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fb3543-a739-481a-9c3b-c4f2325b5964"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_oc = get_score(logreg, X_out, y_out)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression with outliers identified and removed using OneClassSVM: \",\n",
        "    \"{:.4f}\".format(score_oc),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression with outliers identified and removed using OneClassSVM:  0.6917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml2vw4Y0mQqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "696bce96-7682-4449-b6df-aaed164e8912"
      },
      "source": [
        "techniques = [\"Isolation Forest\", \"EllipticEnvelope\", \"OneClassSVM\"]\n",
        "scores = [score_iso, score_ee, score_oc]\n",
        "values_dropped = [iso_missno, ee_missno, oc_missno]\n",
        "\n",
        "pd.DataFrame(\n",
        "    {\n",
        "        \"technique\": techniques,\n",
        "        \"ROCAUC\": scores,\n",
        "        \"Number of rows dropped\": values_dropped,\n",
        "    }\n",
        ").sort_values(by=\"ROCAUC\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>technique</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Number of rows dropped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Isolation Forest</td>\n",
              "      <td>0.767798</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EllipticEnvelope</td>\n",
              "      <td>0.763564</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OneClassSVM</td>\n",
              "      <td>0.691662</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          technique    ROCAUC  Number of rows dropped\n",
              "0  Isolation Forest  0.767798                       3\n",
              "1  EllipticEnvelope  0.763564                       2\n",
              "2       OneClassSVM  0.691662                      49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq_rMR63nzCV"
      },
      "source": [
        "As we can see, `EllipticEnvelope` gave the best improvement in score by dropping only 2 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NynoLNzQmgFG"
      },
      "source": [
        "<a name=\"target\"></a>\n",
        "# Target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meL2rYJDEQNT"
      },
      "source": [
        "Finally we are in a position to look into distribution of the target variable which will show to be quite imbalanced:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU9MOtGSmmDC",
        "outputId": "b38c9686-2620-4d63-e2de-35635eec5b32"
      },
      "source": [
        "counter = Counter(y)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1.0: 160, 0.0: 90})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "46vLctPh3CMG",
        "outputId": "95964955-2288-4d48-f03b-05ec90eb252c"
      },
      "source": [
        "plt.style.use(\"fivethirtyeight\")\n",
        "f, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "train_df[\"target\"].value_counts().plot.pie(\n",
        "    explode=[0, 0.1], autopct=\"%1.1f%%\", ax=ax[0], shadow=True\n",
        ")\n",
        "ax[0].set_title(\"target\")\n",
        "ax[0].set_ylabel(\"\")\n",
        "sns.countplot(\"target\", data=train_df, ax=ax[1])\n",
        "ax[1].set_title(\"target\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAFuCAYAAADNt4pSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXhU5f3+8fdkD0kgAbIBSdhC2EGpsqggKJuAiFBFqbS0SIX2W9FqFbFFbSugKG5IUXEtbkUqa2ldgrKjyBIRAhhZEiEbmZDJMpnl/P6IRvNjSzCTMzO5X9fFJZznnDP3SMiZT57NYrVaDURERERERMRnBZgdQERERERERH4aFXYiIiIiIiI+ToWdiIiIiIiIj1NhJyIiIiIi4uNU2ImIiIiIiPg4FXYiIiIiIiI+ToWdiIiIiIiIj1Nh18hs3LiR6Ohopk+fbnaUejNq1Ciio6M5evSo2VFERES8kp7/Iv5PhZ2IiIiIiIiPU2EnIiIiIiLi41TYNSJz585lzJgxALz11ltER0dX/1q2bBmVlZW88MIL/PznP6d79+7ExcWRkpLC9ddfz/r16896zx49ehAdHY3dbmfu3LlceumlxMbGcv/991ef89FHHzF8+HBatWpF27ZtufXWWzl48CDTp08/5xCK3bt38+tf/5rOnTsTGxtLWloa06ZNIysrq8Z50dHRbN68GYBevXpVv58ePXrU1/82ERERn6bnv0jjEGR2AGk4V155JceOHeOtt96ie/fujBo1qrqtR48eFBUVcf/999O3b18GDx5My5YtOXnyJOvXr2fixIksXLiQKVOmnPXekydPZs+ePVxzzTWMHj2alJQUAN577z2mTp1KaGgoN9xwA4mJiezYsYOhQ4fSvXv3s97r3XffZcaMGYSEhDBy5Ehat25NVlYW7733HuvXr2fNmjX07NkTgPvuu48333yT48ePc8cdd9CsWTOA6v+KiIg0dnr+izQOFqvVapgdQhrOxo0bGTNmDLfccguLFy+u0Wa32ykoKKB169Y1jhcXFzNixAhOnDjB/v37CQ8Pr27r0aMHx48fp2vXrqxevZoWLVpUt5WUlNC9e3dKS0v58MMP6d27d3XbQw89xFNPPQXAnj17qh8EWVlZ9O/fn8TERNatW0erVq1qZL/hhhvo3r07n3zySfXxUaNGsXnz5hr3ERERkR/o+S/i/zQUU6qFhoae8U0dqn76NWnSJKxWK1988cVZr509e3aNb+oA69ato7i4mPHjx9f4pg5wzz33nPWnakuXLsVut/Poo4/W+KYOcNVVVzFy5Ej27NnDgQMH6vr2RERE5Cz0/BfxDxqKKTXs37+fZ555hi1btpCbm0tFRUWN9hMnTpz1uj59+pxxbO/evQD079//jLbIyEh69OjBpk2bahzfvn07AFu2bGHPnj1nXJefnw9AZmYmnTt3rsU7EhERkQvR81/E96mwk2qfffYZ119/PU6nk0GDBjFy5EiioqIICAggIyODdevWYbfbz3ptfHz8GcdOnz4NQGxs7FmviYuLO+PYqVOnAHjuuefOm7W0tPS87SIiIlI7ev6L+AcVdlJtwYIFlJeXs3r1aq666qoabU8++STr1q0757UWi+WMY1FRUcAPP2X7/+Xl5Z1xrGnTpgB88803xMTE1Dq7iIiIXBw9/0X8g+bYNTKBgYEAuFyuM9qysrKIiYk545s6UL2kcF18v3LV1q1bz2iz2WxkZGSccfyyyy4DqoZi1Nb378ntdtc5o4iISGOg57+I/1Nh18g0b94cgOzs7DPakpOTKSoq4ssvv6xx/PXXX+ejjz6q82tdd911NG3alBUrVrB79+4abQsWLKC4uPiMa6ZNm0ZISAgPPvggBw8ePKPd6XTy6aefnvU9HT9+vM4ZRUREGgM9/0X8n7Y7aGRcLhe9evUiJyeHCRMm0KFDBwIDAxk5ciQnT55kwoQJREVFccMNN9C0aVN27drFtm3bGDNmDCtXrmTRokVMmjSp+n7fL3dstVrP+nrvvvsuv/3tbwkLC6uxj01GRgbdu3dn8+bNZGRkkJSUVH3N8uXL+d3vfofT6eTaa6+lQ4cOuFwucnJy2L59O3a7nWPHjlWf/9prr3HnnXfSrl07rr/+eiIjI2nWrBnTpk3z3P9IERERH6Lnv4j/0xy7RiYwMJB//vOfPPTQQ/z3v/+lpKQEwzBo1aoVkyZN4u2332bBggX8+9//JiAggD59+rB69WqOHDnCypUr6/x6N910EzExMTz++OO8//77hISEMGDAAD744AP+/Oc/Az+Mxf/ehAkT6N69O4sWLeKTTz4hPT2dsLAwEhISGDp0KNdff32N82+77Tays7NZvnw5zz//PA6Hg6SkJH1jFxER+Y6e/yL+Tz12Yorvf3LocDjIzMw0O46IiIg0AD3/RTxHc+zEo4qLiykrK6txzDAMHn/8cbKzsxk9erRJyURERMRT9PwXaXjqsROP2rBhA5MnT2bw4MEkJydTWlrKZ599RkZGBm3atCE9Pf2c+9yIiIiIb9LzX6ThqbATjzp27Bh/+9vf2LZtGwUFBTidTlq1asXw4cP54x//eNZNSkVERMS36fkv0vBU2ImIiIiIiPg4zbETERERERHxcSrsREREREREfJwKOxERERERER+nwk5ERERERMTHqbATERERERHxcSrsREREREREfJwKOxERERERER+nwk5ERERERMTHqbATERERERHxcSrsREREREREfJwKOxERERERER+nwk5ERLzS5s2bmThxIl26dCE6Opply5adcc7hw4f5xS9+QXJyMomJiQwcOJDMzMzqdrvdzr333kv79u1p1aoVEydOJCcnpyHfhoiISINQYSciIl6ptLSUrl27Mm/ePMLDw89oP3LkCMOHDyclJYVVq1axdetWHnzwQSIiIqrPmTVrFqtXr2bp0qWsW7eOkpISbr75ZlwuV0O+FREREY+zWK1Ww+wQIiIi59O6dWsee+wxJk2aVH1s6tSpWCwWXnzxxbNeU1xcTMeOHVm0aBE33XQTANnZ2fTo0YPly5dzzTXXNEh2ERGRhqAeOxER8Tlut5v169eTlpbG+PHj6dChA4MHD2bFihXV5+zevRuHw8GQIUOqj7Vp04a0tDS2b99uRmwRERGPCTI7gIiISF3l5+djs9l48skneeCBB5gzZw6ffvopt99+OxEREQwfPpy8vDwCAwNp0aJFjWtjY2PJy8s7570PHTrk6fgi8hMkPP8XsyOI1MrJGY/U6/1SU1PP267CTkREfI7b7Qbguuuu4/e//z0APXv2ZPfu3bz44osMHz78ou99oQeniJirxOwAIrXU0M8TDcUUERGf06JFC4KCgkhLS6txvFOnTmRnZwMQFxeHy+WisLCwxjn5+fnExcU1WFYREZGGoMJORER8TkhICJdeeukZwyYPHz5MUlISAL179yY4OJj09PTq9pycHDIzM+nbt2+D5hUREfE0DcUUERGvZLPZyMrKAqqGXmZnZ7N3715iYmJISkriD3/4A1OmTGHAgAEMHDiQjRs3smLFiur97po1a8Ztt93GnDlziI2NJSYmhtmzZ9OtWzeuvvpqE9+ZiIhI/dN2ByIi4pU2btzImDFjzjh+yy23sHjxYgCWLVvGk08+SU5ODu3bt+fuu+9mwoQJ1efa7XYefPBBli9fTkVFBQMHDuSJJ56gTZs2DfY+RKR+ldx1i9kRRGolauFbDfp6KuxERERExGeosBNf0dCFnebYiYiIiIiI+DgVdiIiIiIiIj5OhZ2IiIiIiIiPU2F3kTZv3szEiRPp0qUL0dHR1auwnc++ffu47rrrSEhIoEuXLsyfPx/D0BRHERERERH5aVTYXaTS0lK6du3KvHnzCA8Pv+D5p0+fZty4ccTFxfHxxx8zb948nn32WZ577rkGSCsiIiIiIv5M+9hdpGHDhjFs2DAAZsyYccHz//Wvf1FeXs7ixYsJDw+na9euHDx4kOeff57f//73WCwWT0cWERERERE/pR67BrJjxw769+9fo3fvmmuu4cSJExw9etTEZCIiIiIi4uvUY9dA8vLyaNWqVY1jsbGx1W1t27Y1IVX9K3G4KXUYnGvmoAUID7IQFWwhQL2UIiIiIiL1QoWdXNCpChfHbC6O2lwcK3FyzObiZLmL4kqD4kp39a/TlQauWq4FYwGaBFloGmIhJiSAmLAAmocGkBAeSHJkIMlRQaREBpISFURMqDqWRURERETOR4VdA4mLiyM/P7/Gse//HBcXZ0akM3xb6iLjlIO9hZVknHJw+LSTbJuL0476X7nTAEqdBqVOgxNl7vOe2zTYQlJkIB2aBtGjeTA9W4TQs0UwiU0C6z2XiIiIiIgvUmHXQC6//HIeeughKioqCAsLAyA9PZ3ExERSUlIaPE9BhYvNJyvZVVDJ3kIHGacc5Fecv8Ayy2mHwb4iJ/uKnKw6WlF9PDYsgJ4tgunRPJhLWoZwRUIILcNU7ImIiIhI46PC7iLZbDaysrIAcLvdZGdns3fvXmJiYkhKSuLhhx9m586drFq1CoAJEyYwf/58ZsyYwT333MPhw4d56qmn+NOf/tQgK2Ja7W42nbTz6Qk7m07Y2W91nnMenK/Ir3DzUY6dj3LsQNXwzi7RQVyVGMrAxFCuSAglWsM4RURERKQRsFitVl//fG+KjRs3MmbMmDOO33LLLSxevJjp06ezadMmMjIyqtv27dvHPffcwxdffEF0dDRTpkzhvvvu80hhZxgGn+c7WHusnPRv7WSccuBuZH/TARbo2TyYq1uFMjolnD4tg7WthIiIiI8ruesWsyOI1ErUwrca9PVU2PkRt2GwJbeSVUfKWXu0gpwyl9mRvErrJoGMTgnj+rbh9I8P0aqcIiIiPkiFnfiKhi7sNBTTxxmGwacnKnn/SBlrj1WQV+6d8+S8QU6ZiyX7S1myv5S48ABGJYdxQ9smDEwMUU+eiIiIiPg0FXY+6mSZi2WHynjjUClHStQzV1d55W5eySzjlcwy2kYFcltqBJNSm5CglTZFRERExAepsPMhbsPgg2w7rx0s5X/HK3BqEG29OFLi4q9fnGburtMMTwrjl50iuLZNqIZqioiIiIjPUGHnA4rsbl7ab+O1g2Vkl6p3zlOcBqw9VsHaYxW0iQjkV2kRTO0coZU1RURERMTrqbDzYkdLnDy3z8ayg6VoHZSGlV3q4m9fnObpjBJ+lRbB77pFapimiIiIiHgtFXZeKNPq4Im9JbyXVY5Lwy1NVeIwePZLGy/st3Fzhybc2T2KDs30z0ZEREREvIs+oXqRA1YHc3edZvWRCrS2pXexu+D1g2X881AZY1PCuf+SKNKig82OJSIiIiICqLDzCrllLh7ddZp/HirFZWjBDm/mNuDfR8pZdbSc21KbMOuSpsRriKaIiIiImEyFnYlKHW6eySjhmYwSyt0WQEWdr3AZ8OrBMpZnlfO77pH8oXskEcFaZEVEREREzKFPoiZwuQ1ezSyl179OMH+P7buiTnyRzWkwf3cJl76Xy6uZpbjcmhQpUl82b97MxIkT6dKlC9HR0Sxbtuyc586cOZPo6GieffbZGsftdjv33nsv7du3p1WrVkycOJGcnBxPRxcREWlwKuwa2Of5lVzx75PM3GKlwG52GqkvueVuZm6xctWqPLbn6i9WpD6UlpbStWtX5s2bR3h4+DnPW7lyJTt37iQxMfGMtlmzZrF69WqWLl3KunXrKCkp4eabb8bl0lLDIiLiX1TYNZASh5u7NxcydE0eB05raRR/9VWRkxHrCrhzcxFWu/6eRX6KYcOG8Ze//IWxY8cSEHD2x9WxY8e4//77eemllwgKqjm7oLi4mDfeeINHHnmEwYMH07t3b5YsWcK+ffvYsGFDA7wDERGRhqPCrgGsO1bOJe/k8PLBCgzNo/N7BvDawTIu/3cu739TbnYcEb/ldDqZOnUq99xzD2lpaWe07969G4fDwZAhQ6qPtWnThrS0NLZv396QUUVERDxOi6d4UG6Zizs35rP+WxeqoRufvHI3v9pwitFZYTzRP1qrZ4rUs7lz59K8eXN+85vfnLU9Ly+PwMBAWrRoUeN4bGwseXl557zvoUOH6jWniNSvBLMDiNRSfT9PUlNTz9uuws5DVn5Tyu8+PYXNrYKusVtzrILNubk8e0UMo1POPU9IRGpv48aNvPnmm2zcuLHe732hB6eImKvE7AAitdTQzxNVHfWswmlw+4ff8ssNVhV1Uq3IbvCLj0/xx61WKpxaOVPkp9q0aRMnT54kLS2NFi1a0KJFC44fP86cOXPo2rUrAHFxcbhcLgoLC2tcm5+fT1xcnBmxRUREPEY9dvVoX2EFN/3nBDmOELOjiJdaeqCUrSftLL26OV1igs2OI+Kzpk6dytixY2scGz9+POPHj+eXv/wlAL179yY4OJj09HR+/vOfA5CTk0NmZiZ9+/Zt8MwiIiKepMKunjy7q4CHd5fhREWdnN9XVidDVufz98ub8evOEWbHEfFaNpuNrKwsANxuN9nZ2ezdu5eYmBiSkpKIjY2tcX5QUBDx8fHVQ1+aNWvGbbfdxpw5c4iNjSUmJobZs2fTrVs3rr766oZ+OyIiIh6lsYI/UZnTzZj3j/Dn3XacaHEMqZ1yl8HdW638Kv0UZU5tiyByNrt27WLgwIEMHDiQ8vJy5s6dy8CBA3n00UdrfY+5c+cyatQopkyZwogRI4iIiODtt98mMFDfr0VExL9YrFarJvxcpK+LKrh+dTY5Li2IIRevR/Ng3rqmOW0i1YEuIiJyISV33WJ2BJFaiVr4VoO+nnrsLtJ/DuYz8N8nVNTJT5ZxysGQNfnsyLObHUVEREREfJQKu4swf9MRfrGpnFKL5tNJ/cgrdzNmfQFvHio1O4qIiIiI+CCN/aoDl9vNr1YdYnVRJFjMTiP+xu6CGZusfFXk5JHLmhJg0ReZiIiIiNSOeuxqqbzSyTVvHagq6kQ86Ll9NqZsOEWlS9NfRURERKR2VNjVQl5xGVe9eYDdlc3MjiKNxMojFdz6USHl2sxcRERERGpBhd0FZOUWMWT5Nxw2YsyOIo3Mhzl2bvxfAacrtR2CiIiIiJyfCrvzyDiWz4g1J8kOiDY7ijRSW3MrGbO+gMIKl9lRRERERMSLqbA7h62Hcrj+gyLyApqaHUUauT2FDq5bV8C3pSruREREROTsVNidxaYDR5n4aRlFARFmRxEBILPYyej/5JNbpuJORERERM6kwu7/80nGYX65uZzigCZmRxGpIavExbj/FWC1a86diIiIiNSkwu5HNmUcYtp2O4UBUWZHETmrr4qcTPigAJtDxZ2IiIiI/ECF3Xd2HTrKHdsryA3UQini3T7PdzDpo1PYtc+diIiIiHxHhR1w4EgOv/mkiOzA5mZHEamVT07YmbLhFE63ijsRERERUWHH8dx8pnzwLVmBsWZHEamTdccq+MNmq9kxRERERMQLNOrCLrfwFNNX7mN/UCuzo4hclDcPl/FsRonZMURERETEZI22sCu2lXL/ih1sDupodhSRn2TOztP873iF2TFERERExESNsrCrdDh49L101gZ2wbBYzI4j8pO4DZj6ySkOWB1mRxERERERkzS6ws4wDJasSudtVxqVliCz44jUi9MOg1s+LORUhTYwFxEREWmMGl1ht/LT7fyjKJFiizYgF//yTYmLyemncGilTBEREZFGp1EVdju/OsRTmW5ytK2B+KlNJyv5687TZscQERERkQbWaAq77NwCnvr0ILuDU8yOIuJRz35pIz1Hi6mIiIiINCaNorArq7Dz3KoNfBjSzewoIh5nAHdsLCK/XPPtRERERBoLvy/sDMPg9TUfsiagM+WWYLPj1E1xPiybBQ9eCfdeAvPGwOHPzn7uuw/BXd0g/ZUL3/fwZ/DEz6vu+dfhsPmdmu0718DD18AD/eH9+TXbrLnwyFAoKbiotyQNI7fczYyNRRiG5tuJiIiINAZ+vyzkx9t3sfJUJNnBMWZHqZvy0/DML6D9pXD78xDZHAqzIeos8wN3/xeOZUCzuAvftzAbXpwOl4+DX8yDrC9g+d8gMgZ6DQNbEbzzF7jl79CiDbw4A1L7Qrerq65/728w7A6Ialmvb1fq3wc5dhbts/H77lFmRxERERERD/PrHrvs3ALe2XWE7UHtzY5Sdx+/DE1bwqS5kNKzqsjq1A/iO9Q879S38P48uO0xCKhFnb7lHWgaC+NnV92r/8/hsrGQ/mpVe+FxCIuES0ZCcg/oeDnkZlW17fkfVJRA3xvr9a2K5zyy8zS7CyrNjiFyUTZv3szEiRPp0qUL0dHRLFu2rLrN4XAwZ84cBgwYQKtWrUhLS2Pq1KkcP368xj3sdjv33nsv7du3p1WrVkycOJGcnJyGfisiIiIe57eFncPp5I21H/NxaHfcFh98mxkfVRV0r/0R/nwVPH4jbFwGPx5a53LCG/fC0N+eWfCdy5E9kDag5rHOV8DxfeByQGwKVFZA9n4otcKxL6FVJygvgVVPwE0PgzZ19xmVbpixsUhbIIhPKi0tpWvXrsybN4/w8PAabWVlZezZs4d77rmHTz75hDfffJOcnBwmTJiA0+msPm/WrFmsXr2apUuXsm7dOkpKSrj55ptxuTQHVURE/IvfDsVc/sFGPnUmUBAcYXaUi1OYDZvfhkGT4ZqpkHMAVvy9qu2qSVX/Xb8IIqLhiom1v29JQVXP349FtQC3E2xWaBYLtz4Kb84CRwVcdj10vrJqDl+/G8F2Cl6/FyrLYeAv4Iqb6+Xtiud8ZXXydIaNe3ppSKb4lmHDhjFs2DAAZsyYUaOtWbNmvP/++zWOLVy4kH79+pGZmUm3bt0oLi7mjTfeYNGiRQwePBiAJUuW0KNHDzZs2MA111zTMG9ERESkAfhlYbc/6yibs/LZEd7f7CgXz3BDUncYfVfVn9t0gYKjsOntqsLu8A747H245736f+2e11b9+l7WTji6B8beC3NHw61zq3oIHx8H7S6p6tETr/b4ntPc0DaMjs18bAEhkTooKSkBIDo6GoDdu3fjcDgYMmRI9Tlt2rQhLS2N7du3q7ATERG/4neFXaXDwb/Tt/JJeA9cvjgE83tNY88cXhnXHqz/rPr94c/gdD7MufqHdrcLVj8Jn7wBD3189vtGtYSSwprHSgqr5udFRp95vrMS/vUI3PxIVS+i0/FDj1/Hy6pyqLDzenYXzNxiZfWIllg0lFb8UGVlJQ8++CAjRoygdevWAOTl5REYGEiLFi1qnBsbG0teXt4573Xo0CGPZhWRnybB7AAitVTfz5PU1NTztvtdYbfm02184WrJ8eCzFCm+pN0lkPdNzWP5RyAmser3V0ysWsXyx5ZMg0uug/4Tzn3ftr2q5u/9WOYWSOoGgWfpzfnghapVMdv2gpz9VcXj91wOMDRPxVdsOlnJG4fKmNzJR4cni5yD0+lk2rRpFBcX89Zbb/3k+13owSki5ioxO4BILTX088SHu7TOlJ1bwOb9x9kU1NHsKD/doMlwdC98sATyj1ZtabBxGVx5S1V7VAtITK35KyCoaiXNuHY/3GfZrKpf3xtwMxTnwb/nQu7XsG151ZDOwb86M8PJw/DFGrjuzqo/x7aDgMCqfe++3gkHt0G7Sz32v0Dq318+KyZPG5eLH3E6nfzmN79h3759rFy5kubNf9gSJi4uDpfLRWFhzVEK+fn5xMXVYnsYERERH+I3hZ3b7ead/25ge3gaFb62EfnZJPeAXz8Du9fDYzfAuqdh5P/BFbfU7T5FJ6p+fa9FG7h9cVVh9vj4qh65cQ+c2ftnGFULpoy9D8K+6+EJCavafiH9FXjlD1WrcSZ3/0lvUxqWtdLgL58Vmx1DpF44HA6mTJnCvn37WL16NfHx8TXae/fuTXBwMOnp6dXHcnJyyMzMpG/fvg0dV0RExKMsVqvVL9ZB/3DbF7z3RRZvh/XD0BwikXOyAOljYundMsTsKCLnZbPZyMqq2kdz+PDhzJw5k5EjRxITE0NiYiKTJ09m165dvPXWWyQmJlZf17Rp0+rtEe6++27Wr1/P888/T0xMDLNnz8ZqtfLJJ58QGBhoyvsSkZ+m5K46/pBbxCRRC3/69IC68IvC7nRpGU+8vpyVoZdyxBJjdhwRr3dFQghrR8aaHUPkvDZu3MiYMWPOOH7LLbdw//3306tXr7Net2jRIiZNqtoWxm638+CDD7J8+XIqKioYOHAgTzzxBG3atPFodhHxHBV24itU2F2EN1Z/yIaTlbwXdPaHvIicadmQ5oxKCb/wiSIiIl5EhZ34ioYu7Hx+jt2J/EIOHM3m08AOFz5ZRKo9svM0LrfP/1xHRERERPCDwu799C1khSWRb4k0O4qIT8ksdrLscJnZMURERESkHvh0YZd5JJtjeYVsCmh34ZNF5Azzd5Vgd6nXTkRERMTX+WxhZxgG6zZu4+vwFEosoWbHEfFJOWUu3lKvnYiIiIjP89nCbseXmRSeLmWHJcnsKCI+7ZmMEtyGeu1EREREfJlPFnZut5tPP9/DsbDWWC1a1U/kp8gqcbHySLnZMURERETkJ/DJwm7PwSxOl5Wz3aJ9iETqw8K9NrMjiIiIiMhP4HOFnWEYbPh8DyfDEsi1RJkdR8Qv7D3l4KOcCrNjiIiIiMhF8rnC7uDRHAqLTrNNc+tE6tWTe0vMjiAiIiIiF8nnCrsPtu2kJLwFxy3RZkcR8SubT1ayu6DS7BgiIiIichF8qrA7fiKPE/mn2BOQaHYUEb/0+kFtfSAiIiLii3yqsPvv1s8JDg/nK+LMjiLil5ZnlVHqcJsdQ0RERETqyGcKO1tZOcdP5pNpiaPSEmR2HBG/dNph8G9tfSAiIiLic3ymsNu060ssARb2WDQMU8STXs/UcEwRERERX+MThZ1hGGQc+obTwc341tLU7Dgifm1HfiVfFTnMjiEiIiIideAThd3X2d9iLbGpt06kgbyWWWp2BBERERGpA58o7D79PIPw8DC+ItbsKCKNwnvflONyG2bHEBEREZFa8vrCrqzCztETuWRboim3hJgdR6RRKKhwsyVXe9qJiIiI+AqvL+y2Z+wH4KClpclJRBqXVVodUxIZXe4AACAASURBVERERMRneH1h99XXRwkNDeEgKuxEGtKaY+UYhoZjioiIiPgCry7sSssryCu0kkNTbJZQs+OINConytzsyNNwTBERERFf4NWF3RdfHQILZGoYpogpVh2tMDuCiIiIiNSCVxd2X359hDANwxQxzaqjmmcnIiIi4gu8trArt9vJLSginyactoSZHUekUTpuc7Ffm5WLiIiIeD2vLex2Z36NG4NjRJsdRaRR23jCbnYEaaQ2b97MxIkT6dKlC9HR0SxbtqxGu2EYzJ07l86dO5OQkMCoUaPYv39/jXOsVivTpk0jOTmZ5ORkpk2bhtVqbci3ISIi0iC8trDbd/gI4aEhHLWosBMx06cq7MQkpaWldO3alXnz5hEeHn5G+9NPP82iRYuYP38+H3/8MbGxsYwbN46SkpLqc6ZOncrevXtZvnw5y5cvZ+/evfz2t79tyLchIiLSIILMDnA2hmGQW2jFEmDhOM3MjiPSqG3OteM2DAIsFrOjSCMzbNgwhg0bBsCMGTNqtBmGweLFi5k5cyZjx44FYPHixaSmprJ8+XKmTJlCZmYmH374IevXr+fyyy8HYOHChYwcOZJDhw6RmprasG9IRETEg7yyxy63sIiyigpyiaTCEmx2HJFGrchukHFK8+zEuxw9epTc3FyGDBlSfSw8PJwBAwawfft2AHbs2EFkZCR9+/atPqdfv35ERERUnyMiIuIvvLLHbnfm14SGBLNP8+tEvMKnJ+z0ahFidgyRarm5uQDExsbWOB4bG8uJEycAyMvLo0WLFlh+1NtssVho2bIleXl557z3oUOHPJBYROpLgtkBRGqpvp8nFxpp4pWF3dETuQQHBXFM8+tEvMLGE3b+r3uU2TFEGoSGaIp4t5ILnyLiFRr6eeJ1QzHdbjf5p6pWLPsWfZAU8QY78irNjiBSQ3x8PAD5+fk1jufn5xMXFwdAXFwchYWFGIZR3W4YBgUFBdXniIiI+AuvK+xy8gooq7BTTKjm14l4CWulwTGb0+wYItVSUlKIj48nPT29+lhFRQVbt26tnlN3+eWXY7PZ2LFjR/U5O3bsoLS0tMa8OxEREX/gdUMxMw4dITw0lCwizY4iIj+yt9BBcqTXfcsQP2az2cjKygKqRnNkZ2ezd+9eYmJiSEpKYvr06Tz55JOkpqbSsWNHFixYQEREBBMmTAAgLS2Na6+9lrvuuounnnoKgLvuuovhw4druKWIiPgdr+uxyy08RVBQILkWFXYi3mSvVsaUBrZr1y4GDhzIwIEDKS8vZ+7cuQwcOJBHH30UgDvvvJPp06dz7733MnjwYE6ePMmKFSuIivphGP9LL71E9+7dGT9+POPHj6d79+4sWbLErLckIiLiMRar1Wpc+LSG88Try6l0OFhu6cbXlhZmxxGR74xICuPta/VvUs5t/vz5jBkzhq5du561ff/+/axatYr77ruvgZOJiD8puesWsyOI1ErUwrca9PW8qseu0uHgtK0UgDwNxRTxKhmF6rGT85s3bx779u07Z/v+/fuZP39+AyYSERFpPLyqsMsttOJwOikjiBJLqNlxRORHcspcFFa4zI4hPsxmsxEcrEWxREREPMGrVkI4dDSbkJBgThJhdhQROYuvipxclRhodgzxIl9++SUZGRnVf966dStO55krqFqtVl5++WUtWiIiIuIhXlXYZecVEBocjJVws6OIyFkctTm5CvWmyw/WrFlTPbzSYrHwyiuv8Morr5z13OjoaF544YWGjCciItJoeFVhZz1tA6DIEmZyEhE5m2M2DcWUmn71q18xYsQIDMNgyJAhPPDAAwwdOvSM8yIiImjXrh1BQV712BEREfEbXvOENQyDkrIyAgIC1GMn4qWOlWiTcqkpISGBhIQEAFavXk1aWhqxsbEmpxIREWl8vKawK7dXYq90EB4WSjHqsRPxRkfVYyfnceWVV5odQUREpNHymsLOWmLD6az60Hhac3hEvNJxFXZyAR999BFvvPEGR44cwWq1Yhg1t0q1WCzs3r3bpHQiIiL+y2sKuxP5pwgIDMCJhTK0HLaIN/q2zIXDbRAcYDE7inihZ555hoceeoi4uDguvfTSc25ULiIiIvXPawq7kwWnCA0JxkYoWPShUcQbuQ3IKXXRNsprvnWIF/nHP/7BwIED+de//qX96kRERBqY13w6s5bYCAoMpNx7IonIWVjtbogyO4V4I6vVytixY1XU/Ui353eYHUGkVvbNuNzsCCLyEwWYHeB7ZRV2ACpU2Il4teJKt9kRxEv16dOHQ4cOmR1DRESkUfKawq6ishIAuwo7Ea9mrTQufJI0SgsWLGDNmjW8++67ZkcRERFpdLyminI4qvbHUo+diHdTj52cy+TJk6msrOSOO+7grrvuIjExkcDAwBrnWCwWtm3bZlJCERER/+U1VVSls6qwU4+diHcrtquwk7Nr2bIlsbGxdOzY0ewoIiIijY7XVFEOh5OgoEDsFq+JJCJnUayhmHIOa9euNTuCiIhIo+UVc+xcLhdOV9XGx3YCL3C2iJhJQzFFREREvI9XdI9VVDowjKpeAJd31Joicg52t3rs5Ow2b95cq/OuuOIKDycRERFpfLyisLNXVuI29GFRxBe49E9VzmH06NFYLJYLnnfq1KkGSCMiItK4eEVh53C6qnvsRMS7qcNOzmX16tVnHHO5XBw7dozXXnsNt9vNnDlzTEgmIiLi/7yisANQXeefIg07vThpdgypBzFRkXTr2JbuzYPNjiJe6sorrzxn26RJkxg5ciSbNm1i0KBBDZhKRESkcfCawk78T5Dh4kZjH4nYzI4i9aBdZAITL+lpdgzxUQEBAdx4440sXLiQ2bNnmx1HRETE76iwE4+5xr6PZq5CyswOIvUiIFALG8lPU1RURHFxsdkxRERE/JIKO/GI8S1sPDN0sNkxpB6FBmsIppzf8ePHz3q8uLiYLVu28Oyzz9K/f/96ez2Xy8XcuXN59913yc3NJT4+nptuuon777+foKCqx5thGMybN4/XXnsNq9VKnz59WLBgAV26dKm3HCIiIt7A6wq7QLRHlq/rFVrCi6M7ERCgHh6RxqRnz57nXBXTMAwuu+wyFi5cWG+v99RTT/HSSy+xePFiunbtyr59+5g+fTohISH86U9/AuDpp59m0aJFLFq0iNTUVB577DHGjRvHZ599RlRUVL1lERERMZvXFXahuMyOID9BgqWU1Td2UFEn0gg999xzZxR2FouF6Oho2rVrR+fOnev19Xbs2MGIESMYOXIkACkpKYwcOZKdO3cCVcXk4sWLmTlzJmPHjgVg8eLFpKamsnz5cqZMmVKveURERMzkFYXdjz8IhBpOuPA2SOKFmhiVrBrTmqZhGrIn0hhNmjSpQV+vX79+LF26lIMHD9KpUycOHDjAxo0bueuuuwA4evQoubm5DBkypPqa8PBwBgwYwPbt21XYiYiIX/GKwi4kOIiA74q7UJwmp5GLEWi4eWFAJJ1aRpgdRURM5nK52LNnD8eOHQMgOTmZ3r1713tP/syZM7HZbPTt25fAwECcTif33HMPU6dOBSA3NxeA2NjYGtfFxsZy4sSJc9730KFD9ZpTxBf40td9gtkBRGqpvv9dpaamnrfdKwq7sJAQAgKqCrswFXY+6Z6OTkZ3jr3wiSLi11asWMHs2bPJzc3F+G6DUovFQnx8PI8++ijjxo2r19d6++23eemll+jcuTMZGRncf//9JCcnM3ny5Iu+74UenHXywY76u5eIB9Xr172HlZgdQKSWGvrflVcUdqEhwdXDMdVj53tGxdiYNTDN7BgiYrK1a9cydepUOnXqxN13302nTp0AOHjwIC+//DJTp04lNDSU6667rl5e7y9/+Qu///3vGT9+PADdunXj+PHjLFy4kMmTJxMfHw9Afn4+SUlJ1dfl5+cTFxdXLxlERES8hVescGGxWAgJrqox1WPnW7oEl/DaGN/5KZ+IeM4TTzxB79692bBhA7fffjuDBg1i0KBB3H777WzYsIGePXuyYMGCenu9srIyAgMDaxwLDAzE7a5aXTklJYX4+HjS09Or2ysqKti6dSt9+/attxwiIiLewCt67ACCg4NwudyEq7DzGS0pY82N7Qmq48bVQemrCH37Hx5KJWZwXDWSyl/8n9kxxGT79+9nzpw5hIWFndEWGhrKzTffzMMPP1xvrzdixAieeuopUlJS6Ny5M3v37mXRokVMnDgRqPqh4fTp03nyySdJTU2lY8eOLFiwgIiICCZMmFBvOURERLyB1xR2IUFBlLsqicKOxTAwzrEXkniHMMPBiusSaNEkpE7XBe7fRegbT2NxaVsLf2Jx6QcyUrXiZGFh4TnbCwoKCA8Pr7fXe+yxx/j73//OH//4RwoKCoiPj+eXv/xl9R52AHfeeSfl5eXce++91RuUr1ixQnvYiYiI3/Gewi44mHJ7JYEYRFJJCaFmR5JzCDDcPNc3jJ4Jtf9g5HI62fbKPxi0bZWKOj9kaN9CAQYNGsSSJUsYPHgwAwYMqNG2bds2XnjhBa699tp6e72oqCjmzZvHvHnzznmOxWJh1qxZzJo1q95eV0RExBt5TWEXHhpKsa0UgGZUqLDzYr9r62BCt6QLn/gdwzDY+t7b9P3sP4Q5Kz2YTEyjwk6Ahx9+mK1btzJ69Gh69epVvRrYoUOH2LNnD/Hx8Tz00EPmhhQREfFTXvNpLLJJWPXS2M2oMDmNnMuQqBL+OqR9na7ZuzGdrhvfJ8Ze6qFUYrrgug3JFf+UnJzMpk2buOOOO7DZbKxatYpVq1Zhs9mYMWMGGzduJDk52eyYIiIifslreuwSWjZn39dHCA0JUWHnpToE2nhnXKc6XXPswH4iV75GckmBh1KJNzAiNF9JoLS0lPLych599FEeffTRM9qPHz9OWVkZTZo0MSGdiIiIf/OaHrtWsc2xV1YtwNDMUGHnbWIoZ92NbQmuwwqY1vw88t54nl4FRz2YTLyBEdHU7AjiBR544AFuvfXWc7ZPmjSJP//5zw2YSEREpPHwmsKuebOmBHw3TydaPXZeJcRw8q+hscRH1n7eY2VFBXtfeoarcvZ5MJl4CyNShZ1Aeno6o0ePPmf76NGj+eijjxowkYiISOPhNYVd04gmBH230WxLNBfLW1gMgwWXBPGzNs1qfY3b5WLzq//g6sM7CDTcHkwnXkNDMQXIzc0lMTHxnO3x8fGcPHmyAROJiIg0Hl5T2AUGBtIkrKpHqAlOogy7yYkE4NetK5h8Ses6XbNj5XL67/qIJloBs9HQUEwBaNmyJQcOHDhn+4EDB2jWrPY/JBIREZHa85rCDiAq4oeNa+OwmZhEAK6IKOGJ4R3rdM2+rZtpn/4eLStKPJRKvJERqR47gaFDh/Lqq6+ya9euM9q++OILXn31VYYOHWpCMhEREf/nNatiArRo1hRriQ2LxUI8Nr6mhdmRGq2UABvvjUut0zU5Xx8i6L2ltD+d56FU4q3UYycAs2bN4oMPPmDo0KEMHTqULl26APDVV1/x4YcfEhcXx+zZs01OKSIi4p+8qrBr1yaRLw9/Q5PwMOING1jMTtQ4NTUqWHNDCmHBgbW+5vSpU+S89hyD87M8mEy8kREaBqFhZscQLxAfH096ejpz5sxh7dq1rF+/HoCoqChuuukm5syZQ3x8vMkpRURE/JNXFXbtWyfgrtqjnDgtoGKKYMPJsiExJDWr/Qd1R6WdXUufYfixDNXijZA7tpXZEcSLxMXFsXjxYgzDoKCgav/Kli1bYrHou4OIiIgneVVh17xZFKEhVZGiqSDUcGC3BJucqhExDP7aI5Cr2jav9SVut5str7/E1ZlbCdIKmI2SkdDG7AjihSwWC7GxsWbHEBERaTS8avGUgIAAYpr+sAhDa7QAR0O6Nb6MOy6r24f0netWctnn/yPCoVVMGyt3fN1WTRURERGR+udVhR1ATFQkhlE1HjPZsJqcpvH4Wdhpnh/VqU7XZO7cQZv/vUtcebGHUokvcMepsBMRERExm9cVdm1bJ2CvdACQggq7htDKUsr7N9ZtBczcY0dwvfMiqcUnPJRKfIVbQzFFRERETOd1hV33Dm1xulwAxGMjzHCYnMi/RRh2Vl/fhsjQ2k+3LD1dTNbSZ/hZ7iEPJhNfYajHTkRERMR0XlfYRTeNpFlkBFC120ESGubnKYGGi1cHNqVD8ya1vsZZWclnLz7D1cf2eN8XjzQ4IyQMI6al2TFEREREGj2v/Gwe3yKmep5diubZeczszgZDO9b+Q7lhGGx56zUG7d9CsNvlwWTiK9wJrUHL2IuIiIiYzisLuy7tkim3VwKQrHl2HjGuRSl3D0ip0zVffPAfLtm+jihHuYdSia9xt00zO4KIiIiI4KWFXef2yfDdRuWxlNHUqDA3kJ/pEVLC0tEd63TN4T27iF+3jMTSIg+lEl/k6tDV7AgiIiIigpcWdk0jmhDdNKL6z50oMDGNf4mnjNU3diAgoPZ/9fk52ZS9+Q86F+V4MJn4Inf7LmZHEBERERG8tLADSGjRHLfbDUCaocKuPoQblbw/OpHo8OBaX1Nus3Fw6dP0O3nQg8nEFxlh4bjbtDU7hoiIiIjgxYXd5T06U1ZhB6A1p4k07CYn8m2Bhpsl/SPoEhtZ62ucDgfbli7i6iO7CPh+bKzId1xt0yAg0OwYIiIiIoIXF3Yd2iTSJCwMqNr2IJVCcwP5uLs6OLm+S1ytzzcMg63Ll3HVvk8IdTk9mEx8lbuDhmGKiIiIeAuvLewCAgJoE9+yetsDDce8eCOa2XhwULs6XbM7/UN6bFxNtL3MQ6nE17naa+EUMd/Jkye544476NChA/Hx8fTt25dNmzZVtxuGwdy5c+ncuTMJCQmMGjWK/fv3m5hYRETEM7y2sAO4vHtnysqrhmAmYSXcqDQ5ke/pHFzCP8em1umaI/u+JGb167QpVS+pnJu7owo7MZfVamX48OEYhsG7777L9u3beeyxx4iNja0+5+mnn2bRokXMnz+fjz/+mNjYWMaNG0dJSYmJyUVEROpfkNkBzic1uTVhoVULfQQAXcjnC1qbG8qHtKCctTe2Jyiw9vV7Ue5JipY9z1Wnjnswmfg6V5v2GNEtzI4hjdwzzzxDQkICS5YsqT7Wtm3b6t8bhsHixYuZOXMmY8eOBWDx4sWkpqayfPlypkyZ0tCRRUREPMare+yCggJpFfvDcMweRq7JiXxHmOHgvRFxtGgSUutrKsrK+PKlZxjwrYYpyfm5evU1O4IIa9eupU+fPkyZMoWOHTty5ZVX8sILL1Q/M44ePUpubi5DhgypviY8PJwBAwawfft2s2KLiIh4hFf32EHVcMy3/5tORHgYCdiIM2zkWWq/smNjFGC4eeayUHonNq31NW6Xi20vL+aarM8JNLQCppyfs1d/syOIcOTIEZYuXcqMGTOYOXMmGRkZ3HfffQBMmzaN3NyqHwb+eGjm938+ceLEOe976NAhz4UW8VK+9HWfYHYAkVqq739Xqannn17l9YVdl/ZJ1atjAvQ0TvKhpaOJibzfHSmV3NQjqdbnG4bBtvfeYcDedMKdmsco52c0icSd2s3sGCK43W4uueQS5syZA0CvXr3IysripZdeYtq0aRd93ws9OOvkgx31dy8RD6rXr3sP0wxZ8RUN/e/Kq4diAgQGBpKa3AqXywVAd3IJNlwmp/JegyNLePSaDnW6JmPTJ6R9uoLmdpuHUok/cXa/TPvXiVeIj48nLS2txrFOnTqRnZ1d3Q6Qn59f45z8/Hzi4mq//YuIiIgv8PrCDuDqn/Wiwu4AIBQXXcgzOZF36hBo450bO9XpmuMHM2ny/quklGg7CakdV69+ZkcQAaBfv34cPny4xrHDhw+TlFQ1YiElJYX4+HjS09Or2ysqKti6dSt9+2qeqIiI+BefKOxim0eT0LJ59Z8vMc49N6KxiqactTe2JaQOK2AWFxaQ+/pz9C444rlg4lcMiwVnT30gFu8wY8YMPvvsMxYsWEBWVhbvv/8+L7zwAlOnTgXAYrEwffp0nn76aVatWsVXX33FjBkziIiIYMKECSanFxERqV9eP8fue317dmZV+laahIeSgI0kw8pxS7TZsbxCiOHk3WGxJESG1vqayooK9rzwNMOz93kwmfgbd/su0FT/7sQ7XHrppSxbtoxHHnmExx9/nDZt2vDAAw9UF3YAd955J+Xl5dx7771YrVb69OnDihUriIqKMjG5iIhI/fOZwu6StI78b8vO6j/3M46rsAMshsH83kFc3qZZra9xu91seXUJQw7vINBwezCd+Btn/2vNjiBSw/Dhwxk+fPg52y0WC7NmzWLWrFkNmEpERKTh+cRQTKja065rhxQqHU4A2lNEvKF1kX7VqoIpl9Zt0/btK5fTb9dHNHHaPZRK/JERFIyj/zVmxxARERGRs/CZwg5gWP8+uN0/9DD1NbJNTGO+/k1KWDiibls/fLVtCx0+fo+WFac9lEr8latnX4isfc+wiIiIiDQcnyrsIsLD6NIuGed3Wx+kkU+MUW5yKnMkB9hYcWPd9sb4NuswActfov3pXA+lEn/muPLcw91ERERExFw+VdgBjLjiMhzfDccMAC43jpsbyARNjQrWjk0hPLj2e4mVFBVx7JXn6JOf5cFk4q+MyKa4evU3O4aIiIiInIPPFXbRTSNp1yaxekhmD3KJMhrPXLEgw8Ubg2NIig6r9TWOSjs7X3qGQdkZWDyYTfyXo+8QCPKZtZZEREREGh2fK+wArrvyMsrtlQAEYnCVccTcQA3FMHikGwxq1/zC537H7Xaz5Y2XuTpzK8FulwfDiT9zXjnC7AgiIiIich4+WdjFt2hOUkIshmEA0J1cYg2byak8b2J8GTP6Jtfpms/XreJnn/+XSEeFh1KJv3MldcDdvrPZMURERETkPHyysAMYM7AfZRVVQzAtwGDjG3MDedilYSU8P7JuK2Bm7txBmw/eIb7M6qFU0hg4RtxkdgQRERERuQCfLexaxbUkNbk1ru9WyGxHEW2NIpNTeUYrSykrb+xIQEDt/7ryjh3F+c6LdLKe8GAy8XfumJY4+2nvOhERERFv57OFHcANQ67A4fxh3thgIwvLd8Mz/UWEYWfV9W2ICq39whWlp4v5+uVnuCz3kAeTSWPgGHqjFk0RERER8QE+Xdg1i4ygV1oHKh0OAOIopRv+s0dboOHi5aua0rF5k1pf46ys5LOlzzHo6B7f/ssV0xmh4TiuHmN2DBERERGpBZ//7H/dlZcTYPnhbVxtfEOo4TAxUf25P83N8NSWtT7fMAy2vPU6A7/aRIjb6cFk0hg4Bl0HEVFmxxARERGRWvD5wi4sNIQrL+lORUXV9gcROPxiIZWxzW3ce0XbOl3zxQf/off2tTStLPdMKGk0jIAAHMMmmB1DRERERGrJ5ws7gIF9ehARHla9/UFPTpJk+O5KkN1DSnh5TGqdrvl6727i1i2jVal/LiAjDcv5s0EYsYlmxxARERGRWvKLwi4wMJDxQ6+i/LteOwswwjhEoOE2N9hFiKOM1Td2ILAOK2AWnviW0jf/QZeiHA8mk8bCCAigctyvzI4hIiIiInXgF4UdQIekVnRtn4zDWTW3rDnlDDCOmZyqbsKNSt4flUhMeHCtr6koLWX/i0/T70SmB5NJY+K8aiRGqxSzY4iIiIhIHfhNYQcw7pora/R09eU4LY1SExPVXoDh5vl+EXSNi6z1NS6nk61Ln2PwkS8IwL+2eRBzuIOCqRw3xewYIiIiIlJHflXYhYWGMPLKyykrtwMQiMEoI9MnhmTObO9gXNe4Wp9vGAZb/7WMq778lFCXf6wCKuZzDP85RkztV2IVEREREe/gV4UdwCWdO5KUEIvLXVXMJWBjoJevkjmsWQl/ubp9na7Zk/4R3TatItruGz2S4v1c4ZE4Rt9qdgwRERERuQh+V9hZLBZuHnE1Tqer+thl5NDOOGViqnPrFFTCm2M71emaI/v30WzN6yTZCj2UShoj59jboEnthwKLiIiIiPfwu8IOoFlkBGMG9aesompIpgUYZWQSYVSaG+z/05xy1o1vT1Bg7f8arHm5nHrjeXoU+tbCMBfj+ax8Lkk/QMzavcSs3csVnx5k7cniGucctFUwYcc3tFi7l6g1e7hsQyb7SyrOe99PCmxcviGTiNV7SP3gK5Z8U1Cj/c3jp2j73320XJfBH7+sudJoTnklHf63j9wK/xr+6oyJxXHtOLNjiIiIiMhF8svCDqBP11Q6t03C7qj6AB6Bg1FGJhjeschIqOFg+Yg4WjYJqfU19vJy9r74NFd8u9+DybxHm/AQHu3ais8GpbF9UCcGt4xi/I5v2FtctQH7N6V2Bm48RNsmIXxwRUf2DO7Mw10SiQw695f1N6V2xmzLon/zCD6/Oo37UuO5MyObFd9W7XtYYHcybfdxHuveiv/078Cbx4tY86Ni8v/2ZjM7LYH4sNqvXOoLnLfMgODafy2KiIiIiHfx28IO4OfDBhIeElK9cXk7iricbJNTgcVws7BPKJcmNq31NW6Xi62vLGZw1uc+sRhMfbg+sRkj45vSMTKUTpFh/K1rIlFBgWwrqppX+Of9Jxga15QF3VtzaXQT2keEcl18U5LCz12gLDlSSKuwIJ7u2YYuUWFMbduCyUnNeeJwHgBZZXaaBQdyU+sYLotpwtUtIzlQUtXzu+JbK8VON1OSm3v+zTegyq59cPYdbHYMEREREfkJ/LqwCwkO5hejrqXC/sMQzEHGNyQbVhNTwW+TK7m1V2KtzzcMg23/fpcBez4m3Oldw0kbissweCe7CJvTTf/mEbgNgzUnT9MlMpTrtn5Nwn8y6PdJJu/mFJ33PtuKShkaW7OgHhbXlJ3WMhxug9SI/9fencdHVd/7H3/NPpM9ZAezQAiEAIUABooIAioqsrrgcq8K6kWkaOtS0NJL34TP4AAAIABJREFUtX0YUItFRasiVW+Vn9biFWyv1tZUQJZQUfZNkU0gCQkJWSeZmfP7Izo1ArJNMpnk/Xw88gj5nu855z3DzCP5zPec79dBjdfHZ+U1lNV7+Fd5Db2jnVQ0eJm59RC/75OKyWRqzofaorwWKw2T7w92DBERERE5T226sAPolBTP8Av7UlvXWBCZgfHGNmKM2qDkGRZRydxLM89qn62rV9Ltn0vpUFfVTKlar83Ha4l+bxNhyzdy98YDvJ2XQe8oF8VuD1VeH3N3F3NZQiTvD85kUqdY/vPTfSfch/ddRXUeEp3WJm2JDiseA47We4i1W/lDbhqTN+znxyt28R+psYxKjGLW1kNMTutASb2Hgf/cSa9/bD/h3rxQ1DDuFozEjsGOIXLe5s+fT0xMDA8++KC/zTAM8vPzyc7OJjk5mdGjR7N9e/u4lF1ERNof6+m7hL5LLuzDlwcO8XVJKXabFRcerjG28j/0pd7Uck9BZ0sVf5p4djNgHty9C8fSxWRUljRTqtate4SDTy/pToXHy58PlTPls/3846KudLA1/r+NTY7iZ10b1//rGx3Gp+U1PPfVUUYnR5/zOcd3jGF8xxj/z6tKq1h3rJonemWR848d/KFfGjmRTnILdjA4LpzeUa7ze5BBUpecikfLG0gbsH79el555RV69uzZpH3BggUsXLiQhQsXkpWVxeOPP86ECRNYv349kZGRQUorIiLSPNr8iB00LoHwn2Muw+Ww4/tmfbt4ahhr7MDUQpOpRFPHX8ZnYD+LGTAryko5/Ooz5B7d23zBWjm72UzXCAf9Y8J4LKcjfaJcLPiyhHiHBasJekQ6m/TPjnRyoPbUM1YmOa0U13matBW7PVhNEG8/sch3e31M33iQ5/qksqe6nnqfj5EJkaQ4bQyLj+Djo6E5iuozmfDd/d9gaRef7UgbVlFRwZ133smzzz5LTMy/P5AxDIPnn3+en/70p4wbN46cnByef/55qqqqePvtt4OYWEREpHm0i8IOwGG3cefEq/B4fP7JVDIpY1gLLF5uNzy8OTKOjlGOM96nvq6Oz1/4HRcf3NqMyUKPD3D7DOxmMwNiwthV5W6yfXeVmzTXqWesHBQbzt9LKpu0/b2kkv4xYdjMJ947l7+7iOHxEQzqEI4PA893Pgeo9xl4W8ksq2er7vLr8KVnBTuGyHn7tnAbOnRok/Z9+/ZRVFTEiBEj/G0ul4vBgwezbt26lo4pIiLS7NrVx/Wx0ZHcPHokry3/Gy5nY5E1kIMcNcLYYkpulnOaDIO5fawMSos5fedv+Hw+Vr/2EiO+KMTaTmbAPJmHth7iquQoUl02Kj0+lhw8xsdHq1g2qAsAD2YlcsP6fQyJC2d4fCT/PFrFm18f4895nf3HuO3TfQC80j8dgKkZcTz31VHu23yQOzPiWV1Wzav7y3h9QPoJ5992vI4lB4/xr2HdAege4cRqghe+OkpOlJOPSqr4Rbfmed00p5rETviuuyPYMUTO26uvvsqePXt48cUXT9hWVFQEQEJCQpP2hIQEDh8+/IPH3b17d+BCioSIUHrdh95vXmmvAv2+ysr64Q/l21VhB9A1rSOX/7g/f1uzAZezcVr8K41duLGy2xQf8PPdklLLlP5nNzJSuHwpAzd8SJjHffrObViRu4FbP93HEbeHaKuF3lFO3vtxF0YlNs5qOS4lht/39TJ3VzE/2/w1WREOXumX3uT+uv21TWcR7RzuYPmgLjyw5Wt+v7eUjk4bv+vdiYkdmxbehmFw18YDPNmrE5E2CwAui5lX+qVzz6aDVHi8PNQtiQGxYc38LARWg8WG7/55WrNOQt7u3bt59NFHef/997HZAruu5Ol+cZ6VDwsDdyyRZhTQ130zqzx9F5FWoaXfV+2usAMY0q83h48eY8sXe3E6bJiBscZ23qYX+0yxATvPwLBKFlyZfVb7bC9cS+e//4mE2uMByxGqFvc7cRTt+25Ni+PWtLhTbv9oyIlvqGHxEay/pPsPHtdkMrHi4hP3vSIpil2X5Zw2V2tV8x8zsCRfEOwYIuetsLCQ0tJSBg0a5G/zer2sXr2axYsXs3btWgBKSkpITU319ykpKSExMbHF84qIiDS3dnOP3fddc+kQUpMTcDc0TrRhxWCisZUUIzAFVaq5incmnl2VfnjvHkx/eonM40UBySDyXWW9B2EZMTbYMUQCYvTo0axevZqVK1f6v3Jzc7nmmmtYuXIlXbt2JSkpiYKCAv8+dXV1rFmzhoEDBwYxuYiISPNolyN2AGazmdvGXs6Lf/4LJccqsNus2PFxvbGFN/gRJaaIcz52pOHmvXFphH1zCd+ZqCovZ9/ipxlZ/OU5n1fkVKoiO2Cf8atgxxAJmJiYmCazYAKEhYURGxtLTk7jqPq0adOYP38+WVlZdO3alSeffJLw8HCuvfbaYEQWERFpVu12xA7AarVwx8QriY4Iw+PxAuDEw/XGZmLPcQFzq+Hlf4bHkB5z5mubNdS7+ddLTzPswGZOnJdR5Px4zBaMB+aBw3n6ziJtyL333su0adN48MEHGT58OEeOHGHp0qVaw05ERNokU3l5eWjO1x5AtW43C5cso67ejcXSOMpWiZ03Tb0pNYWf1bF+3cPLjEFpZ9zfMAxW/OEFhq3+XyIa6s7qXCJn4tikadiumhTsGCLtUs/nNHmKhIatd+cFO8IZq/zZjcGOIHJGIp9a0qLna9cjdt9yORzcdf3VWMwW/wLmkdRzk7GRJOPM5166Lr76rIo6gH/933sMWP++ijppFsV9h6ioExEREWkHVNh9IyLMxbTrx4DJhPeb4i4MDzcam7jAqDjt/rmOSl4Y3fWszrnrs3/R6YMlJNWUn1NmkR9SlJRO2L2PBjuGiIiIiLQAFXbfERMVwU9uGIfdasXjbbznzoGX643NdDHKTrlfsqmaZdd0xWw+86ezeP8+6v/fC3QrP3TeuUW+ryw8Bscvn4azeE2KiIiISOjSX33fEx0Rzk9uHE+400mDxwOADR8Tja10N0pO6B9m1LN8zAVEOs58gtGayuN88YdnyDsS2NXoRQCqbU58Dz2FNTL69J1FREREpE1QYXcSYU4H028YR4eoSOobGos7CwbjjO1caBz097MYXl4eEkFWXNgZH9vT0EDhS89yyd7P9eRLwDWYLVRM+2+cqZ2DHUVEREREWpBqi1Nw2G1Mve5qkuM74K5vXMTcBIww9jDKtwuz4ePn3Xxc2S3hjI9pGAarl7zKxdtXYfd5mim5tFc+4NCEO4nqPzjYUURERESkhamw+wE2q5U7JlxB507J1Lnr/e3Zdft5qvNRZg7JOKvjbfj7B/Rd+1ei62sCnFTaOx+wZ/hE4sbeEOwoIiIiIhIEKuxOw2KxcMuYy8jrlU1NnRt3QwOZaR255ZK+Z3WcPZs3Ev+XP9Kx+tSTsIicCwPYNuAykm+7J9hRRERERCRIznzGj3bMZDIxeuhAEjpEs37LTm66cgQmk+mM9y89cpjjrz/PkGMHT99Z5CwYwOc9LyJz+kPBjiIiIiIiQaTC7izk9comr1f2We1TV13N9kULuPzwzmZKJe3Z59mDyLz/0bNaakNERERE2h79NdiMvB4Pa15+lkv2fIoZI9hxpI35vHseXR78DWaLJdhRRERERCTIVNg1E8MwWPOnJQzZsgKntyHYcaSN+TxrAJ1//hgWqwbdRURERESFXbPZ+PFH5Kx6l1h3dbCjSBviNZko7DGYjJn5KupERERExE9/GTaD/Tu2E7XsVdKqjgY7irQh9WYrq3sNpe89D2G12YIdR0RERERaERV2AVZeUkzJawsZVro/2FGkDam2OlideykX/te9KupERERE5AQq7ALIXVvLppcWMOrQtmBHkTakzBHOZ8MmMvCG2zRRioiIiIiclAq7APF5vax+5fdc+uV6LIYv2HGkjTgUHsueq28j78qxZ7V2ooiIiIi0L5o8JUBWv/e/lFZVcygqMdhRpI3YHtuJw7fcT9+rxqmoExEREZEfpBG7ANiyZhX7d+3A4QxjVZd+lIVF0//gNq1dJ+ekwWzhk445JN4yne7ds4MdR0RERERCgAq7ADi05wvMpn8Pfm5LzuRYWBQX7/kUl6c+iMkk1JQ7wvmocz/63XoX8R07BTuOiIiIiIQIXYoZAJfeeAtp2T2or6v1tx2OSmBZz+EcjNalmXJmvohO5qMLr2LoPbNU1ImIiIjIWVFhFwBms5nBV48nd/hl1NfVYhiNl2DW2Rz8I2sQ61J74THpqZaT85rMrErJZv/Vt3Dp7dNwhocHO5KIiIiIhBhdihlA2QPySOh0ASuWvoW7rg6b3Q7AjqQuHImKZ+ieT4mtrQxySmlNypyRrEjtRbdrG0d9RURERETOhYaRAiwupSNj/ms6nTKzcNfW+NvLXVG812Mo2xM7BzGdtBZek4lPEzP5IHcUg2bMVFEnIiIiIudFhV0zsNpsXDz+Gn48ejyehnp8Xi8APrOFwrTefJg1iEp7WJBTSrAcDYtmaZeBlA4bw+g7pxERHRPsSCIhaf78+QwfPpzU1FQyMzOZNGkS27Zta9LHMAzy8/PJzs4mOTmZ0aNHs3379iAlFhERaT4q7JpR5569GPtfPyE8JoZ6d52//VB0Iu/2Gs7GlG54de9du+E1mSlM7sZfc4bS++YpDL56PGaLJdixRELWqlWruP322/nggw9YtmwZVquV8ePHc+zYMX+fBQsWsHDhQubNm8dHH31EQkICEyZMoLJSl8WLiEjbYiovL9dia83MMAw2rihge+EabHYHJvO/i7mouioG7dtESuXRICaU5lYUHsvHKdlE981j4KjRWL+5/1JEAqeqqoq0tDRef/11rrzySgzDIDs7mzvvvJMHHngAgNraWrKysvj1r3/N5MmTmz1Tz+cKm/0cIoGw9e68YEc4Y5U/uzHYEUTOSORTS1r0fBouagEmk4m+w0Zw1ZSphEVFUV/772URjjsj+Fv3wazo3I9aqyOIKaU5VNrD+CitDx/0HEb/W6Zy0ZgJKupEmklVVRU+n4+YmMbLm/ft20dRUREjRozw93G5XAwePJh169YFK6aIiEiz0KyYLSg6Lp4rb7uTLzZ+xucff4TP68VqswHwVdwFHIxOIvfQDrqV7MViaCA1lNVbrGxMzmJjVAqd++Qy9tIr/P/XItI8Zs2aRe/evcnLaxx5KCoqAiAhIaFJv4SEBA4fPnzK4+zevbv5Qoq0UqH0uk8OdgCRMxTo91VWVtYPbldh18JMJhNZffuR3iOHwg/+j/3bt2J3uTCZTDRYbRSm9WZrUiZ9D+2kS+lBzKjACyU+k4ld8ems65COMymFkaNGk3BBarBjibR5Dz/8MGvXruX999/Hcp73rp7uF+dZ+VCXYkpoCOjrvpnpDlkJFS39vlJhFyR2h5MhYydwdEAea957l6qKcuxOJwDVjjA+6ZzL5uSu9D20k4xjhzAFOa+c3oHoJNYkdqUuJp6+Q4fTpXcfTCb9z4k0t4ceeoilS5eyfPlyMjIy/O1JSUkAlJSUkJr67w9YSkpKSExMbOmYIiIizUr32AVZfMdOjL7jLvoNv7Rx1O47s2ced0WyInMAy3OGcSA6KYgp5VQMYF9MMu92u4i/ZfSj48UjGXfXDDJ/1FdFnUgLmDlzJn/+859ZtmwZ3bp1a7ItPT2dpKQkCgoK/G11dXWsWbOGgQMHtnRUERGRZqURu1bAbDbTfUAeWbn92bZuDTv+tQ5PfT02R+NkKsfCovkoayAJVWX0OvIFF5QfUUUeZD6Tib2xHdmUlEmx2UFa92zGX34VDpcr2NFE2o0HHniAN998kz/+8Y/ExMT476kLDw8nIiICk8nEtGnTmD9/PllZWXTt2pUnn3yS8PBwrr322iCnFxERCSwtd9AKeRoa2PLJSnZ99ik+nwebvelsmRHuarKLvyLr6H7sXk+QUrZPDWYLu+PT2ZKQTjlWklLTGHD5lUTHxQc7mki78+3sl983c+ZMHnroIaBxuZm5c+fyyiuvUF5eTv/+/XnyySfJyclpkYxa7kBChZY7EAm8ll7uQIVdK9ZQ72bjin/y5ebP8Xm92B3OJtutXg+ZpQfoUfwV0XVVQUrZPpQ7I9kdn8bO2I7UGJCS0YX+I0cR1aFDsKOJSCumwk5ChQo7kcBr6cJOl2K2Yja7gwGXjuJHFw9je+Fa9mzeSG1VJXZn4yyaHouVnYmd2ZmQQcfjJWQXf0Wn48WYtVRCQNSbrezt0JHd8WkctkdgMkGnzG7kDh9JeFR0sOOJiIiIiPipsAsBdoeTPhdfQu+LhnJg1w62rf2EsqIi7E4nZrMZTCYORSdyKDoRR4ObjGOH6Fz2NYlVZZpN8xwURXTgi/g0vopJoaahAbvTRVbP3vS+aKh/5lIRERERkdZEhV0IMZvNpGfnkJ6dQ3lJMRtXFHB471dgGP6JVtw2R+MoXmJnwt01dC77ms5lX9Oh9niQ07deBlAaFs2BmGT2xnakzOLA5/PSIT6R3AF5pHbLxnye62KJiIiIiDQnFXYhKiYhkWHXTMJdW8uuTwvZv3MHFUeLsdjsWG02oHE9vC0pWWxJySKm9jhpxw7TqaKY+Orydr/wuddk5nBkPAdjkjgQk0y1zYm7toawiGi6ds8mZ+BgXBER53WORYsW8fTTT1NUVER2djb5+fkMHjz4lP1XrVrFL37xC3bs2EFycjL33nsvU6ZMOa8MIiIiItI+qLALcQ6Xi95DhtF7yDCqK8rZvn4dX3+5m6pjxxov1fxmpKncFUW5K4pNHbtj8zSQUllCx+MldKwoIbK+JsiPomVU25wciYznQEwyX0cn0mC24K6twWq2kpCcQs7AwSSlpQdk/bmlS5cya9Ysfvvb3zJo0CAWLVrEddddx9q1a5sslPytvXv3cv3113PzzTfz4osvsnbtWu6//37i4uIYN27ceecRERERkbZNs2K2QYZhcKy4iB3r13Jk315qKo9jszv8I3nfF1lXRcfjJSRVlhJXU06UO/QLPR+NxWxxRAf/V7UjDJ/XS31dHc7wcOJSOpLZJ5eOnTOxWAP7GcfIkSPp2bMnTz/9tL+tX79+jBs3jjlz5pzQf86cOSxfvpwNGzb422bMmMGOHTv48MMPA5pNRFqOZsWUUKFZMUUCT7NiynkzmUx0SEpm8NXjMQyDiqMl7Nm8keID+6goPUpDfQNOlwuTuXGZ80pnBDudEexM7AyAzdNAXE05cTUVxFU3fo90V7faiVh8JhOVjnAqnBEc+6aYK4mIpcHSWMg21Lvxer1EOhwkXJBKVu4A4pJTAjIydzL19fV8/vnnzJgxo0n7iBEjWLdu3Un3KSwsZMSIEU3aRo4cyZIlS2hoaMB2iqJcRERERARU2LV5JpOJmIRE+o24DGhc/Lxo31d8tXUzZUcOU1VejmEYOL5T6DVYbRyJSuBIVIL/ODZPA7G1x4lyVxPpribSXUOEu5rw+lpcDe5mL/p8mHBb7VTbXVS4IqhwRlDhjKTCGcFxRzjGN9kNw6C+thbqPbginUR3iCcpLYP0Hj2JOMVixoFWWlqK1+slISGhSXtCQgLFxcUn3ae4uJhLLrnkhP4ej4fS0lKSk5ObK66IiIiItAEq7NoZq81Gp67d6NS1GwC1VVUU7d/LoT1fUHmsjMpjx6ivrcVn+HC6wpoUe8WRcRRHxp1wTLPPR1hDLWH1ddi9Ddi8Hv93m9eDzefx/xsMDJMZn8mEYTLhw+T/2Wcy0WCxUWe1U2dzUGd1UGtzUGe147ba4XsjbIZh0OB2462pwWq3ERETS1SHeC7I6kZKRpfznvxERERERCRUqLBr51wREWTk9CIjpxfQWCzVVlVSfHA/h/Z8SWVZGbVVldRVV+P1eDAwsNnsWGw2/6WMPrOZKkc4VY7wZsno83qpr228789iseIMjyA8KgpXZBRxKSkkdEolJj4Bq93eLOc/W3FxcVgsFkpKSpq0l5SUkJiYeNJ9EhMTT9rfarUSF3diMS0iIiIi8l0q7KQJk8lEWGQUGT16kdGjl7/d5/VSffw4FaUlHCs6wvGyUtw1Nbhraqh31+FpaMDn8eD1ejEMH4ZhYPgMfD4fYDQuFmcCMH1TEBqN300mwITZbMZsNmNzOLA5ndgdDmyOxu/O8AjiUjrRITmZiOiYgE90Emh2u52+fftSUFDA+PHj/e0FBQWMHTv2pPvk5eXx3nvvNWkrKCggNzdX99eJiIiIyGm17r+QpdUwWyxExsYSGRvLBd9cxnkyPp8PT309DfVuPA0NeOrrqXe7MVvMWMwWTGZz45epsZhr/LcZh8uFzeFotglNWtr06dOZOnUq/fv3Z+DAgSxevJgjR44wefJkAKZOnQrACy+8AMDkyZN56aWXmDVrFpMnT2bdunW88cYbLFq0KGiPQURERERChwo7CSiz2Yzd6cTudAY7SlBNnDiRsrIynnjiCYqKiujRowdvvfUWaWlpABw8eLBJ/4yMDN566y0efvhhFi9eTHJyMvPmzdMadiIiIiJyRrSOnYiISDPROnYSKrSOnUjgtfQ6duYWPZuIiIiIiIgEnAo7ERERERGREKfCTkREREREJMSpsBMREREREQlxKuxERERERERCnAo7ERERERGREKfCTkREREREJMSpsBMREREREQlxKuxERERERERCnAo7ERFp8xYtWsSPfvQjkpKSGDZsGKtXrw52JBERkYBSYSciIm3a0qVLmTVrFvfffz8rVqwgLy+P6667jgMHDgQ7moiISMCosBMRkTZt4cKF3HTTTdx66610796dJ554gqSkJBYvXhzsaCIiIgFjDXYAERGR5lJfX8/nn3/OjBkzmrSPGDGCdevWNfv5t96d1+znEGlvIp9aEuwIIq2SRuxERKTNKi0txev1kpCQ0KQ9ISGB4uLiIKUSEREJPBV2IiIiIiIiIU6FnYiItFlxcXFYLBZKSkqatJeUlJCYmBikVCIiIoGnwk5ERNosu91O3759KSgoaNJeUFDAwIEDg5RKREQk8DR5ioiItGnTp09n6tSp9O/fn4EDB7J48WKOHDnC5MmTgx1NREQkYDRiJyIibdrEiRPJz8/niSee4OKLL2bt2rW89dZbpKWlBTuanMLZLii/atUqhg0bRlJSEn369NFSFiLf8cknn3DDDTfQo0cPYmJieP3110+7z9atW7nqqqtITk6mR48ezJs3D8MwWiCtnA8VdiIi0ubdcccdbN68meLiYj7++GMuuuiiYEeSUzjbBeX37t3L9ddfT15eHitWrOC+++7j5z//Oe+++24LJxdpnaqrq8nJyWHu3Lm4XK7T9j9+/DgTJkwgMTGRjz76iLlz5/LMM8/w7LPPtkBaOR+m8vJyld8iIiLSKowcOZKePXvy9NNP+9v69evHuHHjmDNnzgn958yZw/Lly9mwYYO/bcaMGezYsYMPP/ywRTKLhIpOnTrx+OOPc/PNN5+yz8svv8yvfvUrdu3a5S8En3jiCRYvXsy2bdswmUwtFVfOkkbsREREpFX4dkH5ESNGNGn/oQXlCwsLT+g/cuRIPvvsMxoaGpotq0hbVVhYyI9//OMmo3sjR47k8OHD7Nu3L4jJ5HRU2ImIiEircC4LyhcXF5+0v8fjobS0tNmyirRVp3pPfbtNWi8VdiIiIiIiIiFOhZ2IiIi0CueyoHxiYuJJ+1utVuLi4potq0hbdar31LfbpPVSYSciIiKtwrksKJ+Xl3fS/rm5udhstmbLKtJW5eXlsWbNGurq6vxtBQUFpKSkkJ6eHsRkcjoq7ERERKTVmD59Om+88QavvfYaO3fuZObMmU0WlJ86dSpTp0719588eTKHDx9m1qxZ7Ny5k9dee4033niDn/zkJ8F6CCKtSlVVFZs2bWLTpk34fD4OHjzIpk2b/EuIPPLII4wdO9bf/9prr8XlcnH33Xezbds2li1bxu9+9zvuvvtuzYjZymm5AxEREWlVFi1axIIFCygqKqJHjx489thj/rUHR48eDcBf/vIXf/9Vq1bx8MMPs2PHDpKTk/npT3/KlClTgpJdpLVZuXIlY8aMOaH9xhtv5Pnnn2fatGmsWrWKzZs3+7dt3bqVBx54gA0bNhATE8PkyZOZOXOmCrtWToWdiIiIiIhIiNOlmCIiIiIiIiFOhZ2IiIiIiEiIU2EnIiIiIiIS4lTYiYiIiIiIhDgVdiIiIiIiIiFOhZ2IiIiIiEiIU2EnIiIiIiIS4lTYiYiIiLRT69atIz8/n/Ly8mBHOa2amhry8/NZuXJlsKOItEoq7ERERETaqcLCQubNm0dFRUWwo5xWbW0t8+bNY9WqVcGOItIqqbATERERkYCprq4OdgSRdkmFnYiIiEg7lJ+fzy9/+UsA+vTpQ0xMDDExMaxcuZK//vWvTJo0iZycHBITE+nVqxe//OUvqaura3KMadOmkZSUxL59+7jhhhtITU3l+uuvB8Dn85Gfn092djYpKSlcffXVbN++nd69ezNt2rQmx6moqODhhx+mV69eJCQk0KdPHx5//HG8Xi8A+/btIzMzE4B58+b5s37/OCLtmTXYAURERESk5Y0ZM4Yvv/ySt99+m8cee4y4uDgAunfvzs9+9jMcDgdTp04lKiqK9evX89xzz/H111+zePHiJsfx+XxMnDiR/v378+ijj2KxWAB45JFHWLBgAaNGjeLSSy9l69atXHPNNbjd7ib719bWMmbMGPbv38+UKVNIS0vj008/Ze7cuRw4cIBnnnmG+Ph45s+fz3333cfVV1/NmDFjAOjcuXMLPFMioUGFnYiIiEg71KtXL/r06cPbb7/N6NGjSU9P92976aWXCAsL8/88efJkMjMz+c1vfsOjjz7KBRdc4N/W0NDAqFGjeOyxx/xtxcXFLFy4kCuuuIIlS5ZgMpkAmDt3LnPnzm2S47nnnmP37t18/PHHdOvWDYB9b7JzAAADlklEQVTbbruN9PR0fvOb33DPPfeQlZXFuHHjuO++++jZsyeTJk1qludEJJTpUkwRERERaeLbos7n81FRUUFpaSmDBg3CMAw2btx4Qv877rijyc8ff/wxHo+H22+/3V/UAUydOvWEfd955x0GDRpEXFwcpaWl/q9LLrkEQJOliJwhjdiJiIiISBPbtm1jzpw5rFq1itra2ibbjh8/3uRns9lMWlpak7YDBw4A0KVLlybtsbGxxMTENGn78ssv2bJli/8euu8rKSk5p8cg0t6osBMRERERv4qKCsaMGUNYWBizZ8+mS5cuuFwuDh06xN13343P52vS32azYbWe+5+UPp+PoUOHct999510e0ZGxjkfW6Q9UWEnIiIiIn4rV66ktLSUV199lSFDhvjbCwoKzvgYqampAOzZs6fJSFxZWdkJi6F37tyZqqoq/6WXp/LdSzpF5ES6x05ERESknQoPDwdoUmx9O6ulYRj+Np/Px8KFC8/4uMOGDcNqtfLyyy83aX/xxRdP6DthwgQ2bNjA3/72txO2VVZW+mfRdLlcJ2QVkX/TiJ2IiIhIO5WbmwvAo48+yrXXXovdbmfo0KF06NCBadOmMXXqVKxWK8uWLaOqquqMj5uYmMhdd93Fs88+y6RJk7jsssvYsmULH374IXFxcU1G3+655x7ef/99brrpJm688Ub69u1LbW0t27dv59133+WTTz4hPT0dl8tFjx49WLp0KV27dqVDhw6kp6czYMCAgD8vIqFIhZ2IiIhIO5Wbm8ucOXN4+eWXmT59Oj6fj+XLl/PWW28xe/Zs8vPzCQ8PZ+zYsUyZMoWLLrrojI/9yCOP4HK5eO2111ixYgUXXngh77zzDldccQVOp9Pfz+Vy8d577/HUU0/xzjvv8OabbxIREUFmZiYPPvggSUlJ/r7PPPMMM2fOZPbs2bjdbm688UYVdiLfMJWXlxun7yYiIiIicn7Ky8vJyMhg9uzZPPDAA8GOI9Km6B47EREREQm47y+TAPD8888DNJmURUQCQ5diioiIiEjALV26lDfeeIPLL7+c8PBw1q5dy9tvv82IESMYNGhQsOOJtDkq7EREREQk4Hr27InVamXBggVUVlb6J1SZPXt2sKOJtEm6x05ERERERCTE6R47ERERERGREKfCTkREREREJMSpsBMREREREQlxKuxERERERERCnAo7ERERERGREKfCTkREREREJMT9f8foiIxh+XFFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTujSBlxEZxV"
      },
      "source": [
        "In our train set we have 160 samples with a value 1 for the target variable and 90 with a value 0. Once we have our baseline score we will try a range of difference techniques to tackle this issue (commonly referred to as *imbalanced classiication*). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "qlXeoeAVmxEb",
        "outputId": "eacf60ea-4f99-485c-e349-005e390253c8"
      },
      "source": [
        "corrs = (\n",
        "    train_df.corr()[\"target\"].sort_values(ascending=False).to_frame().drop([\"target\"])\n",
        ")\n",
        "corrs.query(\"target > 0.1\").sort_values(by=\"target\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.373608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.293846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.173096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>0.164146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0.159442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>0.142238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.132705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>0.127213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>0.124792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>0.124151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.118379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>0.113909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>0.113660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.110998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.110589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.108966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0.108147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.107828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       target\n",
              "33   0.373608\n",
              "65   0.293846\n",
              "24   0.173096\n",
              "183  0.164146\n",
              "199  0.159442\n",
              "201  0.142238\n",
              "30   0.132705\n",
              "289  0.127213\n",
              "114  0.124792\n",
              "164  0.124151\n",
              "101  0.118379\n",
              "272  0.113909\n",
              "226  0.113660\n",
              "17   0.110998\n",
              "105  0.110589\n",
              "0    0.108966\n",
              "244  0.108147\n",
              "13   0.107828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfeBMSRkAIJW"
      },
      "source": [
        "We can see that features `33` and `65` despite of not being strongly correlated with the target variable (for that the correlation coefficient would have to be > 0.5) are correlated with the target the strongest. We will pay an extra attention to these features in what follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLaf0PMpmxqN"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1_73bUpaWUb"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WjLbr4Q-IMn"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R6-jI0n8LOh"
      },
      "source": [
        "In what follows we will start with the simpliest tool (which we will prove to be most effective nonetheless) in our arsenal suited for the type of problem we are facing with: logistic regression. The only motivation for the selection of algorithms to follow (Naive Bayes, K-Nearest Neighbors, Support Vector Machine, Random Forrest Classifier) is that they are all algorithms designed for classification problems. \n",
        "\n",
        "However, there is a way to justify the usage of multiple algorithms that differ in complexity: we expect the variety of different models that we will be using to provide support to our initial hypothesis that simplier models will work better in our case (small training set, huge testing set) as the more complex models better adapt to the patterns in the data and tend to overfit as a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db9kuN5MadwH",
        "outputId": "b1bffc91-b4ec-43b3-cb0d-edc8de5e0154"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_logreg = get_score(logreg, X, y)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression is: \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression is:  0.7590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ6lWteV-XN0",
        "outputId": "5953224b-ffc8-4339-fbfa-b917e857627a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)\n",
        "logreg.fit(X_train, y_train)\n",
        "print(\n",
        "    \"Classification report:\\n\", classification_report(logreg.predict(X_valid), y_valid)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.45      0.67      0.54        21\n",
            "         1.0       0.84      0.69      0.76        54\n",
            "\n",
            "    accuracy                           0.68        75\n",
            "   macro avg       0.65      0.68      0.65        75\n",
            "weighted avg       0.73      0.68      0.69        75\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "gjy6Px0yAPf9",
        "outputId": "5d5c1de3-9145-48fd-9d31-14f691258ae4"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(logreg, X_valid, y_valid, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAEfCAYAAADvBmWXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVRU9f8/8OewGSKCyzCIIi64AGIkCYSpCEr1QVAUtywRWwjNpdzARNFUxPyZpkh8MP3kR81MMbcSNTBBAc0lDdRcAZNNlM3AhZnfHx7nGx+WGZa5s/h8nHPPae593/e8Zjo+ffue971XVFxcLAMREamUnroLICJ6ETBsiYgEwLAlIhIAw5aISAAMWyIiATBsiYgEwLAlIhIAw5aISAAG6i5ACFeL1F2BaundvwZp2x7qLkMQ2aUP1V2CILo8vYvbBlbqLkOlhnU1UUm/lp6LFLbJS1yqkveuzwsRtkT0AhFp5j/YGbZEpFtEInVXUCuGLRHpFo5siYgEwJEtEZEAOLIlIhIAR7ZERALgyJaISAAaOrLVzL8CiIgaS09f8VaLuLg4uLu7w9raGtbW1hg2bBgSEhLkx0NCQmBubl5tGzp0qNJlcWRLRLqlkdMIVlZWWLJkCbp37w6pVIrvvvsOEydOxPHjx9GnTx8AgIeHB2JjY+XnGBkZKd0/w5aIdEsjw9bHx6fa6/DwcHzzzTc4c+aMPGxbtGgBiUTSqP45jUBEukVPpHhToKqqCnv27MHDhw/h4uIi35+amgpbW1s4OztjxowZKCwsVLosjmyJSLc0YTVCRkYGvL29UVlZCRMTE2zbtg0ODg4AgKFDh8LX1xc2NjbIzs7GsmXL4Ofnh+PHj6NFixYK+2bYEpFuacJqhB49eiA5ORmlpaXYt28fQkJCcPDgQdjb22P06NHydg4ODnBycoKjoyMSEhLg5+ensG+GLRHpliaMbI2MjNCtWzcAgJOTE86dO4eNGzdiw4YNNdp26NABVlZWuHnzplJ9M2yJSLc04zpbqVSKx48f13qsqKgIubm5Sv9gxrAlIt3SyJFtREQEvL290bFjR5SXl2P37t1ISUnBrl27UF5ejpUrV8LPzw8SiQTZ2dlYunQpxGIxhg8frlT/DFsi0i2NHNnm5+fjww8/REFBAVq3bg0HBwfs3r0bXl5eqKioQGZmJnbu3ImSkhJIJBIMHDgQW7ZsgampqVL9M2yJSLfUcYWYIjExMXUeMzY2Rnx8fGMrAsCwJSJdwxvREBEJQENvRMOwJSLdwpEtEZEAGLZERALgNAIRkQA4siUiEgBHtkREAuDIlohIABzZEhGpnohhS0SkeiIlnsSgDgxbItIpHNkSEQmAYUtEJACGLRGRABi2RERC0MysZdgSkW7hyJaISAAMWyIiATBsiYgEwLAlIhIAryAjIhIAR7ZERAJg2BIRCUEzs5ZhS0S6hSNbIiIBMGyJiATAsCUiEoCmhq1mPhmNiKixREpstYiLi4O7uzusra1hbW2NYcOGISEhQX5cJpMhMjISvXv3hqWlJXx8fHD58mWly2LYEpFOEYlECrfaWFlZYcmSJfj111+RlJSEQYMGYeLEifjjjz8AAOvWrUN0dDSioqKQmJgIsVgMf39/lJWVKVUXw5aIdIqenp7CrTY+Pj4YNmwYunXrBltbW4SHh6NVq1Y4c+YMZDIZYmJiMGvWLIwYMQL29vaIiYlBeXk5du/erVRdnLPVEr9n3MLO/Sn48+ZfuHe/DKHTRuMtz361tl399Y84cPQMQia9ifEjBgpcKSnjytVs/HQ4Hbdv5+JBcTk+eG84Br3eV3584Af/qfU8L89+mPzum8IUqa2aYcq2qqoKP/74Ix4+fAgXFxdkZWUhPz8fnp6e8jbGxsZwd3dHeno6goKCFPap9pHtpk2b0LdvX0gkEgwePBinTp2qt31KSgoGDx4MiUSCl19+GZs3bxaoUvWqqHyMrp0lmD5lOFoYGdbZ7njqH7h8/Q7atzUVsDpqqMrKx+jUUYx33h4GI6OaY54fV4/F+rUz5NunM8cAAFz72wldqtZp7DQCAGRkZKBjx46wsLDAJ598gm3btsHBwQH5+fkAALFYXK29WCxGQUGBUnWpNWzj4+MRGhqK2bNn48SJE3BxccGYMWOQk5NTa/vbt29j7NixcHFxwYkTJ/Dpp59i3rx52Ldvn8CVC8/NuRc+nOgNj9f6QK+OG23kFTzA+s0HET5rLAz09QWukBrC6WVbjA3wgEt/u1r/8Lczawlzs1by7dz5a7C0bAu73jZqqFa7NCVse/TogeTkZPzyyy947733EBISgszMzGapS61hGx0djbfffhuBgYHo1asXvvjiC0gkkjpHq1u2bIGlpSW++OIL9OrVC4GBgZgwYQI2bNggcOWa52lVFZau3YV3Rw9Bl04W6i6HmlFl5WOknc6ExyAndZeiFZoStkZGRujWrRucnJywePFiODo6YuPGjZBIJACAwsLCau0LCwthYaHcnze1he3jx49x4cKFanMgAODp6Yn09PRazzl9+nSN9l5eXjh//jyePHmislq1wZadv8DMtCVGvumq7lKomZ1Ky8DTp1UYOMBR3aVohaaE7f+SSqV4/PgxbGxsIJFIkJSUJD9WWVmJ1NRUuLoq92dObT+QFRUVoaqqqkFzIAUFBfDw8KjR/unTpygqKoKlpWWt5+ndv9YsNWsMmRSih3nyz3X+ai4OJ57B5kV+//dZpU8g+vuezn32LuouQAX0IEP7qgfo8vRutf3PX6ceP42BL1ujb8sS4GmJOkpUkR6q6baRP5BFRETA29sbHTt2lK8ySElJwa5duyASiRASEoI1a9agR48esLW1xerVq2FiYoKAgACl+n8hViNI26rof6q6iPQgM7GUf67zV8+jqKQC/nN2yZtUSaX4Ov4sfkj8E7vj5qur0maXXfpQ3SU0OylEuKffBrcNrOT7ujy9i9sGVsjKzseVrCL4jfGudlwXqOpPZWOvIMvPz8eHH36IgoICtG7dGg4ODti9eze8vLwAADNnzkRFRQXmzp2L4uJiODs7Iz4+Hqamyv0YrbawbdeuHfT19Rs0B2JhYVFrewMDA7Rr105ltWo6f4/eGOQ5uNq+uZ9vgdfrL2P40FfVVBU1h6Tj5yEWm6OPfRd1l6I1Ghu2MTExCvsNCwtDWFhYo/pXW9gaGRnByckJSUlJGDlypHx/UlIS/Pz8aj3HxcUFBw8erLYvKSkJr7zyCgwN614OpQv+rniEv/KKAABSqQz594px7dZdtG7VEh1aG8OsraRaewN9fbQ1b4XOHcW1dUdqVln5GPkFDwA8uwy0qKgEWdn5MDF5Ce3bmQEAHj16glNpGfB5y01jr/fXRJr6Val1GmHatGkIDg6Gs7MzXF1dsXnzZuTl5ckXCAcHBwMAYmNjAQBBQUGIi4tDaGgogoKCkJ6ejh07dmDTpk1q+wxCuXrjL8xa/I389Zbvf8GW73/Bmx6v4LOJL6uxMmqMW7dzsSJqu/x1/I/JiP8xGa8PcETw+74AgPTTmXj06HG1ix1IsbqWRqqbqLi4WKbOAjZt2oR169YhPz8fdnZ2WLFiBQYMGADg2eVzAHDo0CF5+5SUFCxYsABXrlyBpaUlZs2ahSlTptT7HleLVFe/JtC7f0335qXroItztrV5Pmery4Z1NVFJvy4r0hS2Ob3ATSXvXR+1h60QGLa6g2GrO1QVtq6RisM2PUz4sH0hViMQ0YtDU6cRGLZEpFP4AxkRkQA0deUGw5aIdIqGZi3Dloh0C0e2REQCYNgSEQlAQ7OWYUtEuoUjWyIiAXCdLRGRADR0YMuwJSLdwmkEIiIBaGjWMmyJSLdwZEtEJAANzdq6w7ZNmzYN/htCJBKhqEjH72dIRBpN60a28+bN09iiiYjqoqmxVWfYNvahZkRE6qSpg0TO2RKRTtHQrIVeQxpfv34dH374Iezs7CAWi/Hrr78CAIqKijBt2jT89ttvKimSiEhZenp6Cje11KVsw0uXLsHT0xNJSUno378/qqqq5MfatWuHy5cv45tvvqmnByIi1ROJFG/qoHTYLlmyBBKJBL/99hu+/PJLyGTVnxPp5eWF9PT0Zi+QiKghRCKRwk0dlA7btLQ0BAYGwszMrNZira2tkZeX16zFERE1lKaObBv0A1mLFi3qPFZQUFDvcSIiIWjqagSlR7Yvv/wyEhISaj325MkT7NmzB/3792+2woiIGkNTR7ZKh+3s2bORmJiIGTNm4NKlSwCAvLw8HDt2DH5+frh+/To+/fRTlRVKRKQMPZFI4VabNWvWYMiQIbC2tkb37t0xbtw4ZGZmVmsTEhICc3PzatvQoUOVqkvpaQRPT0/ExsZi3rx52LZtm/yNZTIZzMzM8O9//xtubm7KdkdEpBKNHbmmpKTgvffeQ79+/SCTybBixQqMHDkS6enpaNOmjbydh4cHYmNj5a+NjIyU6r9Bc7ZjxoyBj48PEhMTcfPmTUilUnTt2hWenp4wNTVtSFdERCrR2Dnb+Pj4aq9jY2PRuXNnpKWl4a233pLvb9GiBSQSSYP7b/AVZC1btsTw4cMb/EZEREJorqfilJeXQyqVwtzcvNr+1NRU2NrawszMDAMGDEB4eDjEYrHC/hoctr/++isSEhKQnZ0NAOjcuTPeeOMNDB48uKFdERE1u+Z6BlloaCgcHR3h4uIi3zd06FD4+vrCxsYG2dnZWLZsGfz8/HD8+HGFq7GUDtuHDx9iypQpOHr0KGQymTztDx06hK+//hpeXl7YsmULWrVq1ciPRkTUdCI0PWwXLFiAtLQ0HD58GPr6+vL9o0ePlv+3g4MDnJyc4OjoiISEBPj5+dXbp9KrERYuXIgjR45gzpw5uHHjBm7duoVbt27hxo0bmD17No4dO4bw8PBGfCwiouajJ1K81ScsLAx79uzB/v370aVLl3rbdujQAVZWVrh586biupT9AHv37kVgYCAWLFiAtm3byve3bdsWn332GSZNmoS9e/cq2x0RkUo05XLd+fPny4O2Z8+eCt+rqKgIubm5Sv1gpnTYSqVSODo61nnc0dGxxv0SiIiE1tiLGubMmYMdO3YgLi4O5ubmyM/PR35+PsrLywE8+8Fs4cKFOH36NLKyspCcnIzx48dDLBYrtWhA6bD19vau8woyAEhISIC3t7ey3RERqURjL2rYtGkTysrKMGLECPTq1Uu+rV+/HgCgr6+PzMxMvP3223j11VcREhICW1tbHDlyRKmlr3X+QFZYWFjt9dy5czFlyhSMGzcOH3zwAbp16wYAuHHjBuLi4pCbm4tly5Yp/YUQEalCYy9qKC4urve4sbFxjbW4DVFn2Pbs2bPG3IZMJkNmZiaOHj1aYz8AuLu784GPRKRWmnojGj7wkYh0iqbGFh/4SEQ6pa45WXXjAx+JSKfoTNimp6fjwoULKC0thVQqrXZMJBJh3rx5zVYcEVFDNde9EZqb0mFbXFyMcePG4cyZM5DJZBCJRPIfxp7/N8OWiNRNU39rUnqd7eLFi3Hx4kX8+9//xoULFyCTyRAfH4+zZ89i0qRJ6Nu3L/78809V1kpEpJDWP6khISEBkyZNQkBAgHwBr56eHrp164a1a9eiQ4cOWLBggcoKJSJShtY/XffBgwdwcHAAABgaGgJ4diew54YNG4Zjx441c3lERA3T1BvRqIrSc7YWFha4d+8eAMDU1BSmpqa4du2a/PiDBw9QVVXV/BUSETWAps7ZKh22/fv3R2pqqvz10KFDsX79elhaWkIqlWLjxo3VbrJLRKQOmhm1DZhGeH4/hMrKSgDA559/jrZt2+Kjjz7C1KlT0bZtW6xcuVJlhRIRKaOxN6JRNaVHtq+99hpee+01+euOHTsiLS0NGRkZ0NfXR8+ePWFgwGskiEi9NHQWoWlXkOnp6dV7j1siIqFp3ZxtTk5Oozq0trZudDFERE2lr6GXkNUZtn379m3U3xD3799vUkFERE2hoQPbusN2w4YNGjscb6i+nc3UXYJK/XFf9z/jc4P7f6buEgSRHDcZ7watUHcZKpV3fLlK+tXU3KozbCdOnChkHUREzULpJVYC4/IBItIpWjeyJSLSRhr6+xjDloh0C8OWiEgAnEYgIhIAR7ZERALQ0IFtw1ZJPH78GFu3bsUHH3yAkSNH4vfffwfw7JE53333Hf766y+VFElEpCwDkUjhppa6lG14//59+Pr6IjMzExYWFigsLERxcTEAoHXr1li+fDmuXLmCJUuWqKxYIiJFtH5ku3jxYuTk5ODw4cM4deqU/GGPwLMb0vj5+eHo0aMqKZKISFmaeotFpcP28OHDCA4Ohqura62/9nXv3h137txp1uKIiBpK6x/4WFZWhk6dOtV5/NGjR3wsDhGpXWOfQbZmzRoMGTIE1tbW6N69O8aNG4fMzMxqbWQyGSIjI9G7d29YWlrCx8cHly9fVq4uZT9At27dcP78+TqPJyYmws7OTtnuiIhUorHTCCkpKXjvvfeQkJCA/fv3w8DAACNHjsSDBw/kbdatW4fo6GhERUUhMTERYrEY/v7+KCsrU1yXsh8gMDAQO3bswK5duyCVSgE8Wzz8999/IyIiAomJiQgKClK2OyIilWjsNEJ8fDzeeecd2Nvbw8HBAbGxsbh37x7S0tIAPBvVxsTEYNasWRgxYgTs7e0RExOD8vJy7N69W2FdSq9GCA4OxpUrVxAcHAxTU1MAwJQpU1BcXIyqqiq8//77vFMYEaldc13UUF5eDqlUCnNzcwBAVlYW8vPz4enpKW9jbGwMd3d3pKenKxxsNuiihi+//BLjx4/H3r17cfPmTUilUnTt2hX+/v5wd3dvxMchImpeomZ6vm5oaCgcHR3lTw3Pz88HAIjF4mrtxGIxcnNzFfbX4CvIXF1d4erq2tDTiIgE0Rwj2wULFiAtLQ2HDx+Gvr5+0zuE5t5nl4ioUfT1RAq3+oSFhWHPnj3Yv38/unTpIt8vkUgAAIWFhdXaFxYWwsLCQmFdSo9slXkmmUgkwoULF5Ttkoio2TVlZDt//nzs3bsXBw4cQM+ePasds7GxgUQiQVJSEvr16wcAqKysRGpqKpYuXaqwb6XDdsCAATXCtqqqCjk5OUhPT4ednR369u2rbHdERCrR2IsW5syZg++//x7btm2Dubm5fI7WxMQErVq1gkgkQkhICNasWYMePXrA1tYWq1evhomJCQICAhT2r3TYxsTE1Hns0qVLGD16NMaOHatsd0REKtHYy3E3bdoEABgxYkS1/fPnz0dYWBgAYObMmaioqMDcuXNRXFwMZ2dnxMfHy1do1adZbrHo6OiIyZMnY/Hixfj111+bo0siokZp7DTC8xtr1UckEiEsLEwevg3RbPeztbCwwNWrV5urOyKiRtHUu341S9jev38f//3vf2FlZdUc3RERNZpeM62zbW5Kh62vr2+t+0tKSnDt2jU8fvwYsbGxzVYYEVFjaP3IViqV1liNIBKJYGNjAw8PD7zzzjs1lkoQEQlN659BdujQIVXWQUTULNR1c3BFlLqC7O+//4avry+2bdum6nqIiJqkqVeQqYpSYduyZUv8/vvvvDk4EWk8rX9Sg7u7O06dOqXKWoiImkxPiU1ddSll1apVOHv2LMLDw3H79m35DcSJiDSJSCRSuKlDvT+Qfffdd3B3d4eNjQ1cXFwgk8kQHR2N6Oho6OnpwdDQsFp7kUiEu3fvqrRgIqL6aObPYwrCdtq0aYiNjYWNjQ38/f3V9jcCEZGyNHU1Qr1hK5PJ5P9d341oiIg0hWZGbTPeG4GISBNo6MBWcdhy6oCItImmZpbCsJ02bRqmT5+uVGf8gYyI1E1Tn/WlMGydnZ2rPYeHiEiTae3INigoCGPGjBGiFiKiJtPK1QhERNpGa6cRiIi0idZOIxARaRPNjFoFYfvgwQOh6iAiahYaOrDlyJaIdIvWP4OMiEgbcGRLRCQAEUe2RESqx5EtEZEAOGdLRCQAPQ29qoFhS0Q6RVPnbDX07wAiosbREyne6nLy5EmMHz8ednZ2MDc3x/bt26sdDwkJgbm5ebVt6NChStXFkS0R6ZSmjGwfPnwIe3t7TJgwAR999FGtbTw8PBAbGyt/bWRkpFTfDFst1ddvEXJy7/9jz38AAMMGOGDX2hC11ETKe3/MIEz2HwDrDm0BAFdu5uH/bT6MIyczAAB9nF7FgzOv1jhv0w8nMHfVLkFr1TZNWY3g7e0Nb29vAMDUqVNrbdOiRQtIJJIG982w1VKJ385FVdWzZ8RdybiANpZd4fHuKvgPfUXNlZEy/sp/gIgN+3AjuwB6enqY4OOKbas/xJB3o5Bx/S6u/HEBI+Z8L2//ip0Ndn75EfYeO6fGqrWDqudsU1NTYWtrCzMzMwwYMADh4eEQi8UKz1PrnK2i+ZHaZGRk4F//+hcsLS1hZ2eHqKioag+mfFG0b2MKSfvWkLRvjXZmLXHkZAZMTV7CyKH91F0aKeHnE5dw7FQmbt25hxvZBVgWcwDlDyvR37ErAODp06coKCqTb28NdsS1rHycOnddzZVrvqbM2SoydOhQfP3119i3bx+WLVuGs2fPws/PD48ePVJ4rlpHtsrMj/xTaWkp/P394e7ujsTERFy7dg3Tpk1Dy5YtlX50jy6SyWTYti8VY9/qD+OXlJs/Is2hpyfCSK9+MGnZAqcv3qpx3MTYCKOGOWPVpp/VUJ32UeXIdvTo0fL/dnBwgJOTExwdHZGQkAA/P796z1Vr2CozP/JPP/zwAyoqKhATEwNjY2PY29vjzz//xMaNG/Hxxx9r7H0sVe1M5l1k3S3CpJHu6i6FGsC+uxUSNs/GS0YGeFjxCO/OjUPmjZrP8At4sz+MDPXx3cF0NVSpfYSMgQ4dOsDKygo3b95U2Farln6dPn0ar732GoyNjeX7vLy8kJubi6ysLDVWpl4Hkv9EP3sbOPbspO5SqAGuZeVj0MRIDA1ajc17UrAx4l3Yde9Qo92kke746ddLKCouV0OV2kekxNZcioqKkJubq9QPZlr1A1lBQQGsrKyq7Xs+MV1QUFDngyn/uPCbqktTmwelFUi5kINP3nbT6c/5XHLcZHWXoDIt9KU4FP0h/sp5NnBIjpuMl4yNYdvLBm2MHun0Z29O+k0Y2paXl8tHqVKpFHfu3MHFixfRpk0btGnTBitXroSfnx8kEgmys7OxdOlSiMViDB8+XGHfWhW2jdXHqeYSGl2xbutRGBroYcb7o9GqZQt1l6Nybfp/rO4SVGbfxunIu1eC4EVbkRw3GQM/+A9Wzx8Lg1b30G/CV+our9nlHV+umo6bMHQ9f/48fH195a8jIyMRGRmJCRMmYM2aNcjMzMTOnTtRUlICiUSCgQMHYsuWLTA1NVXYt1aFrYWFBQoLC6vte/7awsJCHSWplUwmw3/3nYKXS9cXImh1yeKP/XAkJQN38h/AtOVLCHjzVbzu3APjPvla3sa4hSHGvNkfX209psZKtU9TfiAbOHAgiouL6zweHx/f6L61KmxdXFwQERGByspKvPTSSwCApKQkdOjQATY2NmquTngpZ6/hRnYh5r3jou5SqIEs2rVG7NJAWLQzRWl5JTKu/4UxM2OQmHZZ3sbf2xktXzLC9gNpaqxU+2jq7+RqDdv65kesra2xZMkSnD17Fvv37wcABAQEICoqClOnTsWcOXNw/fp1rF27FvPmzXshVyIMfLUnHpzZ8ELM1eqaaUu2KWyz40AadjBoG0xTk0CtqxHOnz+PQYMGYdCgQaioqEBkZCQGDRqEFStWAADy8vJw69b/rTs0MzPD3r17kZubiyFDhmDu3LmYNm0aPv5Yd+fxiKiBhFyO0ABqHdkqmh+JiYmpsc/BwQE//8zF3URUO+XmbIW/6lSr5myJiBTR1BlFhi0R6RQNzVqGLRHpGA1NW4YtEekUztkSEQmgKbdQVCWGLRHpFoYtEZHqaerTdRm2RKRTuPSLiEgAGpq1DFsi0jEamrYMWyLSKZyzJSISAOdsiYgEoKFZy7AlIh2joWnLsCUinaKnofMIDFsi0imaGbUMWyLSNRqatgxbItIpXPpFRCQADZ2yZdgSkW7R0Kxl2BKRjtHQtGXYEpFO4ZwtEZEAOGdLRCQADc1a6Km7ACKi5iQSiRRudTl58iTGjx8POzs7mJubY/v27dWOy2QyREZGonfv3rC0tISPjw8uX76sVF0MWyLSKSKR4q0uDx8+hL29PVauXAljY+Max9etW4fo6GhERUUhMTERYrEY/v7+KCsrU1gXw5aIdIpIia0u3t7eWLRoEUaMGAE9verxKJPJEBMTg1mzZmHEiBGwt7dHTEwMysvLsXv3boV1MWyJSKc0ZWRbn6ysLOTn58PT01O+z9jYGO7u7khPT1d4PsOWiHRMU8a2dcvPzwcAiMXiavvFYjEKCgoUns/VCESkUzR16RdHtkSkU1QzrgUkEgkAoLCwsNr+wsJCWFhYKDyfYUtEOkVVc7Y2NjaQSCRISkqS76usrERqaipcXV0Vns9pBCLSKU25XLe8vBw3b94EAEilUty5cwcXL15EmzZtYG1tjZCQEKxZswY9evSAra0tVq9eDRMTEwQEBCjsm2FLRLpFmayV1b77/Pnz8PX1lb+OjIxEZGQkJkyYgJiYGMycORMVFRWYO3cuiouL4ezsjPj4eJiamip8S4YtEekUvSaE7cCBA1FcXFznaSKRCGFhYQgLC2twXQxbItIpvOsXEZEQNDNrGbZEpFs0NGsZtkSkWzT1ogaGLRHpFM7ZEhEJQFNHtryCjIhIABzZEpFO0dSRLcOWiHQK52yJiASgzMi2jgvIVIphS0Q6hWFLRCQATiMQEQmAP5AREQlAQ7OWYUtEOkZD01ZUXFysjrliIqIXCq8gIyISAMOWiEgADFsiIgEwbImIBMCwJSISAMNWC2zatAl9+/aFRCLB4MGDcerUqXrbp6SkYPDgwZBIJHj55ZexefNmgSolRU6ePInx48fDzs4O5ubm2L59u8JzMjIy8K9//QuWlpaws7NDVFQUZDIuItI2DFsNFx8fj9DQUMyePRsnTpyAi4sLxowZg5ycnFrb3759G2PHjoWLiwtOnDiBT9pqy8AAAAtZSURBVD/9FPPmzcO+ffsErpxq8/DhQ9jb22PlypUwNjZW2L60tBT+/v6wsLBAYmIiVq5cifXr12PDhg0CVEvNietsNZyXlxccHBzw1Vdfyff169cPI0aMwOLFi2u0X7x4MQ4cOIBz587J902fPh1XrlzB0aNHBamZlNOxY0esWrUKEydOrLPNN998g4iICPz555/ycP7iiy+wefNmZGZmQqSp16ZSDRzZarDHjx/jwoUL8PT0rLbf09MT6enptZ5z+vTpGu29vLxw/vx5PHnyRGW1kmqcPn0ar732WrVRsJeXF3Jzc5GVlaXGyqihGLYarKioCFVVVRCLxdX2i8ViFBQU1HpOQUFBre2fPn2KoqIildVKqlHX/8/nx0h7MGyJiATAsNVg7dq1g76+PgoLC6vtLywshIWFRa3nWFhY1NrewMAA7dq1U1mtpBp1/f98foy0B8NWgxkZGcHJyQlJSUnV9iclJcHV1bXWc1xcXGpt/8orr8DQ0FBltZJquLi4IDU1FZWVlfJ9SUlJ6NChA2xsbNRYGTUUw1bDTZs2DTt27MDWrVtx9epVzJ8/H3l5eQgKCgIABAcHIzg4WN4+KCgIubm5CA0NxdWrV7F161bs2LEDH3/8sbo+Av1DeXk5Ll68iIsXL0IqleLOnTu4ePGifCnfkiVL4OfnJ28fEBAAY2NjTJ06FZmZmdi/fz/Wrl2LqVOnciWCluHSLy2wadMmrFu3Dvn5+bCzs8OKFSswYMAAAICPjw8A4NChQ/L2KSkpWLBgAa5cuQJLS0vMmjULU6ZMUUvtVF1ycjJ8fX1r7J8wYQJiYmIQEhKClJQUXLp0SX4sIyMDc+bMwblz52Bubo6goCDMnz+fYatlGLZERALgNAIRkQAYtkREAmDYEhEJgGFLRCQAhi0RkQAYtkREAmDYUqP5+PjI1/kCQFZWltI3xBZKZGQkzM3Nm61dbUJCQiCRSBp1bn19Ojo6NmufpF4MWy21fft2mJuby7d27drB3t4eU6dOxd27d9VdXoNcuXIFkZGRvGUg6TQDdRdATRMaGoquXbvi0aNHSEtLw86dO3Hy5EmkpqaiZcuWgtbSuXNn5OXlNfgeDFevXkVUVBRef/11Xu9POothq+W8vLzQv39/AMCkSZPQpk0bREdH46effkJAQECt5zx8+BAmJibNXotIJMJLL73U7P0S6QJOI+iYQYMGAYD8n+TP5xOzsrIwfvx4WFtbY+zYsfL2P/zwA4YMGQJLS0vY2NggMDAQt2/frtHvf/7zHzg5OcHS0hKenp61PnSyrjnbvLw8zJo1C/b29rCwsICjoyNmzJiBsrIybN++HYGBgQAAX19f+bTIP/s4d+4cxowZg86dO8PS0hJvvvkmTpw4UeP9U1NTMWTIEEgkEjg5OWHLli0N/wL/4aeffsK4cePkdffp0wfh4eHV7sD1T9nZ2Rg7diw6duyIHj16ICIiAk+fPq3RTtnvnHQLR7Y65tatWwCAtm3byvdJpVKMGjUKzs7OWLp0KfT19QEAX375JZYuXYoRI0Zg4sSJKC4uRlxcHN58802kpKSgffv2AICtW7di1qxZcHV1xUcffYScnBy8/fbbMDc3R8eOHeutJz8/H15eXigqKkJgYCDs7OyQm5uLgwcP4v79+xgwYACCg4MRGxuL2bNno2fPngAgv4VkSkoKRo8eDUdHR8ydOxeGhob4/vvvMWrUKOzduxcDBw4E8OxmLaNGjUK7du0QGhqKqqoqREVFNekevtu3b0eLFi0QHByM1q1b48yZM9i4cSP++uuvGk8slkqlCAgIgKOjIyIiIpCSkoK1a9eitLQUa9askbdT9jsn3cOw1XKlpaUoKipCZWUl0tPTsWrVKhgbG+ONN96Qt3ny5AneeOMNrFixQr4vJycHy5cvR2hoKObPny/fP3r0aLi5uWHjxo1YtGgRnjx5gs8//xyOjo44cOAAjIyMAAC9e/fG9OnTFYZtREQEcnNzceTIEbz66qvy/WFhYZDJZBCJRHB3d0dsbCw8PDzk4QkAMpkMn3zyCdzc3PDjjz/K73I1ZcoUDBo0CJ9//jmOHDkCAFixYgWkUil+/vlnWFtbAwBGjhwJNze3xn61iIuLqzbvHRQUhO7du2PZsmVYunQpOnXqJD/25MkTuLu7Y+3atQCADz74AMHBwdiyZQumTp0KW1tbpb9z0k2cRtByo0ePRvfu3eHg4IApU6bAwsICO3fuhJWVVbV277//frXXBw4cwNOnTzFq1CgUFRXJt9atW8Pe3h7JyckAgPPnz6OwsBCBgYHyoAWe3RLQzMys3tqkUikOHTqEYcOGVQva5xTdIvDSpUu4du0aAgICcP/+fXmNZWVl8PDwwG+//Ya///4bVVVVSExMxFtvvSUPWgCwtbWFl5dXve9Rn+dBK5VKUVJSgqKiIri5uUEmk+H333+v0f6f9xUGgI8++ggymUz+F4Ky3znpJo5stVxUVBR69eqFFi1aoFOnTujUqVONENPT00Pnzp2r7btx4wYAyH9c+19dunQBAPlNrbt3717tuIGBgcKVA/fu3UNpaSns7OyU/jy11Th9+nRMnz691jb379+HoaEhKioqatRYW90NkZmZicWLFyMlJQUVFRXVjpWWllZ7LRKJ0K1bt1rfOzs7G4Dy3znpJoatluvXr1+df3ifMzQ0hIFB9f/VUqkUALB79+4axwBoxKqC5zVGRETAycmp1jbt27dHSUlJs793SUkJfH190bJlSyxcuBDdunWDsbEx7t69i6lTp8prawht+M5JdRi2L6iuXbsCADp16oTevXvX2e75P8tv3LiBIUOGyPc/ffoUWVlZ6NOnT53ntm/fHq1bt8bly5ebVGOrVq3g4eFRZztDQ0MYGxvLR47/VNs+ZSQnJ6OoqAjffvstXn/9dfn+/32+23MymQw3b96sNop//t7P/1Wh7HdOuolzti8oPz8/6OvrY9WqVZDJaj6so6ioCADwyiuvoH379vj222/x+PFj+fHvvvtO4YhST08PPj4+OHr0KH777bcax5+/7/M1v8XFxdWOOzk5oVu3boiOjkZZWVmN8+/duwcA0NfXh6enJw4fPiyf9gCA69ev45dffqm3xro8X7Hxz+9GKpUiOjq6znNiY2NrvBaJRPD29gag/HdOuokj2xdUly5dEBERgfDwcOTk5MDHxwdmZmbIysrCTz/9BH9/f4SFhcHQ0BALFy7ErFmz4Ovri1GjRiE7Oxvbt29Xao5x8eLFOH78OIYPH47Jkyejd+/eKCgowIEDB7Bt2zbY2Nigb9++0NfXx5dffomSkhIYGxvD2dkZXbp0wfr16xEQEAA3NzdMnDgRHTt2RG5uLk6ePAmZTIaDBw8CeLa64ZdffsFbb72F9957D1KpFHFxcejVqxcyMjIa/P24ubmhbdu2CAkJQXBwMAwMDLB//36Ul5fX2t7Q0BCnTp3C+++/Dzc3NyQnJ2Pfvn2YPHkybG1tG/Sdk25i2L7Apk+fLh85rl69GlKpFFZWVhg0aBBGjhwpbzd58mRUVVXhq6++wqJFi2Bvb48dO3Zg+fLlCt/D0tISx44dw/Lly7Fnzx6UlJTIL4x4vgbWwsIC69atw5o1azBz5kxUVVUhOjoaXbp0wYABA3D06FF88cUX+Oabb1BWVgYLCwv069cPkyZNkr9Pnz59sGfPHnz22WeIjIyElZWV/EnEjQnbNm3aYNeuXVi4cCEiIyNhYmICPz8/TJkyRf6wzX/S09PD7t27MXv2bCxatAgtW7bEjBkzEB4e3qjvnHQPH/hIRCQAztkSEQmAYUtEJACGLRGRABi2REQCYNgSEQmAYUtEJACGLRGRABi2REQCYNgSEQmAYUtEJID/Dzw9Pi01BgTRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTH6FXxJadwJ",
        "outputId": "008a9635-105e-4cbd-998c-ba9890eb5cb8"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using Logistic Regression: 0.670\",\n",
        ")\n",
        "\n",
        "logreg_kaggle = 0.670"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using Logistic Regression: 0.670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_txvEFcC001"
      },
      "source": [
        "That the score on Kaggle is worse that the score that cross-validation has resulted in (0.75) is the first indication that even the simpliest of models suitable for the task overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "h6EOeOndadwJ",
        "outputId": "6558c8be-e159-4c49-a97d-7e7e102aad59"
      },
      "source": [
        "coefficients = pd.DataFrame({\"feature\": X.columns, \"coef\": logreg.coef_[0]}).drop(\n",
        "    [\"feature\"], axis=1\n",
        ")\n",
        "coefficients.query(\"coef > 0.2\").sort_values(by=\"coef\", ascending=False).nlargest(\n",
        "    10, \"coef\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.672171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.529006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>0.447347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>0.427591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0.421678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.419680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>0.363654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0.336187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.323108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.311913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         coef\n",
              "33   0.672171\n",
              "65   0.529006\n",
              "157  0.447347\n",
              "241  0.427591\n",
              "199  0.421678\n",
              "24   0.419680\n",
              "246  0.363654\n",
              "244  0.336187\n",
              "13   0.323108\n",
              "17   0.311913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miijLA4qCkMQ"
      },
      "source": [
        "We see that features `33` and `65`, the same features that we have found to have the highest correlation with the target, have been assigned the largest coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "dnCut19DadwJ",
        "outputId": "6f0b7a34-0754-4c8a-a53d-cdf24d338edc"
      },
      "source": [
        "perm = PermutationImportance(logreg, random_state=13).fit(X, y)\n",
        "eli5.show_weights(perm, top=10, feature_names=train_df.columns[1:].tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0136\n",
              "                \n",
              "                    &plusmn; 0.0064\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                217\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 84.33%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0096\n",
              "                \n",
              "                    &plusmn; 0.0120\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                33\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 86.21%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0080\n",
              "                \n",
              "                    &plusmn; 0.0051\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                119\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 87.19%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0072\n",
              "                \n",
              "                    &plusmn; 0.0128\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                65\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 88.20%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0064\n",
              "                \n",
              "                    &plusmn; 0.0082\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                111\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 89.25%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0056\n",
              "                \n",
              "                    &plusmn; 0.0039\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                17\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 90.35%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0048\n",
              "                \n",
              "                    &plusmn; 0.0106\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                91\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.51%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0040\n",
              "                \n",
              "                    &plusmn; 0.0051\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                244\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.51%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0040\n",
              "                \n",
              "                    &plusmn; 0.0072\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                98\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.51%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0040\n",
              "                \n",
              "                    &plusmn; 0.0072\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                180\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 91.51%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 290 more &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwkMKKmsDTMw"
      },
      "source": [
        "We find feature `33` on top of the list of feature importances under permutations. The same cannot be said of the remaining features we saw in the list of features that are most correlated with the target and the list of coefficients in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4FECdexoO7f",
        "outputId": "610cf100-eb4c-453e-ab0d-e1210a30985a"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using Logistic Regression (using probabilities): 0.739\",\n",
        ")\n",
        "\n",
        "logreg_kaggle_prob = 0.739"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using Logistic Regression (using probabilities): 0.739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srFAbsAEB0y"
      },
      "source": [
        "Submitting probabilities, however, resulted in a much higher score on the public leaderboard. We will see whether this behavior repeats itself with other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3PepMWadwJ"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2VSd8RhadwJ",
        "outputId": "ac14d699-0a9d-439e-c0c5-692b63dbf061"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "score_gnb = get_score(gnb, X, y)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Gaussian Naive Bayes is: \",\n",
        "    \"{:.4f}\".format(score_gnb),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Gaussian Naive Bayes is:  0.6979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRnrbNH3adwK",
        "outputId": "f8aff645-b150-4525-9300-a410efc09fdc"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using Gaussian Naive Bayes: 0.611\",\n",
        ")\n",
        "\n",
        "gnb_kaggle = 0.611"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using Gaussian Naive Bayes: 0.611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtF6k0q2PKgk"
      },
      "source": [
        "Submitting probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx7hLs6ZPn6C",
        "outputId": "896c406c-5416-4975-a520-f667e59974c1"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using Gaussian Naive Bayes (using probabilities): 0.675\",\n",
        ")\n",
        "\n",
        "gnb_kaggle_prob = 0.675"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using Gaussian Naive Bayes (using probabilities): 0.675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2N7mI6nEel6"
      },
      "source": [
        "We observe the same behavior in the case of GaussianNB: submitting probabilities results in the higher score than submitting binary values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqtXm9IDadwK"
      },
      "source": [
        "### K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jAhyVdCadwK",
        "outputId": "bf5c353f-35fd-439c-d9cf-df684305de5c"
      },
      "source": [
        "knn = KNeighborsClassifier()\n",
        "\n",
        "score_knn = get_score(knn, X, y)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using K-Nearest Neighbors is: \",\n",
        "    \"{:.4f}\".format(score_knn),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using K-Nearest Neighbors is:  0.5627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wBMMZlsadwK",
        "outputId": "92b4fd59-acc8-4a7f-ce18-a986327f3fb1"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using K-Nearest Neighbors: 0.549\",\n",
        ")\n",
        "\n",
        "knn_kaggle = 0.549"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using K-Nearest Neighbors: 0.549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b57OxWkRP87w",
        "outputId": "a3ceaf0a-9ae0-481b-bd8c-0626793bacf2"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using K-Nearest Neighbors: 0.568\",\n",
        ")\n",
        "\n",
        "knn_kaggle_prob = 0.568"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using K-Nearest Neighbors: 0.568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfOYO2vdEuEU"
      },
      "source": [
        "Submitting probabilities predicted by K-Nearest Neighbors gives us the worse improvement over the score with binary values so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWpIrI9zadwK"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub8s-yAEadwL",
        "outputId": "f1183500-e0b6-43a4-ca10-100c0e148b13"
      },
      "source": [
        "svm = LinearSVC()\n",
        "\n",
        "score_svm = get_score(svm, X, y)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using LinearSVC is: \",\n",
        "    \"{:.4f}\".format(score_svm),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using LinearSVC is:  0.7441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdS_I2YbadwN",
        "outputId": "1c2b3a81-3e0d-4167-c39f-4b77f277d41d"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using LinearSVC: 0.663\",\n",
        ")\n",
        "\n",
        "svm_kaggle = 0.663"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using LinearSVC: 0.663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQMzdWXFQUv8"
      },
      "source": [
        "Submitting probabilities requires us to additionally use `CalibratedClassifierCV`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlrR-Ks-QT5a"
      },
      "source": [
        "svm = LinearSVC()\n",
        "clf = CalibratedClassifierCV(svm)\n",
        "clf.fit(X, y)\n",
        "\n",
        "y_proba = clf.predict_proba(test_subm)[:, 1]\n",
        "results = pd.DataFrame({\"target\": y_proba}).set_index(test_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv2vs5aqQT5a",
        "outputId": "5a4f286a-d2c7-4057-d8c8-77da2dc09b65"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using LinearSVC: 0.722\",\n",
        ")\n",
        "\n",
        "svm_kaggle_prob = 0.722"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using LinearSVC: 0.722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlEhTuJUFGMv"
      },
      "source": [
        "Submitting probabilities results in a higher score on public leaderboard again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUEcMTuradwN"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaYcWH4BadwN",
        "outputId": "1ff95e29-b78b-4754-d6f4-abd9b8e7edd3"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "score_rf = get_score(rf, X, y)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using RandomForestClassifier is: \",\n",
        "    \"{:.4f}\".format(score_rf),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using RandomForestClassifier is:  0.6642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rttwi1phHheA",
        "outputId": "17476a72-371c-40a2-e49e-a536132b09fe"
      },
      "source": [
        "rf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "dgv2LnBpJBsB",
        "outputId": "d302c0a4-1f5e-4ca5-9719-4ae5d9c8ccb2"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(rf, X_valid, y_valid, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "RqiUCk-MHfNe",
        "outputId": "73c3e453-4807-4436-902f-7338d89098e3"
      },
      "source": [
        "importances = pd.DataFrame(\n",
        "    {\"feature\": X.columns, \"feature_importances\": rf.feature_importances_}\n",
        ").drop([\"feature\"], axis=1)\n",
        "importances.query(\"feature_importances > 0.01\").sort_values(\n",
        "    by=\"feature_importances\", ascending=False\n",
        ").nlargest(10, \"feature_importances\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_importances</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.026011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.016517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.013105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>0.011737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.011607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.011554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.010078</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     feature_importances\n",
              "33              0.026011\n",
              "91              0.016517\n",
              "65              0.013105\n",
              "295             0.011737\n",
              "24              0.011607\n",
              "117             0.011554\n",
              "39              0.010078"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjPUyIdSHfNf"
      },
      "source": [
        "Again we observe that features `33` and `65`, the same features that we have found to have the highest correlation with the target, have been found to be most important using Random Forrest Classifier. `PermutationImportance`, however, produce a slighty different top-three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "8xsiSr6XHfNf",
        "outputId": "a32f9752-fd25-4a45-d6e3-eb73f653da82"
      },
      "source": [
        "perm = PermutationImportance(rf, random_state=13).fit(X, y)\n",
        "eli5.show_weights(perm, top=10, feature_names=train_df.columns[1:].tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0072\n",
              "                \n",
              "                    &plusmn; 0.0032\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                65\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0072\n",
              "                \n",
              "                    &plusmn; 0.0032\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                295\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 81.58%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0064\n",
              "                \n",
              "                    &plusmn; 0.0039\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                24\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 83.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0056\n",
              "                \n",
              "                    &plusmn; 0.0039\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                9\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 83.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0056\n",
              "                \n",
              "                    &plusmn; 0.0039\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                46\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 83.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0056\n",
              "                \n",
              "                    &plusmn; 0.0064\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                157\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 83.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0056\n",
              "                \n",
              "                    &plusmn; 0.0039\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                279\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 83.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0056\n",
              "                \n",
              "                    &plusmn; 0.0039\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                237\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 84.94%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0048\n",
              "                \n",
              "                    &plusmn; 0.0060\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                137\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 84.94%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0048\n",
              "                \n",
              "                    &plusmn; 0.0032\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                33\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.94%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 290 more &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNwb844eCr-w",
        "outputId": "412367b4-0163-4352-8835-70fc5f9db7d9"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using RandomForestClassifier: 0.534\",\n",
        ")\n",
        "\n",
        "rf_kaggle = 0.534"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using RandomForestClassifier: 0.534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lHZ_RZMSDPd",
        "outputId": "73c3403f-5f93-451f-b587-0f45ee2ae6de"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using RandomForestClassifier (using probabilities): 0.685\",\n",
        ")\n",
        "\n",
        "rf_kaggle_prob = 0.685"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using RandomForestClassifier (using probabilities): 0.685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nCglWMuFWga"
      },
      "source": [
        "Probabilities lead to a higher score in the case of Random Forrest Classifier as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2pa6NCJFczB"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "0OlacwBfdaWe",
        "outputId": "f8e7b45f-2fa5-4e6b-f940-f1d124cea6ec"
      },
      "source": [
        "get_scoreboard(\n",
        "    logreg=True, \n",
        "    gnb=True,\n",
        "    knn=True,\n",
        "    svm=True, \n",
        "    rf=True\n",
        "    ).sort_values(by=\"Kaggle score\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Kaggle score</th>\n",
              "      <th>Kaggle score (proba)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.759028</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.744097</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.697917</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>K-Nearest Neighbors</td>\n",
              "      <td>0.562674</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Random Forrest Classifier</td>\n",
              "      <td>0.664236</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.685</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Model    ROCAUC  Kaggle score  Kaggle score (proba)\n",
              "0        Logistic Regression  0.759028         0.670                 0.739\n",
              "3     Support Vector Machine  0.744097         0.663                 0.722\n",
              "1       Gaussian Naive Bayes  0.697917         0.611                 0.739\n",
              "2        K-Nearest Neighbors  0.562674         0.549                 0.568\n",
              "4  Random Forrest Classifier  0.664236         0.534                 0.685"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz1WmkTRGiT9"
      },
      "source": [
        " - There is a clear indication of overfitting in model performance\n",
        " - The simpler models such as Logistic Regression performs better than the more complex models such as Random Forrest Regressor\n",
        " - Submitting probabilities results in a higher score on both private and public leaderboards (only the latter on which is presented in the) on Kaggle\n",
        " - The models inspected suffer severely from the disbalance of classes as it is clear from the the number of false positives (zeroes classified as ones in our case).\n",
        "\n",
        " It is the last issue that we are going to be addressing in the following series of experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wptlo9Wd7Gb"
      },
      "source": [
        "## Iteration 1: optimizing the hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ew7S3BgsWO"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQyhZ89PeCSl",
        "outputId": "ae58754c-6a9e-40a8-842d-09957216d104"
      },
      "source": [
        "parameters_logreg = {\n",
        "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
        "    \"tol\": [1e-4, 1e-5],\n",
        "    \"fit_intercept\": [True, False],\n",
        "    \"C\": [0.6, 0.8, 1.0],\n",
        "    \"class_weight\": [\"balanced\"],\n",
        "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"saga\"],\n",
        "    \"l1_ratio\": [0.6, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "gscv_logreg = GridSearchCV(\n",
        "    logreg, parameters_logreg, n_jobs=-1, scoring=\"roc_auc\", verbose=1, cv=5\n",
        ")\n",
        "gscv_logreg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:    5.6s\n",
            "[Parallel(n_jobs=-1)]: Done 1322 tasks      | elapsed:   36.1s\n",
            "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
              "             param_grid={'C': [0.6, 0.8, 1.0], 'class_weight': ['balanced'],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'l1_ratio': [0.6, 0.8, 1.0],\n",
              "                         'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
              "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
              "                         'tol': [0.0001, 1e-05]},\n",
              "             scoring='roc_auc', verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMlEioYFflj5",
        "outputId": "02d17bec-a7c7-4ff5-ebf5-eec7d4607256"
      },
      "source": [
        "print((gscv_logreg.best_score_))\n",
        "gscv_logreg.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8190972222222221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.8,\n",
              " 'class_weight': 'balanced',\n",
              " 'fit_intercept': True,\n",
              " 'l1_ratio': 0.6,\n",
              " 'penalty': 'l1',\n",
              " 'solver': 'liblinear',\n",
              " 'tol': 0.0001}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haIEIJbCgVCN",
        "outputId": "d11ba39b-61b1-4565-e8f2-e3c3c26a8bcf"
      },
      "source": [
        "logreg_opt = LogisticRegression(\n",
        "    C=0.8,\n",
        "    class_weight=\"balanced\",\n",
        "    fit_intercept=True,\n",
        "    penalty=\"l1\",\n",
        "    solver=\"liblinear\",\n",
        "    tol=0.0001,\n",
        ")\n",
        "\n",
        "score_logreg = get_score(logreg_opt, X, y)\n",
        "print(\n",
        "    \"The ROCAUC score using Logistic Regression (with hyperparameters optimized) is: \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using Logistic Regression (with hyperparameters optimized) is:  0.8181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfcZVz26hC4w",
        "outputId": "f612df7b-5909-4881-85cf-e108f548dfe9"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) is: : 0.738\",\n",
        ")\n",
        "\n",
        "logreg_kaggle = 0.738"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) is: : 0.738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGbytUoBqCkH",
        "outputId": "b30d9a51-e5d8-4881-b3d1-e35076774099"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) is: : 0.815\",\n",
        ")\n",
        "\n",
        "logreg_kaggle_prob = 0.815"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) is: : 0.815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpH6Wuvlg7rO"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hMMYlrTgUeF",
        "outputId": "f09f196e-f1e9-453f-b7ee-3917a828fb55"
      },
      "source": [
        "parameters_gnb = {\"var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-10]}\n",
        "\n",
        "gscv_gnb = GridSearchCV(\n",
        "    gnb, parameters_gnb, n_jobs=-1, scoring=\"roc_auc\", verbose=1, cv=5\n",
        ")\n",
        "gscv_gnb.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=GaussianNB(), n_jobs=-1,\n",
              "             param_grid={'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-10]},\n",
              "             scoring='roc_auc', verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xfbw2YhUgUeG",
        "outputId": "982ef670-555e-413c-d2be-9603ab5fae45"
      },
      "source": [
        "print((gscv_gnb.best_score_))\n",
        "gscv_gnb.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6979166666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'var_smoothing': 1e-09}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30WxCCzWh-MX"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOeN7hkuiCQW",
        "outputId": "54febf5b-6f1b-447a-adbf-420ed33e9b32"
      },
      "source": [
        "parameters_knn = {\n",
        "    \"n_neighbors\": [5, 10, 15, 30],\n",
        "    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
        "    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"],\n",
        "    \"leaf_size\": [10, 20, 30, 40],\n",
        "    \"p\": [1, 2, 3],\n",
        "}\n",
        "\n",
        "gscv_knn = GridSearchCV(\n",
        "    knn, parameters_knn, n_jobs=-1, scoring=\"roc_auc\", verbose=1, cv=5\n",
        ")\n",
        "gscv_knn.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:    8.1s\n",
            "[Parallel(n_jobs=-1)]: Done 1252 tasks      | elapsed:   37.3s\n",
            "[Parallel(n_jobs=-1)]: Done 2436 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=-1,\n",
              "             param_grid={'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
              "                         'leaf_size': [10, 20, 30, 40],\n",
              "                         'metric': ['euclidean', 'manhattan', 'chebyshev',\n",
              "                                    'minkowski'],\n",
              "                         'n_neighbors': [5, 10, 15, 30], 'p': [1, 2, 3]},\n",
              "             scoring='roc_auc', verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKAVvA1HjlJm",
        "outputId": "dc26eea3-103e-493e-811d-19eb5374decf"
      },
      "source": [
        "print((gscv_knn.best_score_))\n",
        "gscv_knn.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6444444444444444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'algorithm': 'ball_tree',\n",
              " 'leaf_size': 10,\n",
              " 'metric': 'minkowski',\n",
              " 'n_neighbors': 30,\n",
              " 'p': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ygmh8bEj2FO",
        "outputId": "d0bc501c-c56c-4067-dffd-f756756dfe9b"
      },
      "source": [
        "knn_opt = KNeighborsClassifier(\n",
        "    algorithm=\"ball_tree\", leaf_size=10, metric=\"minkowski\", n_neighbors=30, p=3\n",
        ")\n",
        "\n",
        "score_knn = get_score(knn_opt, X, y)\n",
        "print(\n",
        "    \"The ROCAUC score using KNeighborsClassifier (with hyperparameters optimized) is: \",\n",
        "    \"{:.4f}\".format(score_knn),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using KNeighborsClassifier (with hyperparameters optimized) is:  0.6444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE3YPv7Sj2FO",
        "outputId": "4ed1fb3c-f445-4302-b3b7-c476180487cb"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using KNeighborsClassifier (with hyperparameters optimized) is: : 0.541\",\n",
        ")\n",
        "\n",
        "knn_kaggle = 0.541"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using KNeighborsClassifier (with hyperparameters optimized) is: : 0.541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9N6j_STTfOv",
        "outputId": "b97bcc32-8024-4f47-de87-a25c2e9fe3e2"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using KNeighborsClassifier (with hyperparameters optimized) is: : 0.635\",\n",
        ")\n",
        "\n",
        "knn_kaggle_prob = 0.635"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using KNeighborsClassifier (with hyperparameters optimized) is: : 0.635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x2KLw9XkWuX"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAmrDY85kYfs",
        "outputId": "09f9ce42-44ce-4e17-fc4b-67b83d86c379"
      },
      "source": [
        "parameters_svm = {\n",
        "    \"penalty\": [\"l1\", \"l2\"],\n",
        "    \"loss\": [\"hinge\", \"squared_hinge\"],\n",
        "    \"tol\": [1e-3, 1e-4, 1e-5],\n",
        "    \"C\": [0.6, 0.8, 1.0],\n",
        "    \"fit_intercept\": [\"True\", False],\n",
        "    \"intercept_scaling\": [0.6, 0.8, 1.0],\n",
        "    \"class_weight\": [\"balanced\", None],\n",
        "}\n",
        "\n",
        "gscv_svm = GridSearchCV(\n",
        "    svm, parameters_svm, n_jobs=-1, scoring=\"roc_auc\", verbose=1, cv=5\n",
        ")\n",
        "gscv_svm.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=-1)]: Done 1500 tasks      | elapsed:   15.4s\n",
            "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed:   22.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=LinearSVC(), n_jobs=-1,\n",
              "             param_grid={'C': [0.6, 0.8, 1.0],\n",
              "                         'class_weight': ['balanced', None],\n",
              "                         'fit_intercept': ['True', False],\n",
              "                         'intercept_scaling': [0.6, 0.8, 1.0],\n",
              "                         'loss': ['hinge', 'squared_hinge'],\n",
              "                         'penalty': ['l1', 'l2'],\n",
              "                         'tol': [0.001, 0.0001, 1e-05]},\n",
              "             scoring='roc_auc', verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51oeGjvmkYfs",
        "outputId": "1f1fc741-f8ee-4d2b-cd1b-b329ba8f6763"
      },
      "source": [
        "print((gscv_svm.best_score_))\n",
        "gscv_svm.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7444444444444445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.6,\n",
              " 'class_weight': 'balanced',\n",
              " 'fit_intercept': 'True',\n",
              " 'intercept_scaling': 1.0,\n",
              " 'loss': 'squared_hinge',\n",
              " 'penalty': 'l2',\n",
              " 'tol': 0.001}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzxRCT9jPgfs",
        "outputId": "9a2d8ce0-c2b3-40c3-b2f4-6d5fec71d416"
      },
      "source": [
        "svm_opt = LinearSVC(C=0.6, class_weight=\"balanced\", fit_intercept=True)\n",
        "\n",
        "score_svm = get_score(svm_opt, X, y)\n",
        "print(\n",
        "    \"The ROCAUC score using LinearSVC (with hyperparameters optimized) is: \",\n",
        "    \"{:.4f}\".format(score_svm),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using LinearSVC (with hyperparameters optimized) is:  0.7444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5koi7VrUAQS"
      },
      "source": [
        "svm_opt = LinearSVC(C=0.6, class_weight=\"balanced\", fit_intercept=True)\n",
        "clf = CalibratedClassifierCV(svm_opt)\n",
        "clf.fit(X, y)\n",
        "\n",
        "y_proba = clf.predict_proba(test_subm)[:, 1]\n",
        "results = pd.DataFrame({\"target\": y_proba}).set_index(test_ids)\n",
        "results.to_csv(\"results_59.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOtPFMdyUAQS",
        "outputId": "686e52a5-a45d-41e0-de04-9d573d5b4fdb"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using LinearSVC: 0.722\",\n",
        ")\n",
        "\n",
        "svm_kaggle_prob = 0.722"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using LinearSVC: 0.722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "pmnq25rLQ_bs",
        "outputId": "8f17b96a-4933-4d3d-d760-fbffc991a393"
      },
      "source": [
        "get_scoreboard(\n",
        "    logreg=True,\n",
        "    knn=True,\n",
        "    svm=True\n",
        ").sort_values(by=\"Kaggle score\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Kaggle score</th>\n",
              "      <th>Kaggle score (proba)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.818056</td>\n",
              "      <td>0.738</td>\n",
              "      <td>0.815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>K-Nearest Neighbors</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.635</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Model    ROCAUC  Kaggle score  Kaggle score (proba)\n",
              "0     Logistic Regression  0.818056         0.738                 0.815\n",
              "2  Support Vector Machine  0.744444         0.663                 0.722\n",
              "1     K-Nearest Neighbors  0.644444         0.541                 0.635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNLpUz3KmzXl"
      },
      "source": [
        "## Iteration 1: oversampling only using SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN986GWuKOc7"
      },
      "source": [
        "In what follows we will be applying SMOTE and its variants which balances the distribution of classes by generating sinthetic instances of the minority class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaEGem4MmokE",
        "outputId": "019e9cff-7d76-4768-a3ea-a18ea23060b2"
      },
      "source": [
        "oversample = SMOTE(sampling_strategy=\"minority\")\n",
        "X_over, y_over = oversample.fit_resample(X, y)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_over, y_over, test_size=0.3)\n",
        "\n",
        "counter = Counter(y_over)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1.0: 160, 0.0: 160})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltvHed4PHVQg"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM1KgWn5m06J",
        "outputId": "01880005-6f08-4740-d7d0-79f76cac87f3"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_logreg = get_score(logreg, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression is (after oversampling using SMOTE): \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression is (after oversampling using SMOTE):  0.9264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLr0mZUYK-wr"
      },
      "source": [
        "Application of SMOTE to balance the distribution of target variables has bolstered the score significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "9240FgWBKK5X",
        "outputId": "118e2de5-9698-40ce-a846-205747e2fa12"
      },
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "plot_confusion_matrix(logreg, X_valid, y_valid, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI9j9NLoKzne"
      },
      "source": [
        "It is clear from the confusion matrix that the false positive rate has been reduced significantly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHMnnqq-m_8D",
        "outputId": "f238cf51-241f-460c-a90a-ec076f39202a"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using SMOTE is: 0.670\",\n",
        ")\n",
        "\n",
        "logreg_kaggle = 0.670"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using SMOTE is: 0.670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5b1UzpLP7w"
      },
      "source": [
        "The score on public leaderboard has not improved, however."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ufnRBwVrZY",
        "outputId": "5d356ea5-6c95-4155-edeb-7083f63df5f4"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression (using probabilities) is: 0.740\",\n",
        ")\n",
        "\n",
        "logreg_kaggle_prob = 0.740"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression (using probabilities) is: 0.740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c08wRUbzLo86"
      },
      "source": [
        "Submission of probabilities produced by the model has not improved significantly (0.01 to be precise)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpRK26wnnC6l"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpFhaxdHnDgD",
        "outputId": "81d23480-dc95-4da5-e78d-280af9f35197"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "score_gnb = get_score(gnb, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Gaussian Naive Bayes is: \",\n",
        "    \"{:.4f}\".format(score_gnb),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Gaussian Naive Bayes is:  0.8898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VomAnGyOCd-v",
        "outputId": "b884b08d-d4eb-4104-a3bd-b340b7321156"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling is: 0.548\",\n",
        ")\n",
        "\n",
        "gnb_kaggle = 0.548"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling is: 0.548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLUZx2_EnFCG"
      },
      "source": [
        "### K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hft4XkDGnHIi",
        "outputId": "1d893db6-6f4f-4c08-c4a7-3ad84099a270"
      },
      "source": [
        "knn = KNeighborsClassifier()\n",
        "\n",
        "score_knn = get_score(knn, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using K-Nearest Neighbors after oversampling is: \",\n",
        "    \"{:.4f}\".format(score_knn),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using K-Nearest Neighbors after oversampling is:  0.5062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEYkjT8ADwJp",
        "outputId": "ebe5ce49-694c-45e1-e261-de15cb5ab23a"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using K-Nearest Neighbors after oversampling is: 0.500\",\n",
        ")\n",
        "\n",
        "knn_kaggle = 0.500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using K-Nearest Neighbors after oversampling is: 0.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcm3GTwwnIku"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSmcRE1RnKUz",
        "outputId": "64422edc-7c76-4917-857a-07ab475d3944"
      },
      "source": [
        "svm = LinearSVC()\n",
        "\n",
        "score_svm = get_score(svm, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using LinearSVC is: \",\n",
        "    \"{:.4f}\".format(score_svm),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using LinearSVC is:  0.9154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NRZs0hEEMxV",
        "outputId": "11b34ebb-c2a2-4043-e436-2b64584bff0d"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using LinearSVC after oversampling is: 0.663\",\n",
        ")\n",
        "\n",
        "svc_kaggle = 0.663"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using LinearSVC after oversampling is: 0.663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPwwshwHW2CX"
      },
      "source": [
        "Submitting probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNPjkag4W2CZ",
        "outputId": "9962f98a-f20e-41f7-e88f-523d7813ce94"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using LinearSVC after oversampling is: 0.722\",\n",
        ")\n",
        "\n",
        "svm_kaggle_prob = 0.722"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using LinearSVC after oversampling is: 0.722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYlhOqtRnMVH"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFj-5NZFnOIm",
        "outputId": "e1cdddf0-9f24-4754-d112-ec54c493ee4e"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "score_rf = get_score(rf, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using RandomForrestClassifier after oversampling is: \",\n",
        "    \"{:.4f}\".format(score_rf),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using RandomForrestClassifier after oversampling is:  0.9071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07OpwxdbEuSo",
        "outputId": "7c8436ac-61ed-4057-83a2-7e3947e788c9"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using RandomForrestClassifier after oversampling is: 0.597\",\n",
        ")\n",
        "\n",
        "rf_over_kag = 0.597"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using RandomForrestClassifier after oversampling is: 0.597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1QWRUFJWpPn"
      },
      "source": [
        "Submitting probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN_O97gUWpPo",
        "outputId": "04e439c4-db80-48be-ed49-3849f1eabb7d"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Random Forrest Classifier after oversampling is (using probabilities): 0.694\",\n",
        ")\n",
        "\n",
        "rf_kaggle_prob = 0.694"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Random Forrest Classifier after oversampling is (using probabilities): 0.694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohHuQHeCNh64"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ZAlrEsYmNh65",
        "outputId": "4178a862-40ae-4cb0-9b75-3e2b80aa9f53"
      },
      "source": [
        "get_scoreboard(\n",
        "    logreg=True,\n",
        "    gnb=True,\n",
        "    knn=True,\n",
        "    svm=True,\n",
        "    rf=True\n",
        ").sort_values(by=\"Kaggle score\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Kaggle score</th>\n",
              "      <th>Kaggle score (proba)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.926367</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.915430</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.889844</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Random Forrest Classifier</td>\n",
              "      <td>0.907129</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>K-Nearest Neighbors</td>\n",
              "      <td>0.506250</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.635</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Model    ROCAUC  Kaggle score  Kaggle score (proba)\n",
              "0        Logistic Regression  0.926367         0.670                 0.740\n",
              "3     Support Vector Machine  0.915430         0.663                 0.722\n",
              "1       Gaussian Naive Bayes  0.889844         0.548                 0.740\n",
              "4  Random Forrest Classifier  0.907129         0.534                 0.694\n",
              "2        K-Nearest Neighbors  0.506250         0.500                 0.635"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l47-Jt9aNh65"
      },
      "source": [
        " - Application of SMOTE has significantly bolstered RSME score after cross-validation except for KNN, which shows lack of sensitivity to class imbalance. As such, it will not be tested further.\n",
        " - The number of negatives classified as positives (false positives) has been reduced significantly\n",
        " - The scores on public leaderboard has not been improved though.\n",
        " - Submitting probabilities still result in a higher score on Kaggle.\n",
        " - Random Forrest Classified has seen the largest jump in the ROCAUC. While insensitive to outliers, it seems to be very sensitive to class imbalance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FftkXpNXOPMH"
      },
      "source": [
        "## Iteration 2: oversampling using BorderlineSMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7APOeVsOSkN",
        "outputId": "860b889d-dbce-4479-e878-f3efa0e46c8b"
      },
      "source": [
        "oversample = BorderlineSMOTE(sampling_strategy=\"minority\")\n",
        "X_over, y_over = oversample.fit_resample(X, y)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_over, y_over, test_size=0.3)\n",
        "\n",
        "counter = Counter(y_over)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1.0: 160, 0.0: 160})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "OVQfemu-S4YB",
        "outputId": "11a14239-6b41-446a-b121-a15af285ca9a"
      },
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "plot_confusion_matrix(logreg, X_valid, y_valid, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHVs8_uSQQmL"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n03CGEGMPGk8",
        "outputId": "0f7e613d-db46-41ad-e88d-eb749c068594"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_logreg = get_score(logreg, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression after oversampling using BorderlineSMOTE is: \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression after oversampling using BorderlineSMOTE is:  0.9303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TkwVdETPGlY",
        "outputId": "fdf1ac5f-f4a1-47e1-eb80-030a5569a6d7"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using BorderlineSMOTE is: 0.675\",\n",
        ")\n",
        "\n",
        "logreg_kaggle = 0.675"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using BorderlineSMOTE is: 0.675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZg9x8c9bXg3",
        "outputId": "e46d58a6-952a-4698-c183-459384fbcae0"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using BorderlineSMOTE is: : 0.739\",\n",
        ")\n",
        "\n",
        "logreg_kaggle_prob = 0.739"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using BorderlineSMOTE is: : 0.739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH1RBW84P2dS"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t--c4FlMP2dl",
        "outputId": "caaba6ad-915c-4235-f1f5-ba5d9961892c"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "score_gnb = get_score(gnb, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using Gaussian Naive Bayes after oversampling using BorderlineSMOTE is: \",\n",
        "    \"{:.4f}\".format(score_gnb),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using Gaussian Naive Bayes after oversampling using BorderlineSMOTE is:  0.8924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvfweFiSP2dm",
        "outputId": "8db45b03-17d9-4037-d38b-a8bf7c450ad1"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling using BorderlineSMOTE is: 0.553\",\n",
        ")\n",
        "\n",
        "gnb_over_kag = 0.553"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling using BorderlineSMOTE is: 0.553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmsSKZ6_Pdpf"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD43pKlSPdpg",
        "outputId": "70bc772f-39f0-4248-99cf-52dc82048473"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "score_rf = get_score(rf, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using RandomForrestClassifier after oversampling using BorderlineSMOTE is: \",\n",
        "    \"{:.4f}\".format(score_rf),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using RandomForrestClassifier after oversampling using BorderlineSMOTE is:  0.8889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ5IztSXPdpg",
        "outputId": "2a38903c-dac1-469b-99c8-06ae7fd4e796"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using RandomForrestClassifier after oversampling is: 0.597\",\n",
        ")\n",
        "\n",
        "rf_over_kag = 0.597"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using RandomForrestClassifier after oversampling is: 0.597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tATLN8TcPdpg"
      },
      "source": [
        "Submitting probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjkAWMkIPdpg",
        "outputId": "aa112559-ff89-455b-d820-a32439ec3998"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Random Forrest Classifier after oversampling using BorderlineSMOTE is (using probabilities): 0.694\",\n",
        ")\n",
        "\n",
        "rf_kaggle_prob = 0.694"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Random Forrest Classifier after oversampling using BorderlineSMOTE is (using probabilities): 0.694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9LQPORsSaKf"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "6ctwMf7oPtQJ",
        "outputId": "844d2653-d23a-4148-9866-1f80389cc5c3"
      },
      "source": [
        "get_scoreboard(logreg=True, gnb=True, rf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Kaggle score</th>\n",
              "      <th>Kaggle score (proba)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.930273</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.892383</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forrest Classifier</td>\n",
              "      <td>0.888867</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Model    ROCAUC  Kaggle score  Kaggle score (proba)\n",
              "0        Logistic Regression  0.930273         0.675                 0.739\n",
              "1       Gaussian Naive Bayes  0.892383         0.548                 0.739\n",
              "2  Random Forrest Classifier  0.888867         0.534                 0.694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNRPbxtaSdi-"
      },
      "source": [
        "- Applying BorderlineSMOTE gave us even higher ROCAUC for cross-validation yet no improvement in scores on Kaggle leaderboard (neither by submitting binary values nor probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbzT8KXwRDAY"
      },
      "source": [
        "## Iteration 3: oversampling using SVMSMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twZHg4MZUthx"
      },
      "source": [
        "Another technique we will use to address the class inbalance in the data is SVMSMOTE. It differs from base SMOTE in that it uses SVM to find and generate the most simple datapoints to bring the numbers of the minority class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8n839AiVNfT",
        "outputId": "cbe4e151-6e56-4312-89d7-9af15e82c746"
      },
      "source": [
        "oversample = SVMSMOTE(sampling_strategy=\"minority\")\n",
        "X_over, y_over = oversample.fit_resample(X, y)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_over, y_over, test_size=0.3)\n",
        "\n",
        "counter = Counter(y_over)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({1.0: 160, 0.0: 160})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Tibbh3VoVNfT",
        "outputId": "774c9e4f-90bc-4baa-fcb3-f6240e812f3f"
      },
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "plot_confusion_matrix(logreg, X_valid, y_valid, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgFIkzFbRDAZ"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlYyKE5lRDAa",
        "outputId": "5e5fd27c-099c-4464-b56b-b512dc80b48e"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_logreg = get_score(logreg, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Logistic Regression after oversampling using SVMSMOTE is: \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Logistic Regression after oversampling using SVMSMOTE is:  0.9082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WosRnqvCRDAb",
        "outputId": "bb6e69d7-34a1-4052-ffcd-006e29f8d73b"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using SVMSMOTE is: 0.677\",\n",
        ")\n",
        "\n",
        "logreg_over_kag = 0.677"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using SVMSMOTE is: 0.677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9hsMoy6b7yl",
        "outputId": "c9189ddd-5dcf-456b-9e28-bc5465fca9b5"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using SVMSMOTE is: : 0.739\",\n",
        ")\n",
        "\n",
        "logreg_kaggle_prob = 0.739"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using SVMSMOTE is: : 0.739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjFaPT2ERDAc"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEVeY0YXRDAc",
        "outputId": "632779f5-5c57-4495-8586-f2af445031a1"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "score_gnb = get_score(gnb, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Gaussian Naive Bayes after oversampling using SVMSMOTE is: \",\n",
        "    \"{:.4f}\".format(score_gnb),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Gaussian Naive Bayes after oversampling using SVMSMOTE is:  0.7363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq0Zj9paRDAd",
        "outputId": "c2793fa2-42ed-4a12-ca80-9a06935aff5e"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling using SVMSMOTE is: 0.577\",\n",
        ")\n",
        "\n",
        "gnb_over_kag = 0.577"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling using SVMSMOTE is: 0.577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvQrxDyXVyOR"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QIXgiHmVyOS",
        "outputId": "d04b71f1-85ec-4dcb-c450-f2ea111afe3e"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "score_rf = get_score(rf, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using RandomForestClassifier after oversampling using SVMSMOTE is: \",\n",
        "    \"{:.4f}\".format(score_rf),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using RandomForestClassifier after oversampling using SVMSMOTE is:  0.8820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmYd1_PZVyOS",
        "outputId": "1777fd91-952c-47ff-cd5f-2be0cbcbbdfa"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using RandomForestClassifier after oversampling using SVMSMOTE is: 0.597\",\n",
        ")\n",
        "\n",
        "rf_over_kag = 0.597"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using RandomForestClassifier after oversampling using SVMSMOTE is: 0.597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkjLMf1tVyOT"
      },
      "source": [
        "Submitting probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlbF242iVyOT",
        "outputId": "c6f1e50d-2ed1-4867-f13d-e07aa8d8d127"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using RandomForestClassifier after oversampling using SVMSMOTE is (using probabilities): 0.694\"\n",
        ")\n",
        "\n",
        "rf_kaggle_prob = 0.694"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using RandomForestClassifier after oversampling using SVMSMOTE is (using probabilities): 0.694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUjbrKk4WBm8"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Hky9g11hWBm8",
        "outputId": "6090655f-e30b-4f0b-f0cf-32c09d17a989"
      },
      "source": [
        "get_scoreboard(logreg=True, gnb=True, rf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Kaggle score</th>\n",
              "      <th>Kaggle score (proba)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.908203</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.736328</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forrest Classifier</td>\n",
              "      <td>0.882031</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.694</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Model    ROCAUC  Kaggle score  Kaggle score (proba)\n",
              "0        Logistic Regression  0.908203         0.675                 0.739\n",
              "1       Gaussian Naive Bayes  0.736328         0.548                 0.739\n",
              "2  Random Forrest Classifier  0.882031         0.534                 0.694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9T_rGhyWBm9"
      },
      "source": [
        "- Applying SVMSMOTE resulted in a sligtly lower score for Logistic Regression, yet in a higher score for Random Forrest Classifier. Scores on public leaderboard have not been effected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BvOOANzSgTP"
      },
      "source": [
        "## Iteration 4: oversampling using ADASYN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a10EBfqWX6H"
      },
      "source": [
        "Final technique that we will used is ADASYN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM7CrbGkWdK2",
        "outputId": "9adc5477-2461-4959-93b0-69dc405ed9c9"
      },
      "source": [
        "oversample = ADASYN()\n",
        "X_over, y_over = oversample.fit_resample(X, y)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_over, y_over, test_size=0.3)\n",
        "\n",
        "counter = Counter(y_over)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.0: 175, 1.0: 160})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "nKtk2PGOWdK3",
        "outputId": "792b091c-a626-47d9-f74d-fb3634b46996"
      },
      "source": [
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "plot_confusion_matrix(logreg, X_valid, y_valid, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyZrsZMLWrbK"
      },
      "source": [
        "We see that ADESYN has balanced out the number of false positives and false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK3JUktLSyPe"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DesoyBEFSyPf",
        "outputId": "3c389c0a-fa47-4454-b67e-2c09475657a1"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "score_logreg = get_score(logreg, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using Logistic Regression after oversampling using ADESYN is: \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using Logistic Regression after oversampling using ADESYN is:  0.9525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bE-oawMSyPg",
        "outputId": "884779ba-a48c-4f3f-a24a-6c9a204c3e06"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using ADESYN is: 0.672\"\n",
        ")\n",
        "\n",
        "logreg_over_kag = 0.672"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using ADESYN is: 0.672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LufB-F-pcS5D",
        "outputId": "9e8a98fd-bfa3-44e1-dda0-a3c92a9d45a7"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Logistic Regression after oversampling using ADESYN is: 0.739\"\n",
        ")\n",
        "\n",
        "logreg_kaggle_prob = 0.739"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Logistic Regression after oversampling using ADESYN is: 0.739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABEKbMgeXOiG"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isB_gxh7XOiG",
        "outputId": "1adaca7e-0f7f-44e5-ff7e-224edadf0535"
      },
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "score_gnb = get_score(gnb, X_over, y_over)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using Gaussian Naive Bayes after oversampling using ADESYN is: \",\n",
        "    \"{:.4f}\".format(score_gnb),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using Gaussian Naive Bayes after oversampling using ADESYN is:  0.9079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kot6TFDsXOiG",
        "outputId": "202e3472-2bca-451f-9f69-06c24ce655df"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling using ADESYN is: 0.577\",\n",
        ")\n",
        "\n",
        "gnb_over_kag = 0.577"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Gaussian Naive Bayes after oversampling using ADESYN is: 0.577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL-ubyxPSyPh"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaQtCrEgSyPh",
        "outputId": "c67b1dc6-7834-4088-ffeb-4c0a5ae680d2"
      },
      "source": [
        "svm = LinearSVC()\n",
        "\n",
        "score_svm = get_score(svm, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using Support Vector Machine after oversampling using ADESYN is: \",\n",
        "    \"{:.4f}\".format(score_svm),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using Support Vector Machine after oversampling using ADESYN is:  0.9309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oIH1qdcSyPi",
        "outputId": "3e77f126-9eba-4e7f-c59b-928e6aac778d"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Suport Vector Machine  after oversampling using ADESYN is: 0.663\",\n",
        ")\n",
        "\n",
        "svm_over_kag = 0.663"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Suport Vector Machine  after oversampling using ADESYN is: 0.663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRXodJhxcpLG",
        "outputId": "3819841f-696b-4d9a-dbe2-e49cd4992ee5"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using LinearSVC after oversampling using ADESYN: 0.722\",\n",
        ")\n",
        "\n",
        "svm_kaggle_prob = 0.722"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using LinearSVC after oversampling using ADESYN: 0.722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulWi66_JXOiJ"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEfJh0p1XOiJ",
        "outputId": "0f27d140-c76a-479e-b08e-862cf3d62cab"
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "\n",
        "score_rf = get_score(rf, X_over, y_over)\n",
        "print(\n",
        "    \"The ROCAUC score using RandomForrestClassifier after oversampling using ADESYN is: \",\n",
        "    \"{:.4f}\".format(score_rf),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score using RandomForrestClassifier after oversampling using ADESYN is:  0.9416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWWnmfw1XOiK",
        "outputId": "e306de66-9391-41b1-bb62-60e832a43d68"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using RandomForrestClassifier after oversampling using ADESYN is: 0.613\",\n",
        ")\n",
        "\n",
        "rf_over_kag = 0.613"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using RandomForrestClassifier after oversampling using ADESYN is: 0.613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwm0c6R9XOiK",
        "outputId": "000cd365-237d-43dc-ce76-0136326e4661"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on public leaderbord using Random Forrest Classifier after oversampling using SVMSMOTE is (using probabilities): 0.687\",\n",
        ")\n",
        "\n",
        "rf_kaggle_prob = 0.687"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on public leaderbord using Random Forrest Classifier after oversampling using SVMSMOTE is (using probabilities): 0.687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TftaovrUXOiK"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "dexZMWDCXOiK",
        "outputId": "b6af56e0-33f0-466c-8e2e-5314008845fc"
      },
      "source": [
        "get_scoreboard(logreg=True, gnb=True, svm=True, rf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>ROCAUC</th>\n",
              "      <th>Kaggle score</th>\n",
              "      <th>Kaggle score (proba)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.952500</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.907857</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.930893</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forrest Classifier</td>\n",
              "      <td>0.941607</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Model    ROCAUC  Kaggle score  Kaggle score (proba)\n",
              "0        Logistic Regression  0.952500         0.675                 0.739\n",
              "1       Gaussian Naive Bayes  0.907857         0.548                 0.739\n",
              "2     Support Vector Machine  0.930893         0.663                 0.722\n",
              "3  Random Forrest Classifier  0.941607         0.534                 0.687"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCEjAUkMXOiK"
      },
      "source": [
        "- Applying ADESYN bolstered ROCAUC cross-validation scores for all the models significantly, yet it hasn't helped to achieve an improvement to the score on public scoreboards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUVyvmYZxV5"
      },
      "source": [
        "## Iteration 5: pruning features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsEGPjfyZ7z8"
      },
      "source": [
        "In what follows we will be conducting experiments to test what effect does lowering the number of features have on the our score. This will be done using [Rasgo](https://www.rasgoml.com/), a free python package that helps accelerating feature engineering by delivering actionable, time-saving data visualizations and feature analysis in a web-based user interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDJX-x-Jj7JL"
      },
      "source": [
        "### First experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVZeMAF6almg",
        "outputId": "d6c51174-66f4-4f81-8a1b-1ef93cb22c14"
      },
      "source": [
        "rasgo.activate_experiment('First Tutorial Experiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activated existing experiment with name First Tutorial Experiment for dataframe: i0RWdhvWWeo-mOJ8G-SxjK4B-ihEiw5QagOiReehvGU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfb1BUZAarSy",
        "outputId": "b6f4e2ff-e0f4-414f-dec8-5acb128c0d89"
      },
      "source": [
        "response = rasgo.evaluate.profile(train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Profile URL: https://app.rasgoml.com/dataframes/keeUAzNTHA4m_pN0o6PidXp7oN-rRCTGMMnkoPKUeuc/features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltDqmxsnaxrk",
        "outputId": "9070b29d-0185-424c-b59d-0216290e2f13"
      },
      "source": [
        "target = 'target'\n",
        "response = rasgo.evaluate.feature_importance(train_df, target_column=target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:10<00:09,  3.29s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/keeUAzNTHA4m_pN0o6PidXp7oN-rRCTGMMnkoPKUeuc/importance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1jM5tZHbymC",
        "outputId": "3d914916-1031-417a-da8b-91acdf9a9f68"
      },
      "source": [
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8026315789473685"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW1Se7q_dPpW",
        "outputId": "74ad17de-87a3-482b-890c-161ba8ae6027"
      },
      "source": [
        "df = rasgo.prune.features(train_df, target_column=target, top_n_pct=.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prune Method: Keeping top 0.5 of features\n",
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:11<00:10,  3.62s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/i0RWdhvWWeo-mOJ8G-SxjK4B-ihEiw5QagOiReehvGU/importance\n",
            "Dropped features not in top 0.5 pct: ['201', '123', '84', '292', '282', '85', '195', '248', '191', '142', '138', '284', '246', '241', '167', '131', '93', '259', '247', '29', '240', '198', '249', '161', '58', '43', '272', '137', '10', '61', '220', '178', '180', '18', '171', '88', '87', '56', '143', '81', '112', '20', '270', '197', '5', '188', '54', '281', '111', '211', '11', '75', '76', '121', '278', '215', '77', '94', '145', '202', '155', '22', '187', '273', '263', '296', '136', '98', '286', '139', '67', '277', '38', '17', '95', '196', '103', '109', '126', '110', '113', '294', '225', '128', '47', '49', '159', '14', '205', '66', '250', '21', '8', '40', '186', '251', '212', '28', '206', '216', '51', '185', '274', '35', '283', '7', '37', '153', '50', '207', '218', '158', '74', '271', '175', '291', '166', '146', '223', '299', '260', '257', '6', '122', '267', '36', '97', '190', '96', '55', '19', '44', '245', '184', '231', '130', '99', '239', '64', '213', '222', '135', '34', '236', '154', '70', '169', '275', '173', '25']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WtQRB44gVko",
        "outputId": "2507cf7b-2957-49a6-e612-907854f1cf94"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:09<00:08,  2.91s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/i0RWdhvWWeo-mOJ8G-SxjK4B-ihEiw5QagOiReehvGU/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8405103668261563"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKkZH1GjghVb"
      },
      "source": [
        "Keeping only a half of the features improved by score by 0.04. Let us removea further quarter of the remaining half of the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk35iIt9g4vO",
        "outputId": "ac565878-0866-4ff8-c69a-d23ea7107833"
      },
      "source": [
        "df = rasgo.prune.features(df, target_column=target, top_n_pct=.75)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prune Method: Keeping top 0.75 of features\n",
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:09<00:08,  2.95s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/i0RWdhvWWeo-mOJ8G-SxjK4B-ihEiw5QagOiReehvGU/importance\n",
            "Dropped features not in top 0.75 pct: ['129', '118', '264', '254', '71', '174', '140', '182', '163', '79', '269', '221', '1', '52', '224', '233', '256', '204', '193', '26', '3', '200', '13', '144', '172', '124', '41', '149', '234', '261', '242', '57', '23', '170', '290', '125', '2', '177']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-tVm3irhsqh",
        "outputId": "3ffecd2a-49b5-4532-b7b1-fd98122a9f4c"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:08<00:07,  2.61s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/i0RWdhvWWeo-mOJ8G-SxjK4B-ihEiw5QagOiReehvGU/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7159090909090909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhG7_kyFiA6W"
      },
      "source": [
        "It seems that once the reduced number of features (112) become smaller than a half of the total number of features (150), ROC starts decreasing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0O-PE0Ei1Z1",
        "outputId": "ed03f69f-7818-4c66-8893-9f4923a07a8e"
      },
      "source": [
        "rasgo.end_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment ended\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHSsHD5ojvYr"
      },
      "source": [
        "### Second experiment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3msj7N9I5OU"
      },
      "source": [
        "We finished last experiment by having our ROC to drop significantly once we dropped more than half the total number of features. However, we've made huge jumps as we switched from having 150 to only 112 features. When we're still with a half of features, how do we know how many more features do we have to drop to have our ROC improve further before it starts decreasing? \n",
        "\n",
        "This is exactly the goal of the present experiment. Once we're with half the features again, we will keep 99% of the remaining features ordered according to their importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kOLtyZei9t5",
        "outputId": "8b3c27b2-973a-48cd-c028-9c5d2ad59e65"
      },
      "source": [
        "rasgo.activate_experiment('Second Experiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activated new experiment with name Second Experiment for dataframe: 4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZjvBkj2jrQn",
        "outputId": "1ef6814e-90f0-4417-f611-d9d4614ec61c"
      },
      "source": [
        "response = rasgo.evaluate.profile(train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Profile URL: https://app.rasgoml.com/dataframes/4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A/features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi1GqBNmj-CD",
        "outputId": "df0a8342-f31f-42d0-ede1-681e367343ec"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(train_df, target_column=target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:09<00:09,  3.11s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A/importance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LUsxsc7kCtH",
        "outputId": "66bf3869-cca3-4b9e-94f3-9ce7f24861a7"
      },
      "source": [
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7841880341880342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDFT-l_1kG5k",
        "outputId": "42c50e83-c9c0-4d36-ddc9-434019a11ac4"
      },
      "source": [
        "df = rasgo.prune.features(train_df, target_column=target, top_n_pct=.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prune Method: Keeping top 0.5 of features\n",
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:10<00:09,  3.28s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A/importance\n",
            "Dropped features not in top 0.5 pct: ['222', '223', '224', '225', '286', '285', '229', '230', '232', '243', '172', '284', '235', '236', '237', '238', '283', '239', '240', '241', '242', '220', '150', '170', '59', '45', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '60', '43', '64', '66', '67', '68', '69', '71', '72', '73', '74', '75', '76', '77', '78', '44', '42', '81', '21', '2', '3', '6', '7', '8', '9', '10', '11', '12', '14', '15', '17', '19', '23', '41', '24', '25', '28', '29', '30', '31', '32', '34', '35', '36', '37', '38', '40', '80', '82', '169', '144', '127', '128', '129', '130', '132', '133', '135', '136', '137', '138', '139', '140', '143', '145', '123', '146', '149', '1', '151', '153', '154', '155', '157', '159', '160', '161', '162', '163', '124', '122', '83', '101', '84', '85', '87', '88', '89', '92', '93', '94', '96', '97', '98', '99', '100', '102', '121', '103', '107', '108', '109', '110', '111', '112', '115', '116', '117', '118', '119', '120', '299']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtCGn5LmkNvM",
        "outputId": "7678fa57-e926-4cae-9f1b-cd491d44dca7"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:08<00:07,  2.58s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7593582887700535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NbiRoqKcLHo"
      },
      "source": [
        "Due to stochastic nature of the default model that is used to produce feature importances and remove features accordingly, we end up with different sets of features that have been removed from the dataset. We will be addressing this issue shortly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzICzrtMkwaS",
        "outputId": "416735c8-ab55-4a41-de56-1a1a93b3919a"
      },
      "source": [
        "set1 = set(\n",
        "    [\n",
        "        \"201\",\n",
        "        \"123\",\n",
        "        \"84\",\n",
        "        \"292\",\n",
        "        \"282\",\n",
        "        \"85\",\n",
        "        \"195\",\n",
        "        \"248\",\n",
        "        \"191\",\n",
        "        \"142\",\n",
        "        \"138\",\n",
        "        \"284\",\n",
        "        \"246\",\n",
        "        \"241\",\n",
        "        \"167\",\n",
        "        \"131\",\n",
        "        \"93\",\n",
        "        \"259\",\n",
        "        \"247\",\n",
        "        \"29\",\n",
        "        \"240\",\n",
        "        \"198\",\n",
        "        \"249\",\n",
        "        \"161\",\n",
        "        \"58\",\n",
        "        \"43\",\n",
        "        \"272\",\n",
        "        \"137\",\n",
        "        \"10\",\n",
        "        \"61\",\n",
        "        \"220\",\n",
        "        \"178\",\n",
        "        \"180\",\n",
        "        \"18\",\n",
        "        \"171\",\n",
        "        \"88\",\n",
        "        \"87\",\n",
        "        \"56\",\n",
        "        \"143\",\n",
        "        \"81\",\n",
        "        \"112\",\n",
        "        \"20\",\n",
        "        \"270\",\n",
        "        \"197\",\n",
        "        \"5\",\n",
        "        \"188\",\n",
        "        \"54\",\n",
        "        \"281\",\n",
        "        \"111\",\n",
        "        \"211\",\n",
        "        \"11\",\n",
        "        \"75\",\n",
        "        \"76\",\n",
        "        \"121\",\n",
        "        \"278\",\n",
        "        \"215\",\n",
        "        \"77\",\n",
        "        \"94\",\n",
        "        \"145\",\n",
        "        \"202\",\n",
        "        \"155\",\n",
        "        \"22\",\n",
        "        \"187\",\n",
        "        \"273\",\n",
        "        \"263\",\n",
        "        \"296\",\n",
        "        \"136\",\n",
        "        \"98\",\n",
        "        \"286\",\n",
        "        \"139\",\n",
        "        \"67\",\n",
        "        \"277\",\n",
        "        \"38\",\n",
        "        \"17\",\n",
        "        \"95\",\n",
        "        \"196\",\n",
        "        \"103\",\n",
        "        \"109\",\n",
        "        \"126\",\n",
        "        \"110\",\n",
        "        \"113\",\n",
        "        \"294\",\n",
        "        \"225\",\n",
        "        \"128\",\n",
        "        \"47\",\n",
        "        \"49\",\n",
        "        \"159\",\n",
        "        \"14\",\n",
        "        \"205\",\n",
        "        \"66\",\n",
        "        \"250\",\n",
        "        \"21\",\n",
        "        \"8\",\n",
        "        \"40\",\n",
        "        \"186\",\n",
        "        \"251\",\n",
        "        \"212\",\n",
        "        \"28\",\n",
        "        \"206\",\n",
        "        \"216\",\n",
        "        \"51\",\n",
        "        \"185\",\n",
        "        \"274\",\n",
        "        \"35\",\n",
        "        \"283\",\n",
        "        \"7\",\n",
        "        \"37\",\n",
        "        \"153\",\n",
        "        \"50\",\n",
        "        \"207\",\n",
        "        \"218\",\n",
        "        \"158\",\n",
        "        \"74\",\n",
        "        \"271\",\n",
        "        \"175\",\n",
        "        \"291\",\n",
        "        \"166\",\n",
        "        \"146\",\n",
        "        \"223\",\n",
        "        \"299\",\n",
        "        \"260\",\n",
        "        \"257\",\n",
        "        \"6\",\n",
        "        \"122\",\n",
        "        \"267\",\n",
        "        \"36\",\n",
        "        \"97\",\n",
        "        \"190\",\n",
        "        \"96\",\n",
        "        \"55\",\n",
        "        \"19\",\n",
        "        \"44\",\n",
        "        \"245\",\n",
        "        \"184\",\n",
        "        \"231\",\n",
        "        \"130\",\n",
        "        \"99\",\n",
        "        \"239\",\n",
        "        \"64\",\n",
        "        \"213\",\n",
        "        \"222\",\n",
        "        \"135\",\n",
        "        \"34\",\n",
        "        \"236\",\n",
        "        \"154\",\n",
        "        \"70\",\n",
        "        \"169\",\n",
        "        \"275\",\n",
        "        \"173\",\n",
        "        \"25\",\n",
        "    ]\n",
        ")\n",
        "set2 = set(\n",
        "    [\n",
        "        \"222\",\n",
        "        \"223\",\n",
        "        \"224\",\n",
        "        \"225\",\n",
        "        \"286\",\n",
        "        \"285\",\n",
        "        \"229\",\n",
        "        \"230\",\n",
        "        \"232\",\n",
        "        \"243\",\n",
        "        \"172\",\n",
        "        \"284\",\n",
        "        \"235\",\n",
        "        \"236\",\n",
        "        \"237\",\n",
        "        \"238\",\n",
        "        \"283\",\n",
        "        \"239\",\n",
        "        \"240\",\n",
        "        \"241\",\n",
        "        \"242\",\n",
        "        \"220\",\n",
        "        \"150\",\n",
        "        \"170\",\n",
        "        \"59\",\n",
        "        \"45\",\n",
        "        \"47\",\n",
        "        \"48\",\n",
        "        \"49\",\n",
        "        \"50\",\n",
        "        \"51\",\n",
        "        \"52\",\n",
        "        \"53\",\n",
        "        \"54\",\n",
        "        \"55\",\n",
        "        \"56\",\n",
        "        \"57\",\n",
        "        \"58\",\n",
        "        \"60\",\n",
        "        \"43\",\n",
        "        \"64\",\n",
        "        \"66\",\n",
        "        \"67\",\n",
        "        \"68\",\n",
        "        \"69\",\n",
        "        \"71\",\n",
        "        \"72\",\n",
        "        \"73\",\n",
        "        \"74\",\n",
        "        \"75\",\n",
        "        \"76\",\n",
        "        \"77\",\n",
        "        \"78\",\n",
        "        \"44\",\n",
        "        \"42\",\n",
        "        \"81\",\n",
        "        \"21\",\n",
        "        \"2\",\n",
        "        \"3\",\n",
        "        \"6\",\n",
        "        \"7\",\n",
        "        \"8\",\n",
        "        \"9\",\n",
        "        \"10\",\n",
        "        \"11\",\n",
        "        \"12\",\n",
        "        \"14\",\n",
        "        \"15\",\n",
        "        \"17\",\n",
        "        \"19\",\n",
        "        \"23\",\n",
        "        \"41\",\n",
        "        \"24\",\n",
        "        \"25\",\n",
        "        \"28\",\n",
        "        \"29\",\n",
        "        \"30\",\n",
        "        \"31\",\n",
        "        \"32\",\n",
        "        \"34\",\n",
        "        \"35\",\n",
        "        \"36\",\n",
        "        \"37\",\n",
        "        \"38\",\n",
        "        \"40\",\n",
        "        \"80\",\n",
        "        \"82\",\n",
        "        \"169\",\n",
        "        \"144\",\n",
        "        \"127\",\n",
        "        \"128\",\n",
        "        \"129\",\n",
        "        \"130\",\n",
        "        \"132\",\n",
        "        \"133\",\n",
        "        \"135\",\n",
        "        \"136\",\n",
        "        \"137\",\n",
        "        \"138\",\n",
        "        \"139\",\n",
        "        \"140\",\n",
        "        \"143\",\n",
        "        \"145\",\n",
        "        \"123\",\n",
        "        \"146\",\n",
        "        \"149\",\n",
        "        \"1\",\n",
        "        \"151\",\n",
        "        \"153\",\n",
        "        \"154\",\n",
        "        \"155\",\n",
        "        \"157\",\n",
        "        \"159\",\n",
        "        \"160\",\n",
        "        \"161\",\n",
        "        \"162\",\n",
        "        \"163\",\n",
        "        \"124\",\n",
        "        \"122\",\n",
        "        \"83\",\n",
        "        \"101\",\n",
        "        \"84\",\n",
        "        \"85\",\n",
        "        \"87\",\n",
        "        \"88\",\n",
        "        \"89\",\n",
        "        \"92\",\n",
        "        \"93\",\n",
        "        \"94\",\n",
        "        \"96\",\n",
        "        \"97\",\n",
        "        \"98\",\n",
        "        \"99\",\n",
        "        \"100\",\n",
        "        \"102\",\n",
        "        \"121\",\n",
        "        \"103\",\n",
        "        \"107\",\n",
        "        \"108\",\n",
        "        \"109\",\n",
        "        \"110\",\n",
        "        \"111\",\n",
        "        \"112\",\n",
        "        \"115\",\n",
        "        \"116\",\n",
        "        \"117\",\n",
        "        \"118\",\n",
        "        \"119\",\n",
        "        \"120\",\n",
        "        \"299\",\n",
        "    ]\n",
        ")\n",
        "len(set1), len(set2), len(set1.difference(set2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 150, 68)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lshBwgGIa7r5"
      },
      "source": [
        "high_imp_features = [\n",
        "    \"182\",\n",
        "    \"266\",\n",
        "    \"160\",\n",
        "    \"114\",\n",
        "    \"162\",\n",
        "    \"298\",\n",
        "    \"150\",\n",
        "    \"41\",\n",
        "    \"125\",\n",
        "    \"256\",\n",
        "    \"238\",\n",
        "    \"79\",\n",
        "    \"34\",\n",
        "    \"86\",\n",
        "    \"290\",\n",
        "    \"175\",\n",
        "    \"242\",\n",
        "    \"134\",\n",
        "    \"223\",\n",
        "    \"281\",\n",
        "    \"229\",\n",
        "    \"178\",\n",
        "    \"3\",\n",
        "    \"136\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKtCcLF_kUcp",
        "outputId": "1910adf9-f13c-479b-c86b-247a8e352fe8"
      },
      "source": [
        "df = rasgo.prune.features(df, target_column=target, top_n_pct=.99)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prune Method: Keeping top 0.99 of features\n",
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:09<00:08,  2.98s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A/importance\n",
            "Dropped features not in top 0.99 pct: ['195', '218']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WhoVjdMlvBT",
        "outputId": "669fbe71-9d90-4bcd-abc7-29213bbcaf5a"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Feature Importance:  62%|██████▎   | 5/8 [00:08<00:08,  2.78s/step]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/4XRCUsFsLBJ_of-meWSZanwgZJg_OVL32tI97e9va_A/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7977777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvt99VufKCeq"
      },
      "source": [
        "We see that dropping a single percent of the features (two, to be exact) resulted in as huge an improvement of the score as initially dropping half the features resulted in. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaHGfZs8z0jW"
      },
      "source": [
        "## Iteration 6: using PyCaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbp_Spo3Ky6H"
      },
      "source": [
        "We have seen that the set of features that has been removed from the dataset during the initial experiment has resulted in a larger improvement of score than that during the second experiment. Let us call this `set1`. In what follows, we will throw everything that PyCaret has to offer at the part of features that remains after `set1` is removed from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCrFoT_0e_q"
      },
      "source": [
        "features_to_drop = set(\n",
        "    [\n",
        "        \"201\",\n",
        "        \"123\",\n",
        "        \"84\",\n",
        "        \"292\",\n",
        "        \"282\",\n",
        "        \"85\",\n",
        "        \"195\",\n",
        "        \"248\",\n",
        "        \"191\",\n",
        "        \"142\",\n",
        "        \"138\",\n",
        "        \"284\",\n",
        "        \"246\",\n",
        "        \"241\",\n",
        "        \"167\",\n",
        "        \"131\",\n",
        "        \"93\",\n",
        "        \"259\",\n",
        "        \"247\",\n",
        "        \"29\",\n",
        "        \"240\",\n",
        "        \"198\",\n",
        "        \"249\",\n",
        "        \"161\",\n",
        "        \"58\",\n",
        "        \"43\",\n",
        "        \"272\",\n",
        "        \"137\",\n",
        "        \"10\",\n",
        "        \"61\",\n",
        "        \"220\",\n",
        "        \"178\",\n",
        "        \"180\",\n",
        "        \"18\",\n",
        "        \"171\",\n",
        "        \"88\",\n",
        "        \"87\",\n",
        "        \"56\",\n",
        "        \"143\",\n",
        "        \"81\",\n",
        "        \"112\",\n",
        "        \"20\",\n",
        "        \"270\",\n",
        "        \"197\",\n",
        "        \"5\",\n",
        "        \"188\",\n",
        "        \"54\",\n",
        "        \"281\",\n",
        "        \"111\",\n",
        "        \"211\",\n",
        "        \"11\",\n",
        "        \"75\",\n",
        "        \"76\",\n",
        "        \"121\",\n",
        "        \"278\",\n",
        "        \"215\",\n",
        "        \"77\",\n",
        "        \"94\",\n",
        "        \"145\",\n",
        "        \"202\",\n",
        "        \"155\",\n",
        "        \"22\",\n",
        "        \"187\",\n",
        "        \"273\",\n",
        "        \"263\",\n",
        "        \"296\",\n",
        "        \"136\",\n",
        "        \"98\",\n",
        "        \"286\",\n",
        "        \"139\",\n",
        "        \"67\",\n",
        "        \"277\",\n",
        "        \"38\",\n",
        "        \"17\",\n",
        "        \"95\",\n",
        "        \"196\",\n",
        "        \"103\",\n",
        "        \"109\",\n",
        "        \"126\",\n",
        "        \"110\",\n",
        "        \"113\",\n",
        "        \"294\",\n",
        "        \"225\",\n",
        "        \"128\",\n",
        "        \"47\",\n",
        "        \"49\",\n",
        "        \"159\",\n",
        "        \"14\",\n",
        "        \"205\",\n",
        "        \"66\",\n",
        "        \"250\",\n",
        "        \"21\",\n",
        "        \"8\",\n",
        "        \"40\",\n",
        "        \"186\",\n",
        "        \"251\",\n",
        "        \"212\",\n",
        "        \"28\",\n",
        "        \"206\",\n",
        "        \"216\",\n",
        "        \"51\",\n",
        "        \"185\",\n",
        "        \"274\",\n",
        "        \"35\",\n",
        "        \"283\",\n",
        "        \"7\",\n",
        "        \"37\",\n",
        "        \"153\",\n",
        "        \"50\",\n",
        "        \"207\",\n",
        "        \"218\",\n",
        "        \"158\",\n",
        "        \"74\",\n",
        "        \"271\",\n",
        "        \"175\",\n",
        "        \"291\",\n",
        "        \"166\",\n",
        "        \"146\",\n",
        "        \"223\",\n",
        "        \"299\",\n",
        "        \"260\",\n",
        "        \"257\",\n",
        "        \"6\",\n",
        "        \"122\",\n",
        "        \"267\",\n",
        "        \"36\",\n",
        "        \"97\",\n",
        "        \"190\",\n",
        "        \"96\",\n",
        "        \"55\",\n",
        "        \"19\",\n",
        "        \"44\",\n",
        "        \"245\",\n",
        "        \"184\",\n",
        "        \"231\",\n",
        "        \"130\",\n",
        "        \"99\",\n",
        "        \"239\",\n",
        "        \"64\",\n",
        "        \"213\",\n",
        "        \"222\",\n",
        "        \"135\",\n",
        "        \"34\",\n",
        "        \"236\",\n",
        "        \"154\",\n",
        "        \"70\",\n",
        "        \"169\",\n",
        "        \"275\",\n",
        "        \"173\",\n",
        "        \"25\",\n",
        "    ]\n",
        ")\n",
        "X_copy = X.copy()\n",
        "X_copy = X_copy.drop(columns=features_to_drop)\n",
        "X_copy[\"target\"] = y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "900c9646e7154f41b19e56706a018018",
            "e1d56b32362d494db9c6f691358c2bfd",
            "5dba314e931446189e8cab20090eedf6",
            "e2654e38187c4bcf89b1e3b00f2ba271",
            "76500ead241642b6ab1dc77573b84f87",
            "5ace1c0922e64acdbfe731bfa8880b56"
          ]
        },
        "id": "WW1Df-7Uwext",
        "outputId": "06090c29-b0cc-4a17-8e43-dcb3e9acea26"
      },
      "source": [
        "pycaret_experiment = pycr.setup(data=X_copy, target=\"target\", session_id=1013)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>session_id</td>\n",
              "      <td>1013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Target</td>\n",
              "      <td>target</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Target Type</td>\n",
              "      <td>Binary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Label Encoded</td>\n",
              "      <td>0.0: 0, 1.0: 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Original Data</td>\n",
              "      <td>(250, 151)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Missing Values</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Numeric Features</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Categorical Features</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ordinal Features</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>High Cardinality Features</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>High Cardinality Method</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Transformed Train Set</td>\n",
              "      <td>(174, 150)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Transformed Test Set</td>\n",
              "      <td>(76, 150)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Shuffle Train-Test</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Stratify Train-Test</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Fold Generator</td>\n",
              "      <td>StratifiedKFold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Fold Number</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>CPU Jobs</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Use GPU</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Log Experiment</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Experiment Name</td>\n",
              "      <td>clf-default-name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>USI</td>\n",
              "      <td>ed97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Imputation Type</td>\n",
              "      <td>simple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Iterative Imputation Iteration</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Numeric Imputer</td>\n",
              "      <td>mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Iterative Imputation Numeric Model</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Categorical Imputer</td>\n",
              "      <td>constant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Iterative Imputation Categorical Model</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Unknown Categoricals Handling</td>\n",
              "      <td>least_frequent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Normalize</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Normalize Method</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Transformation</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Transformation Method</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>PCA</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>PCA Method</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>PCA Components</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Ignore Low Variance</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Combine Rare Levels</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Rare Level Threshold</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Numeric Binning</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Remove Outliers</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Outliers Threshold</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Remove Multicollinearity</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Multicollinearity Threshold</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Clustering</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Clustering Iteration</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Polynomial Features</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Polynomial Degree</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Trignometry Features</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Polynomial Threshold</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Group Features</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Feature Selection</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Feature Selection Method</td>\n",
              "      <td>classic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Features Selection Threshold</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Feature Interaction</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>Feature Ratio</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>Interaction Threshold</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Fix Imbalance</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>Fix Imbalance Method</td>\n",
              "      <td>SMOTE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               Description             Value\n",
              "0                               session_id              1013\n",
              "1                                   Target            target\n",
              "2                              Target Type            Binary\n",
              "3                            Label Encoded    0.0: 0, 1.0: 1\n",
              "4                            Original Data        (250, 151)\n",
              "5                           Missing Values             False\n",
              "6                         Numeric Features               150\n",
              "7                     Categorical Features                 0\n",
              "8                         Ordinal Features             False\n",
              "9                High Cardinality Features             False\n",
              "10                 High Cardinality Method              None\n",
              "11                   Transformed Train Set        (174, 150)\n",
              "12                    Transformed Test Set         (76, 150)\n",
              "13                      Shuffle Train-Test              True\n",
              "14                     Stratify Train-Test             False\n",
              "15                          Fold Generator   StratifiedKFold\n",
              "16                             Fold Number                10\n",
              "17                                CPU Jobs                -1\n",
              "18                                 Use GPU             False\n",
              "19                          Log Experiment             False\n",
              "20                         Experiment Name  clf-default-name\n",
              "21                                     USI              ed97\n",
              "22                         Imputation Type            simple\n",
              "23          Iterative Imputation Iteration              None\n",
              "24                         Numeric Imputer              mean\n",
              "25      Iterative Imputation Numeric Model              None\n",
              "26                     Categorical Imputer          constant\n",
              "27  Iterative Imputation Categorical Model              None\n",
              "28           Unknown Categoricals Handling    least_frequent\n",
              "29                               Normalize             False\n",
              "30                        Normalize Method              None\n",
              "31                          Transformation             False\n",
              "32                   Transformation Method              None\n",
              "33                                     PCA             False\n",
              "34                              PCA Method              None\n",
              "35                          PCA Components              None\n",
              "36                     Ignore Low Variance             False\n",
              "37                     Combine Rare Levels             False\n",
              "38                    Rare Level Threshold              None\n",
              "39                         Numeric Binning             False\n",
              "40                         Remove Outliers             False\n",
              "41                      Outliers Threshold              None\n",
              "42                Remove Multicollinearity             False\n",
              "43             Multicollinearity Threshold              None\n",
              "44                              Clustering             False\n",
              "45                    Clustering Iteration              None\n",
              "46                     Polynomial Features             False\n",
              "47                       Polynomial Degree              None\n",
              "48                    Trignometry Features             False\n",
              "49                    Polynomial Threshold              None\n",
              "50                          Group Features             False\n",
              "51                       Feature Selection             False\n",
              "52                Feature Selection Method           classic\n",
              "53            Features Selection Threshold              None\n",
              "54                     Feature Interaction             False\n",
              "55                           Feature Ratio             False\n",
              "56                   Interaction Threshold              None\n",
              "57                           Fix Imbalance             False\n",
              "58                    Fix Imbalance Method             SMOTE"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "fb87faf6930f47eea3f3593766da3b53",
            "ef6781d36c2e47d792b99317cce0a29a",
            "b9ce89eaa65c41b7953230d3357d986b"
          ]
        },
        "id": "70-faZNWwQcJ",
        "outputId": "32392115-e730-4b16-e498-54a3d73cde6f"
      },
      "source": [
        "best = pycr.compare_models(sort='AUC')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "      <th>TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lr</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.7353</td>\n",
              "      <td>0.8442</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7800</td>\n",
              "      <td>0.7925</td>\n",
              "      <td>0.4240</td>\n",
              "      <td>0.4386</td>\n",
              "      <td>0.295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.7458</td>\n",
              "      <td>0.8403</td>\n",
              "      <td>0.8364</td>\n",
              "      <td>0.7854</td>\n",
              "      <td>0.8067</td>\n",
              "      <td>0.4316</td>\n",
              "      <td>0.4436</td>\n",
              "      <td>0.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>et</th>\n",
              "      <td>Extra Trees Classifier</td>\n",
              "      <td>0.6435</td>\n",
              "      <td>0.7956</td>\n",
              "      <td>0.9727</td>\n",
              "      <td>0.6449</td>\n",
              "      <td>0.7753</td>\n",
              "      <td>0.0539</td>\n",
              "      <td>0.0797</td>\n",
              "      <td>0.465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>catboost</th>\n",
              "      <td>CatBoost Classifier</td>\n",
              "      <td>0.6722</td>\n",
              "      <td>0.7844</td>\n",
              "      <td>0.9727</td>\n",
              "      <td>0.6651</td>\n",
              "      <td>0.7895</td>\n",
              "      <td>0.1515</td>\n",
              "      <td>0.2369</td>\n",
              "      <td>28.078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rf</th>\n",
              "      <td>Random Forest Classifier</td>\n",
              "      <td>0.7072</td>\n",
              "      <td>0.7668</td>\n",
              "      <td>0.9818</td>\n",
              "      <td>0.6889</td>\n",
              "      <td>0.8093</td>\n",
              "      <td>0.2518</td>\n",
              "      <td>0.3344</td>\n",
              "      <td>0.504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lightgbm</th>\n",
              "      <td>Light Gradient Boosting Machine</td>\n",
              "      <td>0.6775</td>\n",
              "      <td>0.7394</td>\n",
              "      <td>0.8545</td>\n",
              "      <td>0.7071</td>\n",
              "      <td>0.7706</td>\n",
              "      <td>0.2392</td>\n",
              "      <td>0.2619</td>\n",
              "      <td>0.133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gbc</th>\n",
              "      <td>Gradient Boosting Classifier</td>\n",
              "      <td>0.6729</td>\n",
              "      <td>0.7346</td>\n",
              "      <td>0.8273</td>\n",
              "      <td>0.7085</td>\n",
              "      <td>0.7612</td>\n",
              "      <td>0.2503</td>\n",
              "      <td>0.2630</td>\n",
              "      <td>0.451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ada</th>\n",
              "      <td>Ada Boost Classifier</td>\n",
              "      <td>0.7003</td>\n",
              "      <td>0.7065</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7465</td>\n",
              "      <td>0.7685</td>\n",
              "      <td>0.3373</td>\n",
              "      <td>0.3425</td>\n",
              "      <td>0.171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>knn</th>\n",
              "      <td>K Neighbors Classifier</td>\n",
              "      <td>0.6892</td>\n",
              "      <td>0.6991</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7368</td>\n",
              "      <td>0.7649</td>\n",
              "      <td>0.3011</td>\n",
              "      <td>0.3078</td>\n",
              "      <td>0.117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lda</th>\n",
              "      <td>Linear Discriminant Analysis</td>\n",
              "      <td>0.6042</td>\n",
              "      <td>0.5793</td>\n",
              "      <td>0.6909</td>\n",
              "      <td>0.6831</td>\n",
              "      <td>0.6789</td>\n",
              "      <td>0.1510</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dt</th>\n",
              "      <td>Decision Tree Classifier</td>\n",
              "      <td>0.5559</td>\n",
              "      <td>0.5315</td>\n",
              "      <td>0.6273</td>\n",
              "      <td>0.6498</td>\n",
              "      <td>0.6339</td>\n",
              "      <td>0.0711</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qda</th>\n",
              "      <td>Quadratic Discriminant Analysis</td>\n",
              "      <td>0.4827</td>\n",
              "      <td>0.5171</td>\n",
              "      <td>0.3818</td>\n",
              "      <td>0.6557</td>\n",
              "      <td>0.4720</td>\n",
              "      <td>0.0297</td>\n",
              "      <td>0.0346</td>\n",
              "      <td>0.023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>svm</th>\n",
              "      <td>SVM - Linear Kernel</td>\n",
              "      <td>0.7245</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7733</td>\n",
              "      <td>0.7839</td>\n",
              "      <td>0.4034</td>\n",
              "      <td>0.4090</td>\n",
              "      <td>0.016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ridge</th>\n",
              "      <td>Ridge Classifier</td>\n",
              "      <td>0.6206</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.6727</td>\n",
              "      <td>0.7136</td>\n",
              "      <td>0.6840</td>\n",
              "      <td>0.2047</td>\n",
              "      <td>0.2046</td>\n",
              "      <td>0.019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
              "lr                    Logistic Regression    0.7353  0.8442  0.8182  0.7800   \n",
              "nb                            Naive Bayes    0.7458  0.8403  0.8364  0.7854   \n",
              "et                 Extra Trees Classifier    0.6435  0.7956  0.9727  0.6449   \n",
              "catboost              CatBoost Classifier    0.6722  0.7844  0.9727  0.6651   \n",
              "rf               Random Forest Classifier    0.7072  0.7668  0.9818  0.6889   \n",
              "lightgbm  Light Gradient Boosting Machine    0.6775  0.7394  0.8545  0.7071   \n",
              "gbc          Gradient Boosting Classifier    0.6729  0.7346  0.8273  0.7085   \n",
              "ada                  Ada Boost Classifier    0.7003  0.7065  0.8000  0.7465   \n",
              "knn                K Neighbors Classifier    0.6892  0.6991  0.8000  0.7368   \n",
              "lda          Linear Discriminant Analysis    0.6042  0.5793  0.6909  0.6831   \n",
              "dt               Decision Tree Classifier    0.5559  0.5315  0.6273  0.6498   \n",
              "qda       Quadratic Discriminant Analysis    0.4827  0.5171  0.3818  0.6557   \n",
              "svm                   SVM - Linear Kernel    0.7245  0.0000  0.8000  0.7733   \n",
              "ridge                    Ridge Classifier    0.6206  0.0000  0.6727  0.7136   \n",
              "\n",
              "              F1   Kappa     MCC  TT (Sec)  \n",
              "lr        0.7925  0.4240  0.4386     0.295  \n",
              "nb        0.8067  0.4316  0.4436     0.017  \n",
              "et        0.7753  0.0539  0.0797     0.465  \n",
              "catboost  0.7895  0.1515  0.2369    28.078  \n",
              "rf        0.8093  0.2518  0.3344     0.504  \n",
              "lightgbm  0.7706  0.2392  0.2619     0.133  \n",
              "gbc       0.7612  0.2503  0.2630     0.451  \n",
              "ada       0.7685  0.3373  0.3425     0.171  \n",
              "knn       0.7649  0.3011  0.3078     0.117  \n",
              "lda       0.6789  0.1510  0.1601     0.026  \n",
              "dt        0.6339  0.0711  0.0760     0.023  \n",
              "qda       0.4720  0.0297  0.0346     0.023  \n",
              "svm       0.7839  0.4034  0.4090     0.016  \n",
              "ridge     0.6840  0.2047  0.2046     0.019  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV2tumB1Ld_J"
      },
      "source": [
        "As we see, the most of the models that we have initially chosen are at the top when there's wider selection of models introduced. Let us further zoom in on the two algorithms that performed best, logistic regression and naive bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "QL11u719yPA_",
        "outputId": "34db6d34-0dac-4f02-ba83-7d213e97983b"
      },
      "source": [
        "nb = pycr.create_model('nb')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7222</td>\n",
              "      <td>0.7922</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7143</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.3662</td>\n",
              "      <td>0.3959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9481</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.8571</td>\n",
              "      <td>0.6582</td>\n",
              "      <td>0.6625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9351</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.8696</td>\n",
              "      <td>0.6400</td>\n",
              "      <td>0.6447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.8571</td>\n",
              "      <td>0.6582</td>\n",
              "      <td>0.6625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.8030</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>0.4848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.5882</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.6364</td>\n",
              "      <td>0.7000</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.1314</td>\n",
              "      <td>0.1324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.9394</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>0.4848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.8636</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7857</td>\n",
              "      <td>0.8800</td>\n",
              "      <td>0.5641</td>\n",
              "      <td>0.6268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.8030</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.6923</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.1639</td>\n",
              "      <td>0.1707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.6471</td>\n",
              "      <td>0.8788</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.6923</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.1639</td>\n",
              "      <td>0.1707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.7458</td>\n",
              "      <td>0.8403</td>\n",
              "      <td>0.8364</td>\n",
              "      <td>0.7854</td>\n",
              "      <td>0.8067</td>\n",
              "      <td>0.4316</td>\n",
              "      <td>0.4436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0864</td>\n",
              "      <td>0.1176</td>\n",
              "      <td>0.0891</td>\n",
              "      <td>0.0777</td>\n",
              "      <td>0.0638</td>\n",
              "      <td>0.2018</td>\n",
              "      <td>0.2053</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.7222  0.7922  0.9091  0.7143  0.8000  0.3662  0.3959\n",
              "1       0.8333  0.9481  0.8182  0.9000  0.8571  0.6582  0.6625\n",
              "2       0.8333  0.9351  0.9091  0.8333  0.8696  0.6400  0.6447\n",
              "3       0.8333  0.9091  0.8182  0.9000  0.8571  0.6582  0.6625\n",
              "4       0.7647  0.8030  0.8182  0.8182  0.8182  0.4848  0.4848\n",
              "5       0.5882  0.5303  0.6364  0.7000  0.6667  0.1314  0.1324\n",
              "6       0.7647  0.9394  0.8182  0.8182  0.8182  0.4848  0.4848\n",
              "7       0.8235  0.8636  1.0000  0.7857  0.8800  0.5641  0.6268\n",
              "8       0.6471  0.8030  0.8182  0.6923  0.7500  0.1639  0.1707\n",
              "9       0.6471  0.8788  0.8182  0.6923  0.7500  0.1639  0.1707\n",
              "Mean    0.7458  0.8403  0.8364  0.7854  0.8067  0.4316  0.4436\n",
              "SD      0.0864  0.1176  0.0891  0.0777  0.0638  0.2018  0.2053"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "VwgVhpSL2Kp7",
        "outputId": "56f15a77-6602-410c-f805-c28045968388"
      },
      "source": [
        "lr = pycr.create_model('lr')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7778</td>\n",
              "      <td>0.8831</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.5068</td>\n",
              "      <td>0.5230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9481</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7857</td>\n",
              "      <td>0.8800</td>\n",
              "      <td>0.6197</td>\n",
              "      <td>0.6701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.7922</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.2174</td>\n",
              "      <td>0.2548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.7222</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7619</td>\n",
              "      <td>0.4304</td>\n",
              "      <td>0.4332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.7727</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7619</td>\n",
              "      <td>0.3796</td>\n",
              "      <td>0.3825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.5882</td>\n",
              "      <td>0.6364</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.6316</td>\n",
              "      <td>0.1905</td>\n",
              "      <td>0.2031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.9697</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7424</td>\n",
              "      <td>0.7424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.8636</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.7826</td>\n",
              "      <td>0.3307</td>\n",
              "      <td>0.3337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.4426</td>\n",
              "      <td>0.4609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.8485</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7619</td>\n",
              "      <td>0.3796</td>\n",
              "      <td>0.3825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.7353</td>\n",
              "      <td>0.8442</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7800</td>\n",
              "      <td>0.7925</td>\n",
              "      <td>0.4240</td>\n",
              "      <td>0.4386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0794</td>\n",
              "      <td>0.0918</td>\n",
              "      <td>0.1286</td>\n",
              "      <td>0.0572</td>\n",
              "      <td>0.0734</td>\n",
              "      <td>0.1603</td>\n",
              "      <td>0.1613</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.7778  0.8831  0.9091  0.7692  0.8333  0.5068  0.5230\n",
              "1       0.8333  0.9481  1.0000  0.7857  0.8800  0.6197  0.6701\n",
              "2       0.6667  0.7922  0.9091  0.6667  0.7692  0.2174  0.2548\n",
              "3       0.7222  0.8182  0.7273  0.8000  0.7619  0.4304  0.4332\n",
              "4       0.7059  0.7727  0.7273  0.8000  0.7619  0.3796  0.3825\n",
              "5       0.5882  0.6364  0.5455  0.7500  0.6316  0.1905  0.2031\n",
              "6       0.8824  0.9697  0.9091  0.9091  0.9091  0.7424  0.7424\n",
              "7       0.7059  0.8636  0.8182  0.7500  0.7826  0.3307  0.3337\n",
              "8       0.7647  0.9091  0.9091  0.7692  0.8333  0.4426  0.4609\n",
              "9       0.7059  0.8485  0.7273  0.8000  0.7619  0.3796  0.3825\n",
              "Mean    0.7353  0.8442  0.8182  0.7800  0.7925  0.4240  0.4386\n",
              "SD      0.0794  0.0918  0.1286  0.0572  0.0734  0.1603  0.1613"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgrk7HsAMF0I"
      },
      "source": [
        "The models performed quite similarly with standard deviation of naive bayes over the series of 10 runs being a tad larger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "YGJFAIC_2OIf",
        "outputId": "256ea520-1116-43f0-e50a-876df03bc763"
      },
      "source": [
        "predict_result_nb = pycr.predict_model(nb, raw_score=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.8319</td>\n",
              "      <td>0.3689</td>\n",
              "      <td>0.409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Model  Accuracy   AUC  Recall  Prec.      F1   Kappa    MCC\n",
              "0  Naive Bayes      0.75  0.77    0.94  0.746  0.8319  0.3689  0.409"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Qzw1aoyI2R2k",
        "outputId": "b553afbc-5522-4233-b425-1432abe492d5"
      },
      "source": [
        "predict_result_lr = pycr.predict_model(lr, raw_score=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.7763</td>\n",
              "      <td>0.8508</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8381</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.4848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy     AUC  Recall  Prec.      F1  Kappa     MCC\n",
              "0  Logistic Regression    0.7763  0.8508    0.88    0.8  0.8381  0.479  0.4848"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "wX49CKHu2VKE",
        "outputId": "5689f8ab-8cdc-4295-cce4-e281a050fa50"
      },
      "source": [
        "nb = pycr.tune_model(nb, optimize = 'AUC')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7778</td>\n",
              "      <td>0.7922</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.5068</td>\n",
              "      <td>0.5230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9610</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.8571</td>\n",
              "      <td>0.6582</td>\n",
              "      <td>0.6625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9610</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.8696</td>\n",
              "      <td>0.6400</td>\n",
              "      <td>0.6447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.8571</td>\n",
              "      <td>0.6582</td>\n",
              "      <td>0.6625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.7879</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>0.4848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.5294</td>\n",
              "      <td>0.5303</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.6000</td>\n",
              "      <td>0.0423</td>\n",
              "      <td>0.0435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.9394</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>0.4848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.8636</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8462</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.7213</td>\n",
              "      <td>0.7511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.7826</td>\n",
              "      <td>0.3307</td>\n",
              "      <td>0.3337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.8696</td>\n",
              "      <td>0.5984</td>\n",
              "      <td>0.6039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.7748</td>\n",
              "      <td>0.8472</td>\n",
              "      <td>0.8364</td>\n",
              "      <td>0.8135</td>\n",
              "      <td>0.8222</td>\n",
              "      <td>0.5126</td>\n",
              "      <td>0.5195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0946</td>\n",
              "      <td>0.1225</td>\n",
              "      <td>0.1135</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0818</td>\n",
              "      <td>0.1912</td>\n",
              "      <td>0.1951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.7778  0.7922  0.9091  0.7692  0.8333  0.5068  0.5230\n",
              "1       0.8333  0.9610  0.8182  0.9000  0.8571  0.6582  0.6625\n",
              "2       0.8333  0.9610  0.9091  0.8333  0.8696  0.6400  0.6447\n",
              "3       0.8333  0.9091  0.8182  0.9000  0.8571  0.6582  0.6625\n",
              "4       0.7647  0.7879  0.8182  0.8182  0.8182  0.4848  0.4848\n",
              "5       0.5294  0.5303  0.5455  0.6667  0.6000  0.0423  0.0435\n",
              "6       0.7647  0.9394  0.8182  0.8182  0.8182  0.4848  0.4848\n",
              "7       0.8824  0.8636  1.0000  0.8462  0.9167  0.7213  0.7511\n",
              "8       0.7059  0.8182  0.8182  0.7500  0.7826  0.3307  0.3337\n",
              "9       0.8235  0.9091  0.9091  0.8333  0.8696  0.5984  0.6039\n",
              "Mean    0.7748  0.8472  0.8364  0.8135  0.8222  0.5126  0.5195\n",
              "SD      0.0946  0.1225  0.1135  0.0666  0.0818  0.1912  0.1951"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "dd4c9d7a7ad3466480b152c4d79e8277",
            "b5714e122792474c9feb195bdad117f7",
            "9e7c487d9de9490dbadced501edd1439"
          ]
        },
        "id": "XlC8ZzOY2YV7",
        "outputId": "072e3bc7-8a22-4e4e-af23-c5f9624af97d"
      },
      "source": [
        "lr = pycr.tune_model(lr, optimize = 'AUC')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7778</td>\n",
              "      <td>0.8701</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.5068</td>\n",
              "      <td>0.5230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.9481</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7857</td>\n",
              "      <td>0.8800</td>\n",
              "      <td>0.6197</td>\n",
              "      <td>0.6701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.7792</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.2174</td>\n",
              "      <td>0.2548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.7778</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.8889</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.5698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.7619</td>\n",
              "      <td>0.3796</td>\n",
              "      <td>0.3825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.5882</td>\n",
              "      <td>0.6364</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.6316</td>\n",
              "      <td>0.1905</td>\n",
              "      <td>0.2031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.8824</td>\n",
              "      <td>0.9697</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.7424</td>\n",
              "      <td>0.7424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.7059</td>\n",
              "      <td>0.8788</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.7826</td>\n",
              "      <td>0.3307</td>\n",
              "      <td>0.3337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.8235</td>\n",
              "      <td>0.9242</td>\n",
              "      <td>0.9091</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>0.8696</td>\n",
              "      <td>0.5984</td>\n",
              "      <td>0.6039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.7647</td>\n",
              "      <td>0.8485</td>\n",
              "      <td>0.7273</td>\n",
              "      <td>0.8889</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.5211</td>\n",
              "      <td>0.5367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.7526</td>\n",
              "      <td>0.8491</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.8042</td>\n",
              "      <td>0.8037</td>\n",
              "      <td>0.4662</td>\n",
              "      <td>0.4820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0828</td>\n",
              "      <td>0.0913</td>\n",
              "      <td>0.1286</td>\n",
              "      <td>0.0725</td>\n",
              "      <td>0.0743</td>\n",
              "      <td>0.1714</td>\n",
              "      <td>0.1708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.7778  0.8701  0.9091  0.7692  0.8333  0.5068  0.5230\n",
              "1       0.8333  0.9481  1.0000  0.7857  0.8800  0.6197  0.6701\n",
              "2       0.6667  0.7792  0.9091  0.6667  0.7692  0.2174  0.2548\n",
              "3       0.7778  0.8182  0.7273  0.8889  0.8000  0.5556  0.5698\n",
              "4       0.7059  0.8182  0.7273  0.8000  0.7619  0.3796  0.3825\n",
              "5       0.5882  0.6364  0.5455  0.7500  0.6316  0.1905  0.2031\n",
              "6       0.8824  0.9697  0.9091  0.9091  0.9091  0.7424  0.7424\n",
              "7       0.7059  0.8788  0.8182  0.7500  0.7826  0.3307  0.3337\n",
              "8       0.8235  0.9242  0.9091  0.8333  0.8696  0.5984  0.6039\n",
              "9       0.7647  0.8485  0.7273  0.8889  0.8000  0.5211  0.5367\n",
              "Mean    0.7526  0.8491  0.8182  0.8042  0.8037  0.4662  0.4820\n",
              "SD      0.0828  0.0913  0.1286  0.0725  0.0743  0.1714  0.1708"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Cf9vX5Md41"
      },
      "source": [
        "Although tuning of models didn't result in much improvement, we would nonetheless like to squeeze every decimal possible out of our score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "K2AH9RBH2bay",
        "outputId": "cee5a36a-3983-45da-8709-5d0a50a87242"
      },
      "source": [
        "tuned_result_nb = pycr.predict_model(nb, raw_score=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.7368</td>\n",
              "      <td>0.7862</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.376</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Model  Accuracy     AUC  Recall  Prec.      F1   Kappa    MCC\n",
              "0  Naive Bayes    0.7368  0.7862     0.9   0.75  0.8182  0.3559  0.376"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "1aktkDkj2dSG",
        "outputId": "536fba25-f3f9-4193-ce4a-9edf1b89f63a"
      },
      "source": [
        "tuned_result_lr = pycr.predict_model(lr, raw_score=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.8026</td>\n",
              "      <td>0.8492</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.8302</td>\n",
              "      <td>0.8544</td>\n",
              "      <td>0.5491</td>\n",
              "      <td>0.5513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy     AUC  ...      F1   Kappa     MCC\n",
              "0  Logistic Regression    0.8026  0.8492  ...  0.8544  0.5491  0.5513\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXegHvi52xo5"
      },
      "source": [
        "final_nb = pycr.finalize_model(nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2efTWUSH7EpO"
      },
      "source": [
        "final_lr = pycr.finalize_model(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWsUf_L97hpI"
      },
      "source": [
        "test_subm_copy = test_subm.copy()\n",
        "test_subm_copy = test_subm_copy.drop(columns=features_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_bWesc69I3f"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O1IPzJQ7Jq5"
      },
      "source": [
        "predictions_nb = pycr.predict_model(final_nb, data=test_subm_copy, raw_score=True)\n",
        "\n",
        "samp_subm = pd.read_csv(\"sample_submission.csv\")\n",
        "results = (\n",
        "    pd.DataFrame(predictions_nb[\"Score_1.0\"])\n",
        "    .set_index(samp_subm[\"id\"])\n",
        "    .rename(columns={\"Score_1.0\": \"target\"})\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJxXRRWH-P4o",
        "outputId": "7037eae6-1128-47fb-dbb3-b1cd599988e8"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC on public leaderboard on Kaggle for Naive Bayes after it has been selected by PyCaret is: 0.720\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC on public leaderboard on Kaggle for Naive Bayes after it has been selected by PyCaret is: 0.720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EkDyRjb9E-G"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OapxHx-d9HnN"
      },
      "source": [
        "predictions_lr = pycr.predict_model(final_lr, data=test_subm_copy, raw_score=True)\n",
        "\n",
        "results = (\n",
        "    pd.DataFrame(predictions_lr[\"Score_1.0\"])\n",
        "    .set_index(samp_subm[\"id\"])\n",
        "    .rename(columns={\"Score_1.0\": \"target\"})\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpA7OJxFlJjx",
        "outputId": "651943e5-deb2-44c8-a5e5-920eaa5a0365"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC on public leaderboard on Kaggle for Logistic Regression after it has been selected by PyCaret is: 0.759\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC on public leaderboard on Kaggle for Logistic Regression after it has been selected by PyCaret is: 0.759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzmECDgoNIdQ"
      },
      "source": [
        "We have achieved further improvement in score, however small (compare 0.740 which we achieved with logistic regression by using all the features for prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmIRTmIO9WD3"
      },
      "source": [
        "## Iteration 7: pruning down features using backward selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3VDCd9hNsQt"
      },
      "source": [
        "Since we're looking to push the score even further, we have no option but to try and further prune down the features to see whether that results in better score. For this we will use technique called *backward feature selection*. On each iteration, the feature that comes last in the list of features found to be most important by the model - i.e.most irrelevant features will be dropped. Then the feature importances will be calculated again and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T69SN3iOB1iV",
        "outputId": "6d8c3bb1-4a0c-472a-c178-73fead262a82"
      },
      "source": [
        "rasgo.activate_experiment('Third Experiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activated existing experiment with name Third Experiment for dataframe: fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsoH0NPZB9gC",
        "outputId": "61f7db9b-26ee-42fe-ec16-19e104c3f746"
      },
      "source": [
        "response = rasgo.evaluate.profile(train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Profile URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDHVpQR0CP27",
        "outputId": "53e95e2a-eed5-4c69-b620-ea669d3b8c7b"
      },
      "source": [
        "target = 'target'\n",
        "response = rasgo.evaluate.feature_importance(train_df, target_column=target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhfftb3CXtG",
        "outputId": "b1814860-11df-473a-c554-bc46da3c48a9"
      },
      "source": [
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.767379679144385"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "On6VySu1DIl5",
        "outputId": "680077be-9b00-4cad-93c6-1ba1027ed5b2"
      },
      "source": [
        "reduced_df = backward_selection(train_df, target, max_features=100)\n",
        "reduced_df.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8277777777777777 with 151\n",
            "Prune Method: Keeping top 150 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 150: ['200']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8131578947368421 with 150\n",
            "Prune Method: Keeping top 149 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 149: ['182']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.881578947368421 with 149\n",
            "Prune Method: Keeping top 148 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 148: ['210']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7983870967741935 with 148\n",
            "Prune Method: Keeping top 147 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 147: ['266']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8269230769230769 with 147\n",
            "Prune Method: Keeping top 146 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 146: ['52']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7520833333333333 with 146\n",
            "Prune Method: Keeping top 145 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 145: ['160']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8452380952380952 with 145\n",
            "Prune Method: Keeping top 144 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 144: ['167']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7843137254901961 with 144\n",
            "Prune Method: Keeping top 143 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 143: ['114']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9055555555555556 with 143\n",
            "Prune Method: Keeping top 142 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 142: ['293']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.742393509127789 with 142\n",
            "Prune Method: Keeping top 141 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 141: ['162']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8111888111888111 with 141\n",
            "Prune Method: Keeping top 140 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 140: ['21']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7521367521367521 with 140\n",
            "Prune Method: Keeping top 139 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 139: ['298']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7918367346938775 with 139\n",
            "Prune Method: Keeping top 138 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 138: ['150']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9268817204301075 with 138\n",
            "Prune Method: Keeping top 137 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 137: ['69']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8402777777777778 with 137\n",
            "Prune Method: Keeping top 136 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 136: ['50']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7857142857142857 with 136\n",
            "Prune Method: Keeping top 135 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 135: ['41']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9408284023668639 with 135\n",
            "Prune Method: Keeping top 134 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 134: ['61']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8520408163265306 with 134\n",
            "Prune Method: Keeping top 133 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 133: ['125']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9051282051282051 with 133\n",
            "Prune Method: Keeping top 132 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 132: ['142']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8595238095238096 with 132\n",
            "Prune Method: Keeping top 131 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 131: ['256']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8888888888888888 with 131\n",
            "Prune Method: Keeping top 130 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 130: ['94']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8441176470588235 with 130\n",
            "Prune Method: Keeping top 129 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 129: ['98']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7102272727272727 with 129\n",
            "Prune Method: Keeping top 128 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 128: ['238']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8185117967332124 with 128\n",
            "Prune Method: Keeping top 127 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 127: ['79']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8246527777777778 with 127\n",
            "Prune Method: Keeping top 126 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 126: ['34']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8591800356506238 with 126\n",
            "Prune Method: Keeping top 125 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 125: ['166']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7905982905982906 with 125\n",
            "Prune Method: Keeping top 124 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 124: ['124']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7848297213622291 with 124\n",
            "Prune Method: Keeping top 123 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 123: ['172']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7477313974591652 with 123\n",
            "Prune Method: Keeping top 122 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 122: ['86']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7874493927125507 with 122\n",
            "Prune Method: Keeping top 121 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 121: ['196']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7418831168831169 with 121\n",
            "Prune Method: Keeping top 120 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 120: ['290']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8007662835249042 with 120\n",
            "Prune Method: Keeping top 119 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 119: ['216']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.7923553719008265 with 119\n",
            "Prune Method: Keeping top 118 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 118: ['175']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9168356997971603 with 118\n",
            "Prune Method: Keeping top 117 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 117: ['215']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8294010889292196 with 117\n",
            "Prune Method: Keeping top 116 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 116: ['242']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8695652173913043 with 116\n",
            "Prune Method: Keeping top 115 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 115: ['53']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8421052631578947 with 115\n",
            "Prune Method: Keeping top 114 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 114: ['170']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.825 with 114\n",
            "Prune Method: Keeping top 113 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 113: ['203']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8287878787878787 with 113\n",
            "Prune Method: Keeping top 112 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 112: ['103']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8117647058823529 with 112\n",
            "Prune Method: Keeping top 111 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 111: ['134']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8378787878787879 with 111\n",
            "Prune Method: Keeping top 110 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 110: ['202']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8288888888888889 with 110\n",
            "Prune Method: Keeping top 109 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 109: ['223']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8571428571428571 with 109\n",
            "Prune Method: Keeping top 108 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 108: ['262']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.6380090497737556 with 108\n",
            "Prune Method: Keeping top 107 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 107: ['281']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8873563218390804 with 107\n",
            "Prune Method: Keeping top 106 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 106: ['229']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9333333333333333 with 106\n",
            "Prune Method: Keeping top 105 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 105: ['46']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9092592592592592 with 105\n",
            "Prune Method: Keeping top 104 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 104: ['220']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8133333333333334 with 104\n",
            "Prune Method: Keeping top 103 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 103: ['178']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8315972222222222 with 103\n",
            "Prune Method: Keeping top 102 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 102: ['254']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8 with 102\n",
            "Prune Method: Keeping top 101 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 101: ['3']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9155844155844156 with 101\n",
            "Prune Method: Keeping top 100 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 100: ['197']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8920145190562614 with 100\n",
            "Prune Method: Keeping top 99 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 99: ['136']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.9576719576719577 with 99\n",
            "Prune Method: Keeping top 98 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 98: ['111']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8347107438016529 with 98\n",
            "Prune Method: Keeping top 97 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 97: ['63']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8101851851851852 with 97\n",
            "Prune Method: Keeping top 96 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 96: ['47']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8058823529411765 with 96\n",
            "Prune Method: Keeping top 95 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 95: ['13']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8162393162393162 with 95\n",
            "Prune Method: Keeping top 94 features\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "Dropped features not in top 94: ['288']\n",
            "Importance URL: https://app.rasgoml.com/dataframes/fpiov3olrAQYLmDhUS8fQfTumPtM7BrMOi97PubtkCM/importance\n",
            "0.8209219858156028 with 94\n",
            "Prune Method: Keeping top 93 features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-3e1919ba811c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreduced_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreduced_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-69036898e903>\u001b[0m in \u001b[0;36mbackward_selection\u001b[0;34m(df, target, max_features)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_features\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrasgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrasgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_cli_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modelPerformance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AUC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Called {func.__name__} with parameters: {kwargs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to log event with exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/storage/dataframe/prune.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, df, target_column, timeseries_index, exclude_columns, top_n, top_n_pct, pct_of_top_feature)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Calculate feature_importance and return the result as a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mfi_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeseries_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeseries_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_cli_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mfi_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfi_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"featureImportance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mfi_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Feature'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfi_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Importance'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfi_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Called {func.__name__} with parameters: {kwargs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to log event with exception: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/storage/dataframe/evaluate.py\u001b[0m in \u001b[0;36mfeature_importance\u001b[0;34m(self, df, target_column, timeseries_index, exclude_columns, return_cli_only)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# Run profile for df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_cli_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Called {func.__name__} with parameters: {kwargs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/storage/dataframe/evaluate.py\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(self, df, exclude_columns, return_cli_only, timestamp_override)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mjson_payload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColumnProfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrasgo_df_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_payload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_cli_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Called {func.__name__} with parameters: {kwargs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/api/save.py\u001b[0m in \u001b[0;36mdataframe_profile\u001b[0;34m(self, id, payload)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# First update timestamp on the DF object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'edit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/utils/monitoring.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Called {func.__name__} with parameters: {kwargs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/api/create.py\u001b[0m in \u001b[0;36mdataframe_profile\u001b[0;34m(self, id, payload)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mSend\u001b[0m \u001b[0ma\u001b[0m \u001b[0mjson\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0mprofile\u001b[0m \u001b[0mso\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mWebApp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \"\"\"\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/dataframes/{id}/profile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyrasgo/api/connection.py\u001b[0m in \u001b[0;36m_post\u001b[0;34m(self, resource, _json, params, api_version)\u001b[0m\n\u001b[1;32m     97\u001b[0m                                  \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                  \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                  params=params or {})\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-e034LFOa52"
      },
      "source": [
        "While the score kept jumping up and down (most likely due to the stochastic nature of the algorithm and the fact the different features are found important on each run), the pruning procedure still allow us to pick out a handful of features that resulted in a boost in a performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go9zcbVKDfWZ",
        "outputId": "ad955be9-bf3f-4d9c-e0c0-f40894d0b126"
      },
      "source": [
        "high_imp_features = [\n",
        "    \"182\",\n",
        "    \"266\",\n",
        "    \"160\",\n",
        "    \"114\",\n",
        "    \"162\",\n",
        "    \"298\",\n",
        "    \"150\",\n",
        "    \"41\",\n",
        "    \"125\",\n",
        "    \"256\",\n",
        "    \"238\",\n",
        "    \"79\",\n",
        "    \"34\",\n",
        "    \"86\",\n",
        "    \"290\",\n",
        "    \"175\",\n",
        "    \"242\",\n",
        "    \"134\",\n",
        "    \"223\",\n",
        "    \"281\",\n",
        "    \"229\",\n",
        "    \"178\",\n",
        "    \"3\",\n",
        "    \"136\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 161;\n",
              "                var nbb_unformatted_code = \"high_imp_features = ['182', '266', '160', '114', '162', '298', '150', '41', '125', '256', '238', '79', '34', '86', '290', '175', '242', '134', '223', '281', '229', '178', '3', '136']  \";\n",
              "                var nbb_formatted_code = \"high_imp_features = [\\n    \\\"182\\\",\\n    \\\"266\\\",\\n    \\\"160\\\",\\n    \\\"114\\\",\\n    \\\"162\\\",\\n    \\\"298\\\",\\n    \\\"150\\\",\\n    \\\"41\\\",\\n    \\\"125\\\",\\n    \\\"256\\\",\\n    \\\"238\\\",\\n    \\\"79\\\",\\n    \\\"34\\\",\\n    \\\"86\\\",\\n    \\\"290\\\",\\n    \\\"175\\\",\\n    \\\"242\\\",\\n    \\\"134\\\",\\n    \\\"223\\\",\\n    \\\"281\\\",\\n    \\\"229\\\",\\n    \\\"178\\\",\\n    \\\"3\\\",\\n    \\\"136\\\",\\n]\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6lG36k2QvTz",
        "outputId": "0aa6fb3c-2ae4-4279-bfaf-2dc290273e2f"
      },
      "source": [
        "rasgo.end_experiment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment ended\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYiUfoHfRzXF"
      },
      "source": [
        "## Iteration 8: selectively dropping features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshVN2y4G3up"
      },
      "source": [
        "At this point we have three different sets of feature which have been dropped over the series of last three experiments. We would like to now to settle on one of these sets so that we can add newly engineered features to that set of features that leads to the highest score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E15rttvSA9c",
        "outputId": "6b63d181-140d-4739-9bf9-7479af698778"
      },
      "source": [
        "rasgo.activate_experiment('Fourth Experiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Activated new experiment with name Fourth Experiment for dataframe: UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fe2v8CjSA9d",
        "outputId": "0c39f24a-87dd-4650-f2a0-584f731c59dc"
      },
      "source": [
        "response = rasgo.evaluate.profile(train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Profile URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KQXp81sSA9d",
        "outputId": "15fc43f3-e838-4d14-87ba-353fb4a0ebd1"
      },
      "source": [
        "target = 'target'\n",
        "response = rasgo.evaluate.feature_importance(train_df, target_column=target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqe7CWEFSA9d",
        "outputId": "feac4ae7-c56d-490e-a644-b8f63d1d46c7"
      },
      "source": [
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7414529914529915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s2ARn3_UDCz"
      },
      "source": [
        "#### First set of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo3_pK_9SA9d"
      },
      "source": [
        "df = train_df.copy()\n",
        "df = df.drop(columns=list(set1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX9Y_zUCTPZ-",
        "outputId": "af9edf52-11bd-4d9a-dcb1-005da56bc7ef"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 151)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6qprcerSA9d",
        "outputId": "0ebc8c41-a8d3-455f-e2d9-da07dcd83c65"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8035087719298246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDl6niIomA26",
        "outputId": "eeebb294-0b1c-4256-8d1c-690fe26ce797"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on Kaggle public scoreboard using Linear Regression (with hyperparamters optimized) after dropping half of features: 0.813\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on Kaggle public scoreboard using Linear Regression (with hyperparamters optimized) after dropping half of features: 0.813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrS58a6KUHOl"
      },
      "source": [
        "#### Second set of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei_OI2vRUHOl"
      },
      "source": [
        "df = train_df.copy()\n",
        "df = df.drop(columns=list(set2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3OQ5ZybUHOl",
        "outputId": "b4ee730f-a3c4-47be-fc9e-8a78f1e16f65"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 151)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwIHzf6RUHOm",
        "outputId": "496371f1-de72-42dc-d799-083e374629b2"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8235294117647058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bdr5WT5m5Pn",
        "outputId": "e0fa7f17-dd3f-4317-d470-ee376c58a8b4"
      },
      "source": [
        "print(\n",
        "    \"The ROCAUC score on Kaggle public scoreboard using Logistic Regression (with hyperparameters optimized) after dropping half the features: 0.751\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROCAUC score on Kaggle public scoreboard using Logistic Regression (with hyperparameters optimized) after dropping half the features: 0.751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkOkAJYgH-PA"
      },
      "source": [
        "It turns out that dropping the second set of features actually reduces the score on public leaderboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvsd0pIjUmJL"
      },
      "source": [
        "#### Third set of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZujUUpBIJ3Y"
      },
      "source": [
        "Let us now try the set of features that we picked out from the backward selection procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0pOq6q2UmJL"
      },
      "source": [
        "df = train_df.copy()\n",
        "df = df.drop(columns=list(high_imp_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nGLwr1uUmJL",
        "outputId": "9b45f727-9a7f-4f4e-ebaf-b3a260ca2c3c"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 277)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCcsIngaUmJM",
        "outputId": "40408d02-9390-4789-9d0a-4501e4f9181d"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8281853281853282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpIVPZJMV7t6",
        "outputId": "b6d97882-e2d2-4595-be97-dca209ef5180"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) on a subset of features: 0.741\",\n",
        ")\n",
        "\n",
        "logreg_kaggle = 0.741"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) on a subset of features: 0.741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QJ9VGh6IYpk"
      },
      "source": [
        "As it turns out, the third set did even worse that the second one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hn_9jH4oSKa"
      },
      "source": [
        "###Fourth set of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKG-FY9xIgl4"
      },
      "source": [
        "Finally, let us give one last try with an union of features of the set that proved to be most effective and the features that were found important yet have not occured in the first set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPmEyqZ_bBV7"
      },
      "source": [
        "set1 = set(\n",
        "    [\n",
        "        \"201\",\n",
        "        \"123\",\n",
        "        \"84\",\n",
        "        \"292\",\n",
        "        \"282\",\n",
        "        \"85\",\n",
        "        \"195\",\n",
        "        \"248\",\n",
        "        \"191\",\n",
        "        \"142\",\n",
        "        \"138\",\n",
        "        \"284\",\n",
        "        \"246\",\n",
        "        \"241\",\n",
        "        \"167\",\n",
        "        \"131\",\n",
        "        \"93\",\n",
        "        \"259\",\n",
        "        \"247\",\n",
        "        \"29\",\n",
        "        \"240\",\n",
        "        \"198\",\n",
        "        \"249\",\n",
        "        \"161\",\n",
        "        \"58\",\n",
        "        \"43\",\n",
        "        \"272\",\n",
        "        \"137\",\n",
        "        \"10\",\n",
        "        \"61\",\n",
        "        \"220\",\n",
        "        \"178\",\n",
        "        \"180\",\n",
        "        \"18\",\n",
        "        \"171\",\n",
        "        \"88\",\n",
        "        \"87\",\n",
        "        \"56\",\n",
        "        \"143\",\n",
        "        \"81\",\n",
        "        \"112\",\n",
        "        \"20\",\n",
        "        \"270\",\n",
        "        \"197\",\n",
        "        \"5\",\n",
        "        \"188\",\n",
        "        \"54\",\n",
        "        \"281\",\n",
        "        \"111\",\n",
        "        \"211\",\n",
        "        \"11\",\n",
        "        \"75\",\n",
        "        \"76\",\n",
        "        \"121\",\n",
        "        \"278\",\n",
        "        \"215\",\n",
        "        \"77\",\n",
        "        \"94\",\n",
        "        \"145\",\n",
        "        \"202\",\n",
        "        \"155\",\n",
        "        \"22\",\n",
        "        \"187\",\n",
        "        \"273\",\n",
        "        \"263\",\n",
        "        \"296\",\n",
        "        \"136\",\n",
        "        \"98\",\n",
        "        \"286\",\n",
        "        \"139\",\n",
        "        \"67\",\n",
        "        \"277\",\n",
        "        \"38\",\n",
        "        \"17\",\n",
        "        \"95\",\n",
        "        \"196\",\n",
        "        \"103\",\n",
        "        \"109\",\n",
        "        \"126\",\n",
        "        \"110\",\n",
        "        \"113\",\n",
        "        \"294\",\n",
        "        \"225\",\n",
        "        \"128\",\n",
        "        \"47\",\n",
        "        \"49\",\n",
        "        \"159\",\n",
        "        \"14\",\n",
        "        \"205\",\n",
        "        \"66\",\n",
        "        \"250\",\n",
        "        \"21\",\n",
        "        \"8\",\n",
        "        \"40\",\n",
        "        \"186\",\n",
        "        \"251\",\n",
        "        \"212\",\n",
        "        \"28\",\n",
        "        \"206\",\n",
        "        \"216\",\n",
        "        \"51\",\n",
        "        \"185\",\n",
        "        \"274\",\n",
        "        \"35\",\n",
        "        \"283\",\n",
        "        \"7\",\n",
        "        \"37\",\n",
        "        \"153\",\n",
        "        \"50\",\n",
        "        \"207\",\n",
        "        \"218\",\n",
        "        \"158\",\n",
        "        \"74\",\n",
        "        \"271\",\n",
        "        \"175\",\n",
        "        \"291\",\n",
        "        \"166\",\n",
        "        \"146\",\n",
        "        \"223\",\n",
        "        \"299\",\n",
        "        \"260\",\n",
        "        \"257\",\n",
        "        \"6\",\n",
        "        \"122\",\n",
        "        \"267\",\n",
        "        \"36\",\n",
        "        \"97\",\n",
        "        \"190\",\n",
        "        \"96\",\n",
        "        \"55\",\n",
        "        \"19\",\n",
        "        \"44\",\n",
        "        \"245\",\n",
        "        \"184\",\n",
        "        \"231\",\n",
        "        \"130\",\n",
        "        \"99\",\n",
        "        \"239\",\n",
        "        \"64\",\n",
        "        \"213\",\n",
        "        \"222\",\n",
        "        \"135\",\n",
        "        \"34\",\n",
        "        \"236\",\n",
        "        \"154\",\n",
        "        \"70\",\n",
        "        \"169\",\n",
        "        \"275\",\n",
        "        \"173\",\n",
        "        \"25\",\n",
        "    ]\n",
        ")\n",
        "set2 = set(\n",
        "    [\n",
        "        \"222\",\n",
        "        \"223\",\n",
        "        \"224\",\n",
        "        \"225\",\n",
        "        \"286\",\n",
        "        \"285\",\n",
        "        \"229\",\n",
        "        \"230\",\n",
        "        \"232\",\n",
        "        \"243\",\n",
        "        \"172\",\n",
        "        \"284\",\n",
        "        \"235\",\n",
        "        \"236\",\n",
        "        \"237\",\n",
        "        \"238\",\n",
        "        \"283\",\n",
        "        \"239\",\n",
        "        \"240\",\n",
        "        \"241\",\n",
        "        \"242\",\n",
        "        \"220\",\n",
        "        \"150\",\n",
        "        \"170\",\n",
        "        \"59\",\n",
        "        \"45\",\n",
        "        \"47\",\n",
        "        \"48\",\n",
        "        \"49\",\n",
        "        \"50\",\n",
        "        \"51\",\n",
        "        \"52\",\n",
        "        \"53\",\n",
        "        \"54\",\n",
        "        \"55\",\n",
        "        \"56\",\n",
        "        \"57\",\n",
        "        \"58\",\n",
        "        \"60\",\n",
        "        \"43\",\n",
        "        \"64\",\n",
        "        \"66\",\n",
        "        \"67\",\n",
        "        \"68\",\n",
        "        \"69\",\n",
        "        \"71\",\n",
        "        \"72\",\n",
        "        \"73\",\n",
        "        \"74\",\n",
        "        \"75\",\n",
        "        \"76\",\n",
        "        \"77\",\n",
        "        \"78\",\n",
        "        \"44\",\n",
        "        \"42\",\n",
        "        \"81\",\n",
        "        \"21\",\n",
        "        \"2\",\n",
        "        \"3\",\n",
        "        \"6\",\n",
        "        \"7\",\n",
        "        \"8\",\n",
        "        \"9\",\n",
        "        \"10\",\n",
        "        \"11\",\n",
        "        \"12\",\n",
        "        \"14\",\n",
        "        \"15\",\n",
        "        \"17\",\n",
        "        \"19\",\n",
        "        \"23\",\n",
        "        \"41\",\n",
        "        \"24\",\n",
        "        \"25\",\n",
        "        \"28\",\n",
        "        \"29\",\n",
        "        \"30\",\n",
        "        \"31\",\n",
        "        \"32\",\n",
        "        \"34\",\n",
        "        \"35\",\n",
        "        \"36\",\n",
        "        \"37\",\n",
        "        \"38\",\n",
        "        \"40\",\n",
        "        \"80\",\n",
        "        \"82\",\n",
        "        \"169\",\n",
        "        \"144\",\n",
        "        \"127\",\n",
        "        \"128\",\n",
        "        \"129\",\n",
        "        \"130\",\n",
        "        \"132\",\n",
        "        \"133\",\n",
        "        \"135\",\n",
        "        \"136\",\n",
        "        \"137\",\n",
        "        \"138\",\n",
        "        \"139\",\n",
        "        \"140\",\n",
        "        \"143\",\n",
        "        \"145\",\n",
        "        \"123\",\n",
        "        \"146\",\n",
        "        \"149\",\n",
        "        \"1\",\n",
        "        \"151\",\n",
        "        \"153\",\n",
        "        \"154\",\n",
        "        \"155\",\n",
        "        \"157\",\n",
        "        \"159\",\n",
        "        \"160\",\n",
        "        \"161\",\n",
        "        \"162\",\n",
        "        \"163\",\n",
        "        \"124\",\n",
        "        \"122\",\n",
        "        \"83\",\n",
        "        \"101\",\n",
        "        \"84\",\n",
        "        \"85\",\n",
        "        \"87\",\n",
        "        \"88\",\n",
        "        \"89\",\n",
        "        \"92\",\n",
        "        \"93\",\n",
        "        \"94\",\n",
        "        \"96\",\n",
        "        \"97\",\n",
        "        \"98\",\n",
        "        \"99\",\n",
        "        \"100\",\n",
        "        \"102\",\n",
        "        \"121\",\n",
        "        \"103\",\n",
        "        \"107\",\n",
        "        \"108\",\n",
        "        \"109\",\n",
        "        \"110\",\n",
        "        \"111\",\n",
        "        \"112\",\n",
        "        \"115\",\n",
        "        \"116\",\n",
        "        \"117\",\n",
        "        \"118\",\n",
        "        \"119\",\n",
        "        \"120\",\n",
        "        \"299\",\n",
        "    ]\n",
        ")\n",
        "high_imp_features = [\n",
        "    \"182\",\n",
        "    \"266\",\n",
        "    \"160\",\n",
        "    \"114\",\n",
        "    \"162\",\n",
        "    \"298\",\n",
        "    \"150\",\n",
        "    \"41\",\n",
        "    \"125\",\n",
        "    \"256\",\n",
        "    \"238\",\n",
        "    \"79\",\n",
        "    \"34\",\n",
        "    \"86\",\n",
        "    \"290\",\n",
        "    \"175\",\n",
        "    \"242\",\n",
        "    \"134\",\n",
        "    \"223\",\n",
        "    \"281\",\n",
        "    \"229\",\n",
        "    \"178\",\n",
        "    \"3\",\n",
        "    \"136\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjgseMxmoU0T"
      },
      "source": [
        "difference = set1.difference(high_imp_features)\n",
        "set1_extended = set1 | difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_tTtr1soZD5",
        "outputId": "40408d02-9390-4789-9d0a-4501e4f9181d"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8281853281853282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q93n1FtpoZD6",
        "outputId": "b4c8dd50-ea10-401c-a36e-bb5dd8719936"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) on a subset of features: 0.813\",\n",
        ")\n",
        "\n",
        "logreg_kaggle = 0.813"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using Logistic Regression (with hyperparameters optimized) on a subset of features: 0.813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azn9uQAwJTcP"
      },
      "source": [
        "Despite the fact the score hasn't improved, the more features dropped - the better, hence we stick with the union in what follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcTQ2BT7WeM-"
      },
      "source": [
        "### Engineering features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRri17HkPALv"
      },
      "source": [
        "##### Second-degree polynomials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ythVkg8VJ1rq"
      },
      "source": [
        "Due to the fact that features are anonymized and as a result there can be no domain knowledge that we could use to our benefit, we start engineering new features first by adding second-degree polynomials of the features that have repeatedly found to be most important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-qxD-UZaAtg"
      },
      "source": [
        "df = train_df.copy()\n",
        "df = df.drop(columns=list(set1_extended))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6YLzgUYaHub"
      },
      "source": [
        "df[\"33**2\"] = df[\"33\"] ** 2\n",
        "df[\"65**2\"] = df[\"65\"] ** 2\n",
        "df[\"117**2\"] = df[\"117\"] ** 2\n",
        "df[\"80**2\"] = df[\"80\"] ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZFUPF9DaAth",
        "outputId": "7fe0116b-9c15-48cb-e557-1be0cf9f19ca"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8552631578947368"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RysGKyCqQ8H",
        "outputId": "a0d2bff5-8f45-42bf-a5a5-fbbc0bbbd037"
      },
      "source": [
        "print(\n",
        "    \"Adding second-degree polynomials after having half the features dropped resulted in 0.816 score on Kaggle public leaderboard\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding second-degree polynomials after having half the features dropped resulted in 0.816 score on Kaggle public leaderboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SugHSKZQKUug"
      },
      "source": [
        "...and an improvement of 0.03 on private scoreboard (0.793). This is the score that is going to be our main target, since we have already reached the required score on public scoreboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-RuvKswcE_e"
      },
      "source": [
        "##### Cross products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHCdjykKpWR"
      },
      "source": [
        "Let us now add feature crosses of the same features. Using feature crosses might turn out effective in case there are non-linear relationships in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9qjf2kubMwK"
      },
      "source": [
        "best_predictors = ['33', '65', '117', '80']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpIPMCi3doMg"
      },
      "source": [
        "for c1 in best_predictors:\n",
        "    for c2 in best_predictors:\n",
        "        c_new = f\"{c1}*{c2}\"\n",
        "        df.insert(len(df.columns), c_new, df[c1] * df[c2])\n",
        "        test_df_copy.insert(\n",
        "            len(test_df_copy.columns), c_new, test_df_copy[c1] * test_df_copy[c2]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apuDdtnvdoMh",
        "outputId": "2ed06a42-6525-43b1-a3b6-d78ffbac5fcf"
      },
      "source": [
        "response = rasgo.evaluate.feature_importance(df, target_column=target)\n",
        "response['modelPerformance']['AUC']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importance URL: https://app.rasgoml.com/dataframes/UyOAYCYgNcF7-AkmLshMqz7wQIb0VpAaTUlv49M6JNc/importance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7655913978494624"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIjzD-dFu0xc",
        "outputId": "d0f1c1f5-28b7-44a3-8e0a-9b79c748de6a"
      },
      "source": [
        "print(\n",
        "    \"Adding featre crosses after having half the features dropped resulted in 0.815 score on Kaggle public leaderboard\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding featre crosses after having half the features dropped resulted in 0.815 score on Kaggle public leaderboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwdADswRLD-h"
      },
      "source": [
        "Adding feature crosses had almost no effect on neither of the scores on Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCmefd6PvBYE"
      },
      "source": [
        "##### Adding more polynomials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPK3AmJvBYE"
      },
      "source": [
        "df = train_df.copy()\n",
        "df = df.drop(columns=list(set1_extended))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpcUvSVmvBYF",
        "outputId": "4291c0a7-3a93-4be5-caf0-744c4150aeb9"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 151)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0EkE1S9vBYF"
      },
      "source": [
        "poly = PolynomialFeatures(4, include_bias=False)\n",
        "polies = poly.fit_transform(df[best_predictors])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsPJDeDEwbEj"
      },
      "source": [
        "polies = pd.DataFrame(polies)\n",
        "polies.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRo7JQOOwYXP",
        "outputId": "2c5613e5-0298-46e1-a500-936804521ed3"
      },
      "source": [
        "df_new = df.join(polies)\n",
        "df_new.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 220)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRz8sptTvBYH"
      },
      "source": [
        "test_df_copy = test_subm.copy()\n",
        "test_df_copy = test_df_copy.drop(set1_extended, axis=1)\n",
        "polies_test = pd.DataFrame(poly.fit_transform(test_df_copy[best_predictors]))\n",
        "test_df_copy = test_df_copy.join(polies_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHtpFs9PvBYH",
        "outputId": "bb1344b7-ef9e-4930-cfff-9a7ed2fbb7b9"
      },
      "source": [
        "test_df_copy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19750, 219)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnRkyWW-vBYN",
        "outputId": "b28d5a48-64ba-4d6d-e1b3-4e045da6a5df"
      },
      "source": [
        "print(\n",
        "    \"Adding second-degree polynomials after having half the features dropped resulted in 0.784 score on Kaggle public leaderboard\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding second-degree polynomials after having half the features dropped resulted in 0.784 score on Kaggle public leaderboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qFwbcQLlD0"
      },
      "source": [
        "Adding second degree polynomials to the halved set of features didn't turn out well. Both private and public scores have become worse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhs4mNI7ZNv1"
      },
      "source": [
        "## Iteration 9: looking for that final push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfFVfaV0tcdX"
      },
      "source": [
        "After experimenting with principal components (experiments the results of which we have records on Kaggle yet which we decided to exclude from the final notebook as there are enough failed experiments as it is) we are becoming quite desperate for the final push in our private scoreboard. Let us borrow the idea of adding noise the the training data as an attempt to reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohCTTnCOTLNg"
      },
      "source": [
        "First let us put all the steps that lead to the improvement of score into classes so they can all be put into a pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atVVSG6RPslD"
      },
      "source": [
        "class HalfFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Removes a subset of features from the data. The removal of the this set of features (which is taken to be fixed) has proved to gave the best score.\n",
        "\n",
        "    :param x: the dataset which is to have half of its features removed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, removed_features: set = set1_extended) -> None:\n",
        "        self.removed_features = removed_features\n",
        "        pass\n",
        "\n",
        "    def fit(self, x: pd.DataFrame, y: pd.Series = None) -> None:\n",
        "        return self\n",
        "\n",
        "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = x.copy()\n",
        "        df = df.drop(columns=list(self.removed_features))\n",
        "\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qif0vFwjRHUO"
      },
      "source": [
        "class SecondPolynomials(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Removes a subset of features from the data. The removal of the this set of features (which is taken to be fixed) has proved to gave the best score.\n",
        "\n",
        "    :param x: the dataset which is to have half of its features removed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feats_for_polies: list = best_predictors) -> None:\n",
        "        self.feats_for_polies = feats_for_polies\n",
        "        pass\n",
        "\n",
        "    def fit(self, x: pd.DataFrame, y: pd.Series = None) -> None:\n",
        "        return self\n",
        "\n",
        "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = x.copy()\n",
        "        for feat in self.feats_for_polies:\n",
        "            name = f\"{feat}**2\"\n",
        "            df[name] = df[feat] ** 2\n",
        "\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaoq5B8LSKOA"
      },
      "source": [
        "class Noiser(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Adds noise the the dataset by randomly sampling from a Gaussian distribution with a mean of 0 and a standard deviation the value of which is inputted.\n",
        "\n",
        "    :param x: the dataset which is to have noise added to its features\n",
        "    :return df: the dataset with noise added\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, noise_std: float = 0.01) -> None:\n",
        "        self.noise_std = noise_std\n",
        "        pass\n",
        "\n",
        "    def fit(self, x: pd.DataFrame, y: pd.Series = None) -> None:\n",
        "        return self\n",
        "\n",
        "    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = x.copy()\n",
        "        df += np.random.normal(0, self.noise_std, df.shape)\n",
        "\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN-fw4Ueewe3"
      },
      "source": [
        "ee = EllipticEnvelope(contamination=0.01)\n",
        "yhat = ee.fit_predict(X)\n",
        "mask = yhat != -1\n",
        "X_out, y_out = X[mask], y[mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSBWZfJEgMrr"
      },
      "source": [
        "logreg_opt = LogisticRegression(\n",
        "    C=0.8,\n",
        "    class_weight=\"balanced\",\n",
        "    fit_intercept=True,\n",
        "    penalty=\"l1\",\n",
        "    solver=\"liblinear\",\n",
        "    tol=0.0001,\n",
        ")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"adasyn\", ADASYN()),\n",
        "        (\"polynomials\", SecondPolynomials()),\n",
        "        (\"half_features\", HalfFeatures()),\n",
        "        (\"noise\", Noiser()),\n",
        "        (\"scaler\", RobustScaler()),\n",
        "        (\"model\", logreg_opt),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2BnGBhiUFG2",
        "outputId": "5f3b4601-171d-4d28-82b5-13227fa5f7cc"
      },
      "source": [
        "score_pipe = get_score(pipe, X_out, y_out)\n",
        "print(\n",
        "    \"using Logistic Regression (with hyperparamaters optimized) on public leaderboard after having all the previous steps combined and noise added: \",\n",
        "    \"{:.4f}\".format(score_pipe),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using Logistic Regression (with hyperparamaters optimized) on public leaderboard after having all the previous steps combined and noise added:  0.8522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOTrizu6Tepf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2db804-4ae9-4730-b8b7-a1f076faf75f"
      },
      "source": [
        "print(\n",
        "    \"The ROC score using Logistic Regression (with hyperparamaters optimized) on public leaderboard after having all the previous steps combined and noise added: 0.816\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ROC score using Logistic Regression (with hyperparamaters optimized) on public leaderboard after having all the previous steps combined and noise added: 0.816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13sARvs9gA11"
      },
      "source": [
        "To our disappointment, everything that we managed to come up thorough the course of the project, when combined still did not give us the require score on the private leaderboard. Out of desparation, let us simply throw a hyperoptimized lasso at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ9I6FEkhfQ6"
      },
      "source": [
        "random_seed = 213\n",
        "\n",
        "model = Lasso(alpha=0.031, tol=0.01, random_state=random_seed, selection=\"random\")\n",
        "\n",
        "param_grid = {\n",
        "    \"alpha\": [\n",
        "        0.022,\n",
        "        0.021,\n",
        "        0.02,\n",
        "        0.019,\n",
        "        0.023,\n",
        "        0.024,\n",
        "        0.025,\n",
        "        0.026,\n",
        "        0.027,\n",
        "        0.029,\n",
        "        0.031,\n",
        "    ],\n",
        "    \"tol\": [0.0013, 0.0014, 0.001, 0.0015, 0.0011, 0.0012, 0.0016, 0.0017],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    model, param_grid=param_grid, verbose=0, n_jobs=-1, scoring=\"roc_auc\", cv=20\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTyVp8rNiGgf",
        "outputId": "ab81380b-ba54-439e-988c-cc64353b0f7b"
      },
      "source": [
        "grid_search.fit(X_out, y_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=20, error_score=nan,\n",
              "             estimator=Lasso(alpha=0.031, copy_X=True, fit_intercept=True,\n",
              "                             max_iter=1000, normalize=False, positive=False,\n",
              "                             precompute=False, random_state=213,\n",
              "                             selection='random', tol=0.01, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'alpha': [0.022, 0.021, 0.02, 0.019, 0.023, 0.024,\n",
              "                                   0.025, 0.026, 0.027, 0.029, 0.031],\n",
              "                         'tol': [0.0013, 0.0014, 0.001, 0.0015, 0.0011, 0.0012,\n",
              "                                 0.0016, 0.0017]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='roc_auc', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP6PIflsiIqy",
        "outputId": "37731b3c-69b2-4828-e203-387965a923ff"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.031, 'tol': 0.0013}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fvp5jF2iYWZ"
      },
      "source": [
        "model = Lasso(alpha=0.031, tol=0.0013, random_state=random_seed, selection=\"random\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRi9Fe4gn_0"
      },
      "source": [
        "...which, to our surprise, gave us scores of 0.817 and \n",
        "0.836 for private and public leaderboards respectively, therefore helping us meet the required score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrwlzdWhilu7"
      },
      "source": [
        "# Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql5t6buaYKOt"
      },
      "source": [
        "The first limitation of our approach pertains to the fact that we have set our baseline score using all the features while it is recommended to start with the minimum number of features. We could have alternatively set our baseline score using the two features that have been found to be most correlated with the target variable "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vil7Pa-KYfgu",
        "outputId": "c25e6354-2a9a-4626-d302-8ef432d7af9a"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "X_two = X[[\"33\", \"65\"]]\n",
        "test_two = test_subm[[\"33\", \"65\"]]\n",
        "\n",
        "score_logreg = get_score(logreg, X_two, y)\n",
        "print(\n",
        "    \"Our baseline ROCAUC score using LogisticRegression on only two features is: \",\n",
        "    \"{:.4f}\".format(score_logreg),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our baseline ROCAUC score using LogisticRegression on only two features is:  0.7951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIHAGKapYwxe",
        "outputId": "613754ca-6fef-48ed-a572-ea429f8c0261"
      },
      "source": [
        "print(\n",
        "    \"The baseline ROCAUC score on public leaderbord using LogisticRegression on only two features: 0.746\",\n",
        ")\n",
        "\n",
        "gnb_kaggle = 0.746"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The baseline ROCAUC score on public leaderbord using LogisticRegression on only two features: 0.746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Vj0dNUopui"
      },
      "source": [
        "...and picked up from there. We will try this approach next time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePBDTzPYtMkT"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klEpNn6-tRks"
      },
      "source": [
        "1. Having too many features to inspect each of them for outliers visually, we've employed automatic outlier identification techniques (IsolationForest, EllipticEnvelope, OneClassSVM). EllipticEnvelope provided a best improvement on the score by removing the least number of rows. \n",
        "2. To address the issue of imbalanced distribution of classes, we applied a range of oversampling techniques. Out of 4 techniques (SMOTE, BorderlineSMOTE, SVMSMOTE and ADASYN), ADESYN lead to the best cross-validation results. The results on public leaderboard have not been effected by application of any of the techniques, however.\n",
        "3. We used a range of models that differ in complexity in order to put the hypothesis that the simpler models should perform better to test. While simpler models (Logistic Regression and GaussianNB) did outperform models such as Random Forest, the trend to overfit has been similar both with respect to simple and complex models.\n",
        "4. We have used PyCaret to give us a perspective on how does our selection of models fare with respect to a larger range of models. We have seen that the models we selected outperformed most of the others models implemented by PyCaret.\n",
        "5. The least succesful part of our project is that of feature selection as Rasgo which we used for calculating feature importances and dropping the least important features produced different results on each run. We have attempted to circumvent this problem by taking a union of features which have been selected for removal over the course of several runs. This way, we have reduced our feature space in half. \n",
        "6. Out of all the attempts in our feature engineering part, only second-degree polynomials of the features that were found to be most important proved beneficial towards the score. Neither feature crosses nor polynomials of a higher degree led to any improvements. At this point we have reached the required score of 0.8 on public Kaggle leaderboard.\n",
        "7. When combined, all the techniques that shown to improve the score when applied separately did not provide the required push for the score on the private leaderboard.\n",
        "8. To our surprise, Lasso, applied to a problem that initially seemed a classification problem, gave the best score (8.36) and also helped us to reach the required score on private scoreboard. This is most likely due to the fact that Lasso implements 'l1' regularization which brings the coefficients for the least important features to 0, thus zeroing out their constribution in predictions. While this means that we were not succesful in identifying the list of features the removal of which would have lead to the highest score, this is surprising since we have also implemented 'l1' regularization when using Logistic Regression.\n",
        "9. Another unexpected result is that submitting probabilities lead to better scores for almost all the models (except for K-Nearest Neighbors) although the sample prediction had only binary values in it."
      ]
    }
  ]
}